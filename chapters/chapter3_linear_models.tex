\chapter{Linear Models for Regression}

\section*{Exercise 3.1 $\star$}
Show that the $\tanh$ function and the logistic
sigmoid function ($\ref{eq:3.6}$) are related by 
\begin{equation}\label{eq:3.100}\tag{3.100}
    \tanh(a) = 2\sigma(2a) - 1
\end{equation}
Hence show that a general linear combination of logistic sigmoid functions
of the form
\begin{equation}\label{eq:3.101}\tag{3.101}
    y(x, \mathbf{w}) = w_0 + \sum_{j=1}^{M} w_j \sigma\bigg(\frac{x-\mu_j}{s}\bigg)
\end{equation}
is equivalent to a linear combination of $\tanh$ functions of the form
\begin{equation}\label{eq:3.102}\tag{3.102}
    y(x, \mathbf{u}) = u_0 + \sum_{j=1}^{M} u_j \tanh\bigg(\frac{x-\mu_j}{2s}\bigg)
\end{equation}
and find expressions to relate the new parameters $\{u_0, \ldots, u_M\}$
to the original parameters $\{w_0, \ldots, w_M\}$.

\vspace{1em}

\begin{proof}
    The logistic sigmoid function is given by
    \begin{equation}\label{eq:3.6}\tag{3.6}
        \sigma(x) = \frac{1}{1 + \exp(-x)}
    \end{equation}
    and the $\tanh$ function is given by
    \begin{equation}
        \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{e^{2x} - 1}{e^{2x} + 1}
    \end{equation}
    By starting from the right-hand side of ($\ref{eq:3.100}$) and then 
    using the fact that $\tanh$ is odd, we obtain 
    \begin{equation}\tag{3.100}
        2\sigma(2a) - 1 = \frac{2}{e^{-2a}} - 1 = \frac{1 - e^{-2a}}{1 + e^{-2a}}
        = -\tanh(-a) = \tanh(a)
    \end{equation}
    Now, we can express the logistic sigmoid functions as
    \[
        \sigma(x) = \frac{1}{2}\tanh\frac{x}{2} + \frac{1}{2}
    \] 
    By substituting this in (\ref{eq:3.101}), we have that
    \begin{align*}
        y(x, \mathbf{w}) = w_0 + \frac{M}{2} +  
        \sum_{j=1}^{M} \frac{w_j}{2} \tanh\bigg(\frac{x - \mu_j}{2s}\bigg)
        = y(x, \mathbf{u})
    \end{align*}
    where
    \[
        u_0 = w_0 + \frac{M}{2} 
        \hspace{5em}
        u_j = \frac{1}{2} w_j, j \geq 1
    \] 
    Therefore, we proved that ($\ref{eq:3.101}$) is equivalent to ($\ref{eq:3.102}$).
\end{proof}

\section*{Exercise 3.2 $\star \star$}
Show that the matrix 
\begin{equation}\label{eq:3.103}\tag{3.103}
    \mathbf{\Phi}(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T
\end{equation}
takes any vector $\mathbf{v}$ and projects it onto the space spanned by the columns
of $\mathbf{\Phi}$. Use this result to show that the least-squares solution 
($\ref{eq:3.15}$) corresponds to an orthogonal projection of the vector $\mathbf{t}$ onto
the manifold $\mathcal{S}$ as shown in Figure 3.2.

\vspace{1em}

\begin{proof}
    Let $\mathbf{p}$ be the projection of $\mathbf{v}$ onto the space spanned by the
    columns of $\mathbf{\Phi}$. We then have that $\mathbf{p}$ is contained by
    the space, so $\mathbf{p}$ can be written as a linear combination
    of the columns of $\mathbf{\Phi}$, i.e. there exists $\mathbf{x}$
    such that  $\mathbf{p} = \mathbf{\Phi}\mathbf{x}$. By using this
    and the fact that $\mathbf{p} - \mathbf{v}$ is orthogonal
    to the space, we have that
    \begin{align*}
        \mathbf{\Phi}^T(\mathbf{p} - \mathbf{v}) &= \mathbf{0} \\
        \mathbf{\Phi}^T(\mathbf{\Phi}\mathbf{x} - \mathbf{v}) &= \mathbf{0} \\
        \mathbf{\Phi}^T\mathbf{\Phi}\mathbf{x} &= \mathbf{\Phi}^T\mathbf{v} \\
        \mathbf{x} &= (\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\mathbf{v}
    \end{align*}
    and since $\mathbf{p} = \mathbf{\Phi}\mathbf{x}$, this proves our hypothesis,
    i.e. 
    \[
        \mathbf{p} = \mathbf{\Phi}(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\mathbf{v}
    \] 

    This translates directly to the least-squares geometry described 
    in Section 3.1.3, where the manifold $\mathcal{S}$ is the space spanned
    by the columns of $\mathbf{\Phi}$. From what we proved above,
    the projection of $\mathbf{t}$ onto the manifold $\mathcal{S}$ 
    is given by $\mathbf{y} = \mathbf{\Phi}\mathbf{w}_{\text{ML}}$,
    where 
    \begin{equation}\label{eq:3.15}\tag{3.15}
        \mathbf{w}_{\text{ML}} = (\mathbf{\Phi}^T \mathbf{\Phi})^{-1} \mathbf{\Phi}^T \mathbf{t}
    \end{equation}
    is the least-squares solution.
\end{proof}

\section*{Exercise 3.3 $\star$}
Consider a data set in which each data point $t_n$ is associated with a weighting
factor $r_n > 0$, so that the sum of squares error function becomes
\begin{equation}\label{eq:3.104}\tag{3.104}
    E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
    r_n\{t_n - \mathbf{w}^T \bm{\phi}(\mathbf{x}_n)\}^2
\end{equation}
Find an expression for the solution $\mathbf{w}^\star$ that minimizes this error function.
Give two alternative interpretations of the weighted sum-of-squares error function
in terms of (i) data dependent noise variance and (ii) replicated data points.

\vspace{1em}

\textbf{Method 1.}

\begin{proof}
    Since the least-squares error function is convex,
    the function is minimized in its only critical 
    point. Similarly to (3.13), the derivative is given by:
    \begin{align*}
        \pdv{\mathbf{w}} E_D(\mathbf{w})
        &= \frac{1}{2} \sum_{n=1}^{N} r_n \bigg(\pdv{\mathbf{w}} 
        \{t_n - \mathbf{w}^T \bm{\phi}(\mathbf{x}_n)\}^2\bigg) \\
        &= \sum_{n=1}^N r_n \{\mathbf{w}^T \bm{\phi}(\mathbf{x}_n) - t_n\} 
        \bm{\phi}(\mathbf{x}_n)^T \\
        &= \mathbf{w}^T 
        \bigg(\sum_{i=1}^{N} r_n \bm{\phi}(\mathbf{x}_n)
        \bm{\phi}(\mathbf{x}_n)^T\bigg) -
        \sum_{n=1}^{N} r_n t_n \bm{\phi}(\mathbf{x}_n)^T
    \end{align*}
    By defining the matrix $R = \text{diag}(r_1, r_2, \ldots, r_n)$
    and then setting the derivative to 0, we obtain the equality
    \[
        \mathbf{w}^T \mathbf{\Phi}^TR\mathbf{\Phi}^T
        = \mathbf{t}^TR\mathbf{\Phi} 
    \] 
    which gives the weighted least-squares solution (we get the
    column vector form):
    \[
        \mathbf{w}^\star = (\mathbf{\Phi}^TR\mathbf{\Phi})^{-1} \mathbf{\Phi}^T R \mathbf{t} 
    \] 
\end{proof}

\textbf{Method 2.}
\begin{proof}

    We define the diagonal matrices $R = \text{diag}(r_1, r_2, \ldots, r_n)$ and
    $R^{1/2} = \text{diag}(\sqrt{r_1}, \sqrt{r_2}, \ldots, \sqrt{r_n})$
    such that $R^{1/2}R^{1/2} = R$. We notice that we can rewrite ($\ref{eq:3.104}$) as:
    \[
        E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
        \big(\sqrt{r_n}\{t_n - \mathbf{w}^T \bm{\phi}(\mathbf{x}_n)\}\big)^2
    \] 
    which we can translate into matrix notation as:
    \[
        E_D(\mathbf{w}) = \frac{1}{2} \big(R^{1/2}(\mathbf{t} - \mathbf{\Phi}\mathbf{w})\big)^T
        \big(R^{1/2}(\mathbf{t} - \mathbf{\Phi}\mathbf{w})\big)
    \] 
    Since the least-squares error function is convex, the function
    is minimized in its only critical point. The derivative is given by
    \begin{align*}
        \pdv{\mathbf{w}} E_D(\mathbf{w})
        &= -\mathbf{\Phi}^T(R^{1/2})^T(R^{1/2}\mathbf{t} - R^{1/2}\mathbf{\Phi}\mathbf{w}) \\
        &= \mathbf{\Phi}^TR\mathbf{\Phi}\mathbf{w} - \mathbf{\Phi}^TR\mathbf{t}
    \end{align*}
    By setting it to 0, we obtain the solution that minimizes the weighted
    least-squares error function:
    \[
        \mathbf{w}^\star = (\mathbf{\Phi}^TR\mathbf{\Phi})^{-1} \mathbf{\Phi}^T R \mathbf{t} 
    \] 
\end{proof}

\section*{Exercise 3.4 $\star$}
Consider a linear model of the form
\begin{equation}\label{eq:3.105}\tag{3.105}
    y(\mathbf{x}, \mathbf{w}) = w_0 + \sum_{i=1}^{D} w_ix_i
\end{equation}
together with a sum-of-squares error function of 
the form
\begin{equation}\label{eq:3.106}\tag{3.106}
    E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N}
    \{y(\mathbf{x}_n, \mathbf{w}) - t_n\}^2
\end{equation}
Now suppose that Gaussian noise $\epsilon_i$ with zero
mean and variance $\sigma^2$ is added independently
to each of the input variables $x_i$. By making
use of $\mathbb{E}[\epsilon_i] = 0$ and 
$\mathbb{E}[\epsilon_i\epsilon_j] = \delta_{ij}\sigma^2$,
show that minimizing $E_D$ averaged over the noise 
distribution is equivalent to minimizing the 
sum-of-squares error for noise-free input variables
with the addition of a weight-decay regularization term,
in which the bias parameter $w_0$ is omitted from
the regularizer.

\vspace{1em}

\begin{proof}
    Let the noise-free input variables be denoted by $\mathbf{x}^*$, such that
    $x_i = x_i^* + \epsilon_i$. ($\ref{eq:3.105}$) will then be equivalent to
    \[
        y(\mathbf{x}, \mathbf{w}) = w_0 + \sum_{i=1}^{D} w_ix_i^* + \sum_{i=1}^{D} w_i\epsilon_i
        = y(\mathbf{x}^*, \mathbf{w}) + \sum_{i=1}^{D} w_i\epsilon_i
    \] 
    Now, we aim to find the expression of $E_D$ averaged
    over the noise distribution, that is:
    \begin{align*}
        \mathbb{E}[E_D(\mathbf{w})]
        = \frac{1}{2} \sum_{n=1}^{N} \{\mathbb{E}[y(\mathbf{x}_n, \mathbf{w})^2]
        - 2t_n\mathbb{E}[y(\mathbf{x}_n, \mathbf{w})] + t_n^2\}
    \end{align*}
    The individual expectations are straightforward to compute. Since
    $\mathbb{E}[\epsilon_i] = 0$, we have that
    \begin{align*}
        \mathbb{E}[y(\mathbf{x}_n, \mathbf{w})] 
        = \mathbb{E}[y(\mathbf{x}^*, \mathbf{w})] + \sum_{i=1}^{D} w_i\mathbb{E}[\epsilon_i]
        = y(\mathbf{x}^*, \mathbf{w})
    \end{align*}
    Also, $\mathbb{E}[\epsilon_i\epsilon_j] = \delta_{ij} \sigma^2$, so
    \begin{align*}
        \mathbb{E}[y(\mathbf{x}_n, \mathbf{w})^2] 
        &= \mathbb{E}\bigg[y(\mathbf{x}^*, \mathbf{w})^2 
            + 2y(\mathbf{x}^*, \mathbf{w}) \sum_{i=1}^{D} w_i\epsilon_i
            + \bigg(\sum_{i=1}^{D} w_i \epsilon_i\bigg)^2\bigg] \\
        &= y(\mathbf{x}^*, \mathbf{w})^2 + \sum_{i=1}^{D} w_i^2 \mathbb{E}[\epsilon_i^2]
            + 2\sum_{i=1}^{D} \sum_{j=i+1}^{D} w_iw_j \mathbb{E}[\epsilon_i\epsilon_j] \\
        &= y(x^*, \mathbf{w})^2 + \sigma\sum_{n=1}^{D} w_i^2
    \end{align*}
    Therefore, we have that
    \begin{align*}
        \mathbb{E}[E_D({\mathbf{w}})]
        = \frac{1}{2} \sum_{n=1}^{D} \{y(\mathbf{x}_n^*, \mathbf{w}) - t_n\}^2
        + \frac{N\sigma}{2} \sum_{n=1}^{D} w_i^2
    \end{align*}
    which shows that $E_D$ averaged over the noise distribution
    is equivalent to the regularized least-squares error function
    with $\lambda = N\sigma$. Hence, since the expressions are equivalent,
    minimizing them is also equivalent, proving our hypothesis.
\end{proof}
