\chapter{Linear Models for Regression}

\section*{Exercise 3.1 $\star$}
Show that the $\tanh$ function and the logistic
sigmoid function ($\ref{eq:3.6}$) are related by 
\begin{equation}\label{eq:3.100}\tag{3.100}
    \tanh(a) = 2\sigma(2a) - 1
\end{equation}
Hence show that a general linear combination of logistic sigmoid functions
of the form
\begin{equation}\label{eq:3.101}\tag{3.101}
    y(x, \mathbf{w}) = w_0 + \sum_{j=1}^{M} w_j \sigma\bigg(\frac{x-\mu_j}{s}\bigg)
\end{equation}
is equivalent to a linear combination of $\tanh$ functions of the form
\begin{equation}\label{eq:3.102}\tag{3.102}
    y(x, \mathbf{u}) = u_0 + \sum_{j=1}^{M} u_j \tanh\bigg(\frac{x-\mu_j}{2s}\bigg)
\end{equation}
and find expressions to relate the new parameters $\{u_0, \ldots, u_M\}$
to the original parameters $\{w_0, \ldots, w_M\}$.

\vspace{1em}

\begin{proof}
    The logistic sigmoid function is given by
    \begin{equation}\label{eq:3.6}\tag{3.6}
        \sigma(x) = \frac{1}{1 + \exp(-x)}
    \end{equation}
    and the $\tanh$ function is given by
    \begin{equation}
        \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{e^{2x} - 1}{e^{2x} + 1}
    \end{equation}
    By starting from the right-hand side of ($\ref{eq:3.100}$) and then 
    using the fact that $\tanh$ is odd, we obtain 
    \begin{equation}\tag{3.100}
        2\sigma(2a) - 1 = \frac{2}{e^{-2a}} - 1 = \frac{1 - e^{-2a}}{1 + e^{-2a}}
        = -\tanh(-a) = \tanh(a)
    \end{equation}
    Now, we can express the logistic sigmoid functions as
    \[
        \sigma(x) = \frac{1}{2}\tanh\frac{x}{2} + \frac{1}{2}
    \] 
    By substituting this in (\ref{eq:3.101}), we have that
    \begin{align*}
        y(x, \mathbf{w}) = w_0 + \frac{M}{2} +  
        \sum_{j=1}^{M} \frac{w_j}{2} \tanh\bigg(\frac{x - \mu_j}{2s}\bigg)
        = y(x, \mathbf{u})
    \end{align*}
    where
    \[
        u_0 = w_0 + \frac{M}{2} 
        \hspace{5em}
        u_j = \frac{1}{2} w_j, j \geq 1
    \] 
    Therefore, we proved that ($\ref{eq:3.101}$) is equivalent to ($\ref{eq:3.102}$).
\end{proof}

\section*{Exercise 3.2 $\star \star$}
Show that the matrix 
\begin{equation}\label{eq:3.103}\tag{3.103}
    \mathbf{\Phi}(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T
\end{equation}
takes any vector $\mathbf{v}$ and projects it onto the space spanned by the columns
of $\mathbf{\Phi}$. Use this result to show that the least-squares solution 
($\ref{eq:3.15}$) corresponds to an orthogonal projection of the vector $\mathbf{t}$ onto
the manifold $\mathcal{S}$ as shown in Figure 3.2.

\vspace{1em}

\begin{proof}
    Let $\mathbf{p}$ be the projection of $\mathbf{v}$ onto the space spanned by the
    columns of $\mathbf{\Phi}$. We then have that $\mathbf{p}$ is contained by
    the space, so $\mathbf{p}$ can be written as a linear combination
    of the columns of $\mathbf{\Phi}$, i.e. there exists $\mathbf{x}$
    such that  $\mathbf{p} = \mathbf{\Phi}\mathbf{x}$. By using this
    and the fact that $\mathbf{p} - \mathbf{v}$ is orthogonal
    to the space, we have that
    \begin{align*}
        \mathbf{\Phi}^T(\mathbf{p} - \mathbf{v}) &= \mathbf{0} \\
        \mathbf{\Phi}^T(\mathbf{\Phi}\mathbf{x} - \mathbf{v}) &= \mathbf{0} \\
        \mathbf{\Phi}^T\mathbf{\Phi}\mathbf{x} &= \mathbf{\Phi}^T\mathbf{v} \\
        \mathbf{x} &= (\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\mathbf{v}
    \end{align*}
    and since $\mathbf{p} = \mathbf{\Phi}\mathbf{x}$, this proves our hypothesis,
    i.e. 
    \[
        \mathbf{p} = \mathbf{\Phi}(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\mathbf{v}
    \] 

    This translates directly to the least-squares geometry described 
    in Section 3.1.3, where the manifold $\mathcal{S}$ is the space spanned
    by the columns of $\mathbf{\Phi}$. From what we proved above,
    the projection of $\mathbf{t}$ onto the manifold $\mathcal{S}$ 
    is given by $\mathbf{y} = \mathbf{\Phi}\mathbf{w}_{\text{ML}}$,
    where 
    \begin{equation}\label{eq:3.15}\tag{3.15}
        \mathbf{w}_{\text{ML}} = (\mathbf{\Phi}^T \mathbf{\Phi})^{-1} \mathbf{\Phi}^T \mathbf{t}
    \end{equation}
    is the least-squares solution.
\end{proof}

\section*{Exercise 3.3 $\star$}
Consider a data set in which each data point $t_n$ is associated with a weighting
factor $r_n > 0$, so that the sum of squares error function becomes
\begin{equation}\label{eq:3.104}\tag{3.104}
    E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
    r_n\{t_n - \mathbf{w}^T \bm{\phi}(\mathbf{x}_n)\}^2
\end{equation}
Find an expression for the solution $\mathbf{w}^\star$ that minimizes this error function.
Give two alternative interpretations of the weighted sum-of-squares error function
in terms of (i) data dependent noise variance and (ii) replicated data points.

\vspace{1em}

\textbf{Method 1.}

\begin{proof}
    Since the least-squares error function is convex,
    the function is minimized in its only critical 
    point. Similarly to (3.13), the derivative is given by:
    \begin{align*}
        \pdv{\mathbf{w}} E_D(\mathbf{w})
        &= \frac{1}{2} \sum_{n=1}^{N} r_n \bigg(\pdv{\mathbf{w}} 
        \{t_n - \mathbf{w}^T \bm{\phi}(\mathbf{x}_n)\}^2\bigg) \\
        &= \sum_{n=1}^N r_n \{\mathbf{w}^T \bm{\phi}(\mathbf{x}_n) - t_n\} 
        \bm{\phi}(\mathbf{x}_n)^T \\
        &= \mathbf{w}^T 
        \bigg(\sum_{i=1}^{N} r_n \bm{\phi}(\mathbf{x}_n)
        \bm{\phi}(\mathbf{x}_n)^T\bigg) -
        \sum_{n=1}^{N} r_n t_n \bm{\phi}(\mathbf{x}_n)^T
    \end{align*}
    By defining the matrix $R = \text{diag}(r_1, r_2, \ldots, r_n)$
    and then setting the derivative to 0, we obtain the equality
    \[
        \mathbf{w}^T \mathbf{\Phi}R\mathbf{\Phi}^T
        = \mathbf{t}^TR\mathbf{\Phi} 
    \] 
    which gives the weighted least-squares solution (we get the
    column vector form):
    \[
        \mathbf{w}^\star = (\mathbf{\Phi}^TR\mathbf{\Phi})^{-1} \mathbf{\Phi}^T R \mathbf{t} 
    \] 
\end{proof}

\textbf{Method 2.}
\begin{proof}

    We define the diagonal matrices $R = \text{diag}(r_1, r_2, \ldots, r_n)$ and
    $R^{1/2} = \text{diag}(\sqrt{r_1}, \sqrt{r_2}, \ldots, \sqrt{r_n})$
    such that $R^{1/2}R^{1/2} = R$. We notice that we can rewrite ($\ref{eq:3.104}$) as:
    \[
        E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
        \big(\sqrt{r_n}\{t_n - \mathbf{w}^T \bm{\phi}(\mathbf{x}_n)\}\big)^2
    \] 
    which we can translate into matrix notation as:
    \[
        E_D(\mathbf{w}) = \frac{1}{2} \big(R^{1/2}(\mathbf{t} - \mathbf{\Phi}\mathbf{w})\big)^T
        \big(R^{1/2}(\mathbf{t} - \mathbf{\Phi}\mathbf{w})\big)
    \] 
    Since the least-squares error function is convex, the function
    is minimized in its only critical point. The derivative is given by
    \begin{align*}
        \pdv{\mathbf{w}} E_D(\mathbf{w})
        &= -\mathbf{\Phi}^T(R^{1/2})^T(R^{1/2}\mathbf{t} - R^{1/2}\mathbf{\Phi}\mathbf{w}) \\
        &= \mathbf{\Phi}^TR\mathbf{\Phi}\mathbf{w} - \mathbf{\Phi}^TR\mathbf{t}
    \end{align*}
    By setting it to 0, we obtain the solution that minimizes the weighted
    least-squares error function:
    \[
        \mathbf{w}^\star = (\mathbf{\Phi}^TR\mathbf{\Phi})^{-1} \mathbf{\Phi}^T R \mathbf{t} 
    \] 
\end{proof}

\section*{Exercise 3.4 $\star$}
Consider a linear model of the form
\begin{equation}\label{eq:3.105}\tag{3.105}
    y(\mathbf{x}, \mathbf{w}) = w_0 + \sum_{i=1}^{D} w_ix_i
\end{equation}
together with a sum-of-squares error function of 
the form
\begin{equation}\label{eq:3.106}\tag{3.106}
    E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N}
    \{y(\mathbf{x}_n, \mathbf{w}) - t_n\}^2
\end{equation}
Now suppose that Gaussian noise $\epsilon_i$ with zero
mean and variance $\sigma^2$ is added independently
to each of the input variables $x_i$. By making
use of $\mathbb{E}[\epsilon_i] = 0$ and 
$\mathbb{E}[\epsilon_i\epsilon_j] = \delta_{ij}\sigma^2$,
show that minimizing $E_D$ averaged over the noise 
distribution is equivalent to minimizing the 
sum-of-squares error for noise-free input variables
with the addition of a weight-decay regularization term,
in which the bias parameter $w_0$ is omitted from
the regularizer.

\vspace{1em}

\begin{proof}
    Let the noise-free input variables be denoted by $\mathbf{x}^*$, such that
    $x_i = x_i^* + \epsilon_i$. ($\ref{eq:3.105}$) will then be equivalent to
    \[
        y(\mathbf{x}, \mathbf{w}) = w_0 + \sum_{i=1}^{D} w_ix_i^* + \sum_{i=1}^{D} w_i\epsilon_i
        = y(\mathbf{x}^*, \mathbf{w}) + \sum_{i=1}^{D} w_i\epsilon_i
    \] 
    Now, we aim to find the expression of $E_D$ averaged
    over the noise distribution, that is:
    \begin{align*}
        \mathbb{E}[E_D(\mathbf{w})]
        = \frac{1}{2} \sum_{n=1}^{N} \{\mathbb{E}[y(\mathbf{x}_n, \mathbf{w})^2]
        - 2t_n\mathbb{E}[y(\mathbf{x}_n, \mathbf{w})] + t_n^2\}
    \end{align*}
    The individual expectations are straightforward to compute. Since
    $\mathbb{E}[\epsilon_i] = 0$, we have that
    \begin{align*}
        \mathbb{E}[y(\mathbf{x}_n, \mathbf{w})] 
        = \mathbb{E}[y(\mathbf{x}^*, \mathbf{w})] + \sum_{i=1}^{D} w_i\mathbb{E}[\epsilon_i]
        = y(\mathbf{x}^*, \mathbf{w})
    \end{align*}
    Also, $\mathbb{E}[\epsilon_i\epsilon_j] = \delta_{ij} \sigma^2$, so
    \begin{align*}
        \mathbb{E}[y(\mathbf{x}_n, \mathbf{w})^2] 
        &= \mathbb{E}\bigg[y(\mathbf{x}^*, \mathbf{w})^2 
            + 2y(\mathbf{x}^*, \mathbf{w}) \sum_{i=1}^{D} w_i\epsilon_i
            + \bigg(\sum_{i=1}^{D} w_i \epsilon_i\bigg)^2\bigg] \\
        &= y(\mathbf{x}^*, \mathbf{w})^2 + \sum_{i=1}^{D} w_i^2 \mathbb{E}[\epsilon_i^2]
            + 2\sum_{i=1}^{D} \sum_{j=i+1}^{D} w_iw_j \mathbb{E}[\epsilon_i\epsilon_j] \\
        &= y(x^*, \mathbf{w})^2 + \sigma\sum_{n=1}^{D} w_i^2
    \end{align*}
    Therefore, we have that
    \begin{align*}
        \mathbb{E}[E_D({\mathbf{w}})]
        = \frac{1}{2} \sum_{n=1}^{D} \{y(\mathbf{x}_n^*, \mathbf{w}) - t_n\}^2
        + \frac{N\sigma}{2} \sum_{n=1}^{D} w_i^2
    \end{align*}
    which shows that $E_D$ averaged over the noise distribution
    is equivalent to the regularized least-squares error function
    with $\lambda = N\sigma$. Hence, since the expressions are equivalent,
    minimizing them is also equivalent, proving our hypothesis.
\end{proof}

\section*{Exercise 3.5 $\star$}
Using the technique of Lagrange multipliers, discussed in Appendix E,
show that minimization of the regularized error function (3.29)
is equivalent to minimizing the unregularized sum-of-squares error (3.12)
subject to the constraint (3.30). Discuss the relationship
between the parameters $\eta$ and $\lambda$.

\begin{proof}
    To minimize the unregularized sum-of-squares error (3.12) subject to
    the constraint (3.30), is equivalent to minimizing the Lagrangian
    \[
        L(\mathbf{x}, \lambda) = 
        \frac{1}{2}\sum_{n=1}^{N} \{y(\mathbf{x}_n, \mathbf{w}) - t_n\}^2 
        - \lambda\bigg(\eta - \sum_{j=1}^{M} |w_j|^q\bigg)
    \] subject to the KKT conditions (see E.9, E.10, E.11 in Appendix E).
    Our Lagrangian and the regularized sum-of-squares error have the same
    dependency over $\mathbf{w}$, so their minimization is equivalent.
    By following (E.11), we have that
     \[
         \lambda\bigg(\eta - \sum_{j=1}^{M} |w_j|^q\bigg) = 0
    \] 
    which means that if $\mathbf{w}^\star(\lambda)$ is the solution of minimization
    for a fixed $\lambda > 0$, we then have that 
     \[
         \eta = \sum_{j=1}^{M} |w^\star(\lambda)_j|^q
    \] 
\end{proof}

\section*{Exercise 3.6 $\star$}
Consider a linear basis function regression model
for a multivariate target variable $\mathbf{t}$ having 
a Gaussian distribution of the form
\begin{equation}\label{eq:3.107}\tag{3.107}
    p(\mathbf{t} | \mathbf{W}, \mathbf{\Sigma})
    = \mathcal{N}(\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{W}), \mathbf{\Sigma})
\end{equation}
where 
\begin{equation}\label{eq:3.108}\tag{3.108}
    \mathbf{y}(\mathbf{x}, \mathbf{W}) = \mathbf{W}^T \bm{\phi}(\mathbf{x})
\end{equation}
together with a training data set compromising input
basis vectors $\bm{\phi}(\mathbf{x}_n)$ and corresponding
target vectors $\mathbf{t}_n$, with $n = 1, \ldots, N$. Show
that the maximum likelihood solution $\mathbf{W}_{\text{ML}}$ for
the parameter matrix $\mathbf{W}$ has the property that each 
column is given by an expression of the form ($\ref{eq:3.15}$),
which was the solution for an isotropic noise distribution.
Note that this is independent of the covariance matrix
$\mathbf{\Sigma}$. Show that the maximum likelihood solution
for $\mathbf{\Sigma}$ is given by
\begin{equation}\label{eq:3.109}\tag{3.109}
    \mathbf{\Sigma} = \frac{1}{N} \sum_{n=1}^{N} 
    \big(\mathbf{t}_n - \mathbf{W}_{\text{ML}}^T\bm{\phi}(\mathbf{x}_n)\big)
    \big(\mathbf{t}_n - \mathbf{W}_{\text{ML}}^T\bm{\phi}(\mathbf{x}_n)\big)^T
\end{equation}

\vspace{1em}

\begin{proof}
    Similarly to what we did in Section 3.1.5, we combine
    the set of target vectors into a matrix $\mathbf{T}$ of
    size $N \cross K$ such that the $n^{\text{th}}$ row
    is given by $\mathbf{t}_n^T$. We do the same for
    $\mathbf{X}$. The log likelihood 
    function is then given by
    \begin{align*}
        \ln p(\mathbf{T} | \mathbf{X}, \mathbf{W}, \mathbf{\Sigma})
        &= \ln \prod_{n = 1}^N 
            \mathcal{N}(\mathbf{t}_n | \mathbf{W}^T\bm{\phi}(\mathbf{x}_n), \mathbf{\Sigma}) \\
        &= \sum_{n=1}^{N} 
            \ln \mathcal{N}(\mathbf{t}_n | \mathbf{W}^T\bm{\phi}(\mathbf{x}_n), \mathbf{\Sigma}) \\
        &= \sum_{n=1}^{N} \ln \bigg[\frac{1}{(2\pi)^{K/2}|\mathbf{\Sigma}|^{1/2}}
            \exp\big\{\big(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big)^T
            \mathbf{\Sigma}^{-1}(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big)
        \big\}\bigg] \\
        &= -\frac{NK}{2} \ln(2\pi) - \frac{1}{2} \ln|\mathbf{\Sigma}| 
            + \sum_{n=1}^{N} \big(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big)^T
            \mathbf{\Sigma}^{-1}(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big)
    \end{align*}
    Our goal is to maximise this function with respect to $\mathbf{W}$.
    We take the derivative of the likelihood and use the fact that
    $\mathbf{\Sigma}^{-1}$ is symmetric and (88) from the 
    \href{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}{matrix cookbook}
    to obtain:
    \begin{align*}
        \pdv{\mathbf{W}} \ln p(\mathbf{T} | \mathbf{X}, \mathbf{W}, \mathbf{\Sigma})
        &= \sum_{n=1}^{N} \pdv{\mathbf{W}} 
            \big(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big)^T
            \mathbf{\Sigma}^{-1}(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big) \\
        &= -2\mathbf{\Sigma}^{-1} \sum_{n=1}^{N} \big(\mathbf{t}_n - \mathbf{W}^T\bm{\phi}(\mathbf{x}_n)\big)
            \bm{\phi}(\mathbf{x}_n)^T
    \end{align*}
    By setting the derivative equal to 0, we find the maximum likelihood
    solution for $\mathbf{W}$:
    \begin{align*}
            -2\mathbf{\Sigma}^{-1} \sum_{n=1}^{N} \big(\mathbf{t}_n 
            - \mathbf{W}_{\text{ML}}^T\bm{\phi}(\mathbf{x})\big) \bm{\phi}(\mathbf{x})^T 
        &= 0 \\
            \mathbf{\Sigma}^{-1} \sum_{n=1}^{N} \mathbf{t}_n \bm{\phi}(\mathbf{x}_n)^T
        &= \mathbf{\Sigma}^{-1} \mathbf{W}_{\text{ML}}^T 
            \sum_{n=1}^{N} \bm{\phi}(\mathbf{x}_n)\bm{\phi}(\mathbf{x}_n)^T \\
            \mathbf{\Sigma}^{-1} \mathbf{T}^T \mathbf{\Phi} 
        &= \mathbf{\Sigma}^{-1} \mathbf{W}_{\text{ML}}^T \mathbf{\Phi}^T\mathbf{\Phi} \\
            \mathbf{\Phi}^T\mathbf{T}\mathbf{\Sigma}^{-1}
        &= \mathbf{\Phi}^T\mathbf{\Phi}\mathbf{W}_{\text{ML}}\mathbf{\Sigma}^{-1}
    \end{align*}
    Note that $\mathbf{\Sigma}^{-1}$ cancels out and we finally get that:
     \[
         \mathbf{W}_{\text{ML}} = (\mathbf{\Phi}^T\mathbf{\Phi})^{-1} \mathbf{\Phi}^T\mathbf{T}
    \] 
    Now, let $A, B$ be two matrices of size $N \cross M$ and let $b_1, b_2, \ldots, b_N$ 
    be the column vectors of $B$. One could easily prove that
     \[
         AB = A \big(b_1 \hspace{0.25em} b_2 \hspace{0.25em} \ldots \hspace{0.25em} b_N\big)
         = \big(Ab_1 \hspace{0.25em} Ab_2 \hspace{0.25em} \ldots \hspace{0.25em} Ab_N\big)
    \] 
    By using this for our case, that is to find the columns of $\mathbf{W}_{\text{ML}}$,
    we'd find that they are of the form $(\ref{eq:3.15})$, i.e. the $n^{\text{th}}$
    column of $\mathbf{W}_{\text{ML}}$ is given by
    \[
        \mathbf{W}_{\text{ML}}^{(n)} =
        (\mathbf{\Phi}^T\mathbf{\Phi})^{-1} \mathbf{\Phi}^T\mathbf{T}^{(n)}
    \] 
    where $\mathbf{T}^{(n)}$ is the $n^{\text{th}}$ column of $\mathbf{T}$.
\end{proof}

\section*{Exercise 3.7 $\star$}
By using the technique of completing the square, verify the result $(\ref{eq:3.49})$ for
the posterior distribution of the parameters $\mathbf{w}$ in the linear basis function
model in which $\mathbf{m}_N$ and $\mathbf{S}_N$ are defined by ($\ref{eq:3.50}$) and ($\ref{eq:3.51}$) respectively.

\vspace{1em}

\begin{proof}
    Since 
    \begin{align*}
        p(\mathbf{w} | \mathbf{t})
        &\propto p(\mathbf{w}) p(\mathbf{t} | \mathbf{X}, \mathbf{w}, \beta^{-1}) \\
        &\propto \mathcal{N}(\mathbf{w} | \mathbf{m}_0, \mathbf{S}_0) 
            \prod_{n=1}^N \mathcal{N}(t_n | \mathbf{w}^T\bm{\phi}(\mathbf{x}_n), \beta^{-1})
    \end{align*}
    we have that
    \begin{equation}\label{eq:3.7.1}\tag{3.7.1}
        \ln p(\mathbf{w} | \mathbf{t})
        = \ln \mathcal{N}(\mathbf{w} | \mathbf{m}_N, \mathbf{S}_N) 
            + \ln \prod_{n=1}^N \mathcal{N}(t_n | \mathbf{w}^T\bm{\phi}(\mathbf{x}_n), \beta^{-1}) 
            + \text{const}
    \end{equation}
    We compute the first logarithm, expand the square and keep only the terms that depend
    on $\mathbf{w}$ to obtain:
    \begin{align*}
        \ln \mathcal{N}(\mathbf{w} | \mathbf{m}_0, \mathbf{S}_0)
        &= -\frac{1}{2} \mathbf{w}^T \mathbf{S}_0^{-1} \mathbf{w} 
            + \mathbf{w}^T\mathbf{S}_0^{-1}\mathbf{m}_0
            + \text{const}
    \end{align*}
    By doing the same for the second term, we'll have that:
    \begin{align*}
        \ln \prod_{n = 1}^N \mathcal{N}(t_n | \mathbf{w}^T \bm{\phi}(\mathbf{x}_n), \beta^{-1})   
        &= \sum_{n=1}^N \ln \mathcal{N}(t_n | \mathbf{w}^T \bm{\phi}(\mathbf{x}_n), \beta^{-1}) \\
        &= -\beta \mathbf{w}^T \sum_{n=1}^N t_n \bm{\phi}(\mathbf{x}_n) 
            - \frac{\beta}{2} \sum_{n=1}^N \mathbf{w}^T \bm{\phi}(\mathbf{x}_n)\bm{\phi}(\mathbf{x}_n)^T \mathbf{w} 
            + \text{const} \\
        &= -\beta \mathbf{w}^T \mathbf{\Phi}^T \mathbf{t} 
            - \frac{\beta}{2} \mathbf{w}^T\mathbf{\Phi}^T\mathbf{\Phi}\mathbf{w} + \text{const}
    \end{align*}
    By replacing back into ($\ref{eq:3.7.1}$),
    \begin{align*}
        \ln p(\mathbf{w} | \mathbf{t}) 
        &= -\frac{1}{2} \mathbf{w}^T\mathbf{S}_0^{-1}\mathbf{w} 
            + \mathbf{w}^T\mathbf{S}_0^{-1}\mathbf{m}_0
            -\beta\mathbf{w}^T\mathbf{\Phi}^T\mathbf{t}
            - \frac{\beta}{2} \mathbf{w}^T\mathbf{\Phi}^T\mathbf{\Phi}\mathbf{w} 
            + \text{const} \\
        &= -\frac{1}{2} \mathbf{w}(\mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^T\mathbf{\Phi})\mathbf{w}^T
            + \mathbf{w}^T (\mathbf{S}_0^{-1}\mathbf{m}_0 - \beta\mathbf{\Phi}^T\mathbf{t})
            + \text{const}
    \end{align*}
    The quadratic term corresponds to a Gaussian with the covariance matrix $\mathbf{S}_N$, where
    \begin{equation}\label{eq:3.51}\tag{3.51}
        \mathbf{S}_N^{-1} = \mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^T\mathbf{\Phi}
    \end{equation}
    Now, since the mean is found in the linear term, we'd have that
    \begin{equation*}
        \mathbf{w}^T(\mathbf{S}_0^{-1}\mathbf{m}_0 - \beta\mathbf{\Phi}^T\mathbf{t})
        = \mathbf{w}^T\mathbf{S}_N^{-1}\mathbf{m}_N
    \end{equation*}
    which gives
    \begin{equation}\label{eq:3.50}\tag{3.50}
        \mathbf{m}_N = \mathbf{S}_N(\mathbf{S}_0^{-1}\mathbf{m_0} + \beta\mathbf{\Phi}^T\mathbf{t})
    \end{equation}
    Since we proved both $(\ref{eq:3.50})$ and $(\ref{eq:3.51})$, we showed that
    \begin{equation}\label{eq:3.49}\tag{3.49}
        p(\mathbf{w} | \mathbf{t}) = \mathcal{N}(\mathbf{w} | \mathbf{m}_N, \mathbf{S}_N)
    \end{equation}
\end{proof}

\section*{Exercise 3.8 $\star \star$}
Consider the linear basis function model in Section 3.1, and suppose
that we already have observed $N$ data points, so that the posterior
distribution over $\mathbf{w}$ is given by ($\ref{eq:3.49}$). This posterior
can be regarded as the prior for the next observation. By considering an additional
data point $(\mathbf{x}_{N + 1}, t_{N + 1})$, and by completing the square in the
exponential, show that the resulting posterior distribution is again
given by ($\ref{eq:3.49}$) but with $\mathbf{S}_N$ replaced by $\mathbf{S}_{N + 1}$
and $\mathbf{m}_N$ replaced by $\mathbf{m}_{N + 1}$.

\vspace{1em}

\begin{proof}
    Our approach will be very similar to the previous exercise. The
    posterior distribution is given by the proportionality relation
    \begin{align*}
        p(\mathbf{w} | \mathbf{t}) 
        &\propto p(\mathbf{w}) p(t_{N + 1} | \mathbf{x}_{N + 1}, \mathbf{w}, \beta^{-1}) \\
        &\propto \mathcal{N}(\mathbf{w} | \mathbf{m}_N, \mathbf{S}_N) 
            \mathcal{N}(t_{N + 1} | \mathbf{w}^T\bm{\phi}(\mathbf{x}_{N + 1}), \beta^{-1})
    \end{align*}
    , so
    \begin{equation}\label{eq:3.8.1}\tag{3.8.1}
        \ln p(\mathbf{w} | \mathbf{t})
        = \ln \mathcal{N}(\mathbf{w} | \mathbf{m}_N, \mathbf{S}_N)
        + \ln \mathcal{N}(t_{N + 1} | \mathbf{w}^T\bm{\phi}(\mathbf{x}_{N + 1}), \beta^{-1})
        + \text{const}
    \end{equation}
    We now compute the log likelihood and keep only the terms depending on
    $\mathbf{w}$ to obtain:
    \begin{align*}
        \ln \mathcal{N}(t_{N + 1} | \mathbf{w}^T\bm{\phi}(\mathbf{x}_{N + 1}), \beta^{-1})
        &= - \frac{\beta}{2} \mathbf{w}^T \bm{\phi}(\mathbf{x}_{N + 1})\bm{\phi}(\mathbf{x}_{N + 1})^T             \mathbf{w}
            -\beta t_{N + 1} \mathbf{w}^T \bm{\phi}(\mathbf{x}_{N + 1})
            + \text{const}
    \end{align*}
    By expanding the square and then doing the same with the prior, we have that:
    \begin{align*}
        \ln \mathcal{N}(\mathbf{w} | \mathbf{m}_N, \mathbf{S}_{N})
        &= -\frac{1}{2} \mathbf{w}^T \mathbf{S}_N^{-1} \mathbf{w}
            + \mathbf{w}^T \mathbf{S}_N^{-1} \mathbf{m}_N 
            + \text{const}
    \end{align*}
    Substituting these results back into ($\ref{eq:3.8.1}$) yields:
    \begin{align*}
        \ln p(\mathbf{w} | \mathbf{t}) 
        &= -\frac{1}{2} \mathbf{w}^T
                \big(\mathbf{S}_N^{-1} - \beta \bm{\phi}(\mathbf{x}_{N + 1})\bm{\phi}
                (\mathbf{x}_{N + 1})^T\big) \mathbf{w}
            + \mathbf{w}^T \big(\mathbf{S}_N^{-1} \mathbf{m}_N 
                - \beta t_{N + 1} \bm{\phi}(\mathbf{x}_{N + 1})\big)
            + \text{const}
    \end{align*}
    which is equivalent to
    \begin{equation*}
        \ln p(\mathbf{w} | \mathbf{t}) = \mathcal{N}(\mathbf{w} | \mathbf{m}_{N + 1}, \mathbf{S}_{N + 1})
    \end{equation*}
    for
    \begin{equation}\label{eq:3.8.2}\tag{3.8.2}
        \mathbf{S}_{N + 1}^{-1}
        = \mathbf{S}_N^{-1} + \beta \bm{\phi}(\mathbf{x}_{N + 1})\bm{\phi}(\mathbf{x}_{N + 1})^T
    \end{equation}
    and
    \begin{equation*}
        \mathbf{m}_{N + 1}
        = \mathbf{S}_{N + 1}\big(\mathbf{S}_N^{-1} \mathbf{m}_N 
             - \beta t_{N + 1} \bm{\phi}(\mathbf{x}_{N + 1})\big)
    \end{equation*}
\end{proof}

\section*{Exercise 3.9 $\star \star$}
Repeat the previous exercise but instead of completing the square by hand,
make use of the general result for linear-Gaussian models given by 
($\ref{eq:2.116}$).

\vspace{1em}

\begin{proof}
    As shown in Section 2.3.3, given a marginal Gaussian distribution for 
    $\mathbf{x}$ and a conditional Gaussian distribution for $\mathbf{y}$
    given $\mathbf{x}$ in the form
    \begin{equation}\label{eq:2.113}\tag{2.113}
        p(\mathbf{x}) = \mathcal{N}(\mathbf{x} | \bm{\mu}, \mathbf{\Lambda}^{-1})
    \end{equation}
    \vspace{-1em}
    \begin{equation}\label{eq:2.114}\tag{2.114}
        p(\mathbf{y} | \mathbf{x}) 
        = \mathcal{N}(\mathbf{y} | \mathbf{Ax} + \mathbf{b}, \mathbf{L}^{-1})
    \end{equation}
    the conditional distribution of $\mathbf{x}$ given $\mathbf{y}$ is given
    by
    \begin{equation}\label{eq:2.116}\tag{2.116}
        p(\mathbf{x} | \mathbf{y}) 
        = \mathcal{N}(\mathbf{x} | \mathbf{\Sigma}
            \{\mathbf{A}^T\mathbf{L}(\mathbf{y} - \mathbf{b}) + \mathbf{\Lambda}\bm{\mu}\}, \mathbf{\Sigma})
    \end{equation}
    where
    \begin{equation}\label{eq:2.117}\tag{2.117}
        \mathbf{\Sigma} = (\mathbf{\Lambda} + \mathbf{A}^T\mathbf{L}\mathbf{A})^{-1}
    \end{equation}
    Our goal is to match these results with our model.
    The prior is given by
    \begin{equation*}
        p(\mathbf{w}) = \mathcal{N}(\mathbf{m}_N, \mathbf{S}_N)
    \end{equation*}
    and the likelihood is
    \begin{equation*}
        p(t_{N + 1} | \mathbf{x}_{N + 1}, \mathbf{w}, \beta^{-1}) 
        = \mathcal{N}(t_{N + 1} | \mathbf{w}^T\bm{\phi}(\mathbf{x}_n), \beta^{-1})
    \end{equation*}
    By comparing those with ($\ref{eq:2.113}$) and ($\ref{eq:2.114}$), we'd have that
    the variables are related as follows: 
    \begin{equation*}
        \mathbf{x} = \mathbf{w}
        \hspace{2em}
        \mathbf{y} = t_{N + 1}
        \hspace{2em}
        \bm{\mu} = \mathbf{m}_N
        \hspace{2em}
        \mathbf{\Lambda}^{-1} = \mathbf{S}_N
        \hspace{2em}
        \mathbf{A} = \bm{\phi}(\mathbf{x}_N)^T
        \hspace{2em}
        \mathbf{b} = 0
        \hspace{2em}
        \mathbf{L^{-1}} = \beta^{-1}
    \end{equation*}
    Therefore, the covariance matrix $\mathbf{\Sigma}$ of the conditional (the $\mathbf{S}_{N + 1}$ 
    of our posterior) will be given by substituting our variables into ($\ref{eq:2.117}$), so
    \begin{equation*}
        \mathbf{S}_{N + 1}^{-1} = \mathbf{S}_N^{-1} + \beta \bm{\phi}(\mathbf{x}_N)\bm{\phi}(\mathbf{x}_N)^T
    \end{equation*}
    The mean can also be easily obtained from $(\ref{eq:2.116})$ as
    \begin{equation*}
        \mathbf{m}_{N + 1}
        = \mathbf{S}_{N + 1}\big(\mathbf{S}_N^{-1} \mathbf{m}_N 
             - \beta t_{N + 1} \bm{\phi}(\mathbf{x}_{N + 1})\big)
    \end{equation*}
\end{proof}

\section*{Exercise 3.10}
By making use of the result $(\ref{eq:2.115})$ to evaluate the integral
in (3.57), verify that the predictive distribution for the Bayesian linear
regression model is given by ($\ref{eq:3.58}$) in which the input-dependent variance is
given by ($\ref{eq:3.59})$.

\vspace{1em}

\begin{proof}
    We've seen in Section 2.3.3 that given a marginal Gaussian distribution
    for $\mathbf{x}$ and a conditional Gaussian distribution for $\mathbf{y}$
    given $\mathbf{x}$ in the forms ($\ref{eq:2.113}$) and ($\ref{eq:2.114}$),
    we have that the marginal distribution of $\mathbf{y}$ is given by
    \begin{equation}\label{eq:2.115}\tag{2.115}
        p(\mathbf{y}) 
        = \mathcal{N}(\mathbf{y} | \mathbf{A}\bm{\mu} + \mathbf{b}, \mathbf{L}^{-1}
            + \mathbf{A\Lambda}^{-1} \mathbf{A}^T) 
    \end{equation}
    Therefore, if we consider the terms under the integral in (3.57),
    we have that
    \[
        p(\mathbf{w} | \mathbf{t}, \mathbf{x}, \alpha, \beta) 
        = \mathcal{N}(\mathbf{w} | \mathbf{m}_N, \mathbf{S}_N)
    \] 
    \[
        p(t | \mathbf{w}, \mathbf{x}, \alpha, \beta) 
        = \mathcal{N}(t | \mathbf{w}^T\bm{\phi}(\mathbf{x}), \beta^{-1}) 
    \] 
    so the marginal distribution of $t$ will be given by ($\ref{eq:2.115}$).
    Our goal is to find the parameters of this distribution. By considering
    the notation used in $(\ref{eq:2.113})$, $(\ref{eq:2.114})$, and $(\ref{eq:2.115}$),
    we'd have that
    \[
        \bm{\mu} = \mathbf{m}_N 
        \hspace{1em}
        \mathbf{S}_N = \mathbf{\Lambda}^{-1}
        \hspace{1em}
        \mathbf{A} = \bm{\phi}(\mathbf{x})^T
        \hspace{1em}
        \mathbf{b} = 0
        \hspace{1em}
        \mathbf{L}^{-1} = \beta^{-1}
    \] 
    Finally, by substituting our values into $(\ref{eq:2.115})$, it is 
    straightforward to see that the predictive distribution for the Bayesian 
    linear regression model is given by
    \begin{equation}\label{eq:3.58}\tag{3.58}
        p(t | \mathbf{t}, \mathbf{x}, \alpha, \beta) 
        = \mathcal{N}\big(t | \bm{\phi}(\mathbf{x})^T\mathbf{m}_N, \sigma^2_N(\mathbf{x})\big)
    \end{equation}
    where the input-dependent variance is given by
    \begin{equation}\label{eq:3.59}\tag{3.59}
        \sigma_{N}^2(\mathbf{x})
        = \frac{1}{\beta} + \bm{\phi}(\mathbf{x})\mathbf{S}_N\bm{\phi}(\mathbf{x})^T
    \end{equation}
\end{proof}

\section*{Exercise 3.11}
We have seen that, as the size of a data set increases, the uncertainty associated
with the posterior distribution over model parameters decreases. Make use of the
matrix identity (Appendix C)
\begin{equation}\label{eq:3.110}\tag{3.110}
    \big(\mathbf{M} + \mathbf{v}\mathbf{v}^T\big)^{-1}
    = \mathbf{M}^{-1} - \frac{(\mathbf{M}^{-1}\mathbf{v})\big(\mathbf{v}^T\mathbf{M}^{-1}\big)}
        {1 + \mathbf{v}^T\mathbf{M}^{-1}\mathbf{v}}
\end{equation}
to show that the uncertainty $\sigma^2_{N + 1}(\mathbf{x})$ associated with the
linear regression function given by (\ref{eq:3.59}) satisfies
\begin{equation}\label{eq:3.111}\tag{3.111}
    \sigma^2_{N + 1}(\mathbf{x}) \leq \sigma^2_N(\mathbf{x})
\end{equation}

\vspace{1em}

\begin{proof}
    By using (\ref{eq:3.59}) and then ($\ref{eq:3.8.2}$) we have that:
     \[
         \sigma_{N + 1}^2(\mathbf{x})
         = \frac{1}{\beta} + \bm{\phi}(\mathbf{x})^T\mathbf{S}_{N + 1}\bm{\phi}(\mathbf{x})
         = \frac{1}{\beta} \bm{\phi}(\mathbf{x})^T
         \bigg[\mathbf{S}_N^{-1} + \beta \bm{\phi}(\mathbf{x}_N)\bm{\phi}(\mathbf{x}_N)^T\bigg]^{-1}
                \bm{\phi}(\mathbf{x})
    \] 
    We apply ($\ref{eq:3.110}$) with $\mathbf{M} = \mathbf{S}_N^{-1}$ 
    and $\mathbf{v} = \beta^{1/2}\bm{\phi}(\mathbf{x})$ and get that
    \begin{align*}
        \sigma^{2}_{N + 1}(\mathbf{x})
        &= \frac{1}{\beta} + \bm{\phi}(\mathbf{x})^T
            \bigg[\mathbf{S}_N 
            - \frac{\beta\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)\bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N}
            {1 + \beta \bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)}\bigg]
        \bm{\phi}(\mathbf{x}) \\
        &= \frac{1}{\beta} + \bm{\phi}(\mathbf{x})^T\mathbf{S}_N\bm{\phi}(\mathbf{x})
            - \bm{\phi}(\mathbf{x})^T\frac{\mathbf{S}_N\bm{\phi}
            (\mathbf{x}_N)\bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N}
            {\frac{1}{\beta} + \bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)}
            \bm{\phi}(\mathbf{x}) \\
        &= \sigma_{N}^2(\mathbf{x})
            - \bm{\phi}(\mathbf{x})^T
            \frac{\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)\bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N}
            {\frac{1}{\beta} + \bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)}
            \bm{\phi}(\mathbf{x}) \\
    \end{align*}
    Therefore,
    \begin{equation}\label{eq:3.11.1}\tag{3.11.1}
        \sigma_N^2(\mathbf{x}) - \sigma_{N + 1}^2(\mathbf{x})
        = \bm{\phi}(\mathbf{x})^T
            \frac{\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)\bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N}
            {\frac{1}{\beta} + \bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)}
            \bm{\phi}(\mathbf{x})
    \end{equation}
    Since $\mathbf{S}_N$ is a precision matrix, it is symmetric, so:
    \[
        \mathbf{S}_N\bm{\phi}(\mathbf{x}_N)\bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N
        = \big(\bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N\big)^T\bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N
        = ||\mathbf{S}_N \bm{\phi}(\mathbf{x}_N)||^2 \geq 0
    \] 
    Even more, because $\mathbf{S}_N$ is a precision matrix, it is 
    positive semidefinite. By using this and the fact that the noise
    precision constant $\beta$ is positive, we have that:
    \[
        \frac{1}{\beta} + \bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N\bm{\phi}(\mathbf{x}_N) \geq 0
    \] 
    Hence, we finally have that 
    \[
        \bm{\phi}(\mathbf{x})^T
        \frac{\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)\bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N}
        {\frac{1}{\beta} + \bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)}
        \bm{\phi}(\mathbf{x}) 
        = \frac{\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)\bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N}
          {\frac{1}{\beta} + \bm{\phi}(\mathbf{x}_N)^T\mathbf{S}_N\bm{\phi}(\mathbf{x}_N)}
          ||\bm{\phi}(\mathbf{x})||^2 \geq 0
    \] 
    which, by $(\ref{eq:3.11.1})$, becomes equivalent to $(\ref{eq:3.111})$.
\end{proof}

\section*{Exercise 3.12}
We saw in Section 2.3.6 that the conjugate prior for a Gaussian distribution
with unknown mean and unknown precision (inverse variance) is a normal-gamma
distributuion. This property also holds for the case of the conditional
Gaussian distribution $p(t | \mathbf{x}, \mathbf{w}, \beta)$ of the linear
regression model. If we consider the likelihood function (3.10), then
the conjugate prior for $\mathbf{w}$ and $\beta$ is given by
\begin{equation}\label{eq:3.112}\tag{3.112}
    p(\mathbf{w}, \beta) 
    = \mathcal{N}(\mathbf{w} | \mathbf{m}_0, \beta^{-1}\mathbf{S}_0) \text{Gam}(\beta | a_0, b_0)
\end{equation}
Show that the corresponding posterior distribution takes the same functional
form, so that 
\begin{equation}\label{eq:3.113}\tag{3.113}
    p(\mathbf{w}, \beta | \mathbf{t}) 
    = \mathcal{N}(\mathbf{w} | \mathbf{m}_N, \beta^{-1} \mathbf{S}_N) \text{Gam}(\beta | a_N, b_N)
\end{equation}
and find expressions for the posterior parameters $\mathbf{m}_N$, $\mathbf{S}_N$, $a_N$, 
and $b_N$.

\vspace{1em}

\begin{proof}
    We have that
    \begin{align*}
        p(\mathbf{w}, \beta | \mathbf{t}) 
        &\propto p(\mathbf{w}, \beta) p(\mathbf{t} | \mathbf{w}, \beta) \\
        &\propto \mathcal{N}(\mathbf{w} | \mathbf{m}_0, \beta^{-1}\mathbf{S}_0) 
            \text{Gam}(\beta | a_0, b_0)
            \prod_{n=1}^N \mathcal{N}(t_n | \mathbf{w}^T\bm{\phi}(\mathbf{x}_n), \beta^{-1})
    \end{align*}
    so 
    \[
        \ln p(\mathbf{w}, \beta | \mathbf{t}) 
        = \ln \mathcal{N}(\mathbf{w} | \mathbf{m}_0, \beta^{-1}\mathbf{S}_0) 
            + \ln \text{Gam}(\beta | a_0, b_0)
            + \ln \prod_{n=1}^N \mathcal{N}(t_n | \mathbf{w}^T\bm{\phi}(\mathbf{x}_n), \beta^{-1})
            + \text{const}
    \] 
    We decompose each logarithm, this time keeping each term. 
    The log likelihood is derived like in Exercise 3.7,
    that is:
    \[
        \ln \prod_{n=1}^N \mathcal{N}(t_n | \mathbf{w}^T\bm{\phi}(\mathbf{x}_n), \beta^{-1})
        = -\frac{\beta}{2} \mathbf{w}^T\mathbf{\Phi}^T\mathbf{\Phi}\mathbf{w}
            -\beta \mathbf{w}^T \mathbf{\Phi}^T \mathbf{t} 
            -\frac{\beta}{2} \mathbf{t}^T\mathbf{t}
            
    \] 
    The logarithms of factors in the prior are given by:
    \[
        \ln \mathcal{N}(\mathbf{w} | \mathbf{m}_0, \beta^{-1}\mathbf{S}_0) 
        &= -\frac{\beta}{2} \mathbf{w}^T\mathbf{S}_0^{-1}\mathbf{w} 
        + \beta \mathbf{w}^T\mathbf{S}_0^{-1}\mathbf{m}_0 
            - \frac{\beta}{2} \mathbf{m}_0^T\mathbf{S}_0^{-1}\mathbf{m}_0
    \] 
    \[
        \ln \text{Gam}(\beta | a_0, b_0)
        = -\ln \Gamma(a_0) + a_0\ln b_0 + a_0 \ln \beta - \ln \beta - b_0 \beta
    \] 
    Now, the log of the posterior is given by:
    \begin{align*}
        \ln p(\mathbf{w}, \beta | \mathbf{t})
        = -&\frac{1}{2} \mathbf{w}^T
            (\beta \mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^T\mathbf{\Phi}) \mathbf{w}
            + \mathbf{w}^T(\beta \mathbf{S}_0^{-1} \mathbf{m}_0 - \beta \mathbf{\Phi}^T \mathbf{t})
            - \frac{\beta}{2} \mathbf{t}^T\mathbf{t}
            - \frac{\beta}{2} \mathbf{m}_0^T\mathbf{S}_0^{-1}\mathbf{m}_0 \\
        +& (a_0 - 1)\ln \beta - b_0 \beta + \text{const}
    \end{align*}
    The covariance matrix of the posterior is easily found
    from the quadratic term, that is:
     \[
         \mathbf{S}_N^{-1} = \beta \mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^T\mathbf{\Phi}
    \] 
    The mean is obtained from the linear term by using the fact that
    \[
        \mathbf{w}^T \mathbf{S}_N^{-1} \mathbf{m}_N
        = \mathbf{w}^T(\beta \mathbf{S}_0^{-1} \mathbf{m}_0 - \beta \mathbf{\Phi}^T \mathbf{t})
    \] 
    so 
    \[
        \mathbf{m}_N = \mathbf{S}_N 
            (\beta \mathbf{S}_0^{-1} \mathbf{m}_0 - \beta \mathbf{\Phi}^T \mathbf{t})
    \] 
    Finally, the constant term with respect to $\mathbf{w}$ will give us the
    parameters of the Gamma distribution.
\end{proof}
