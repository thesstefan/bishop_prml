\chapter{Linear Models for Regression}

\section*{Exercise 3.1 $\star$}
Show that the $\tanh$ function and the logistic
sigmoid function ($\ref{eq:3.6}$) are related by 
\begin{equation}\label{eq:3.100}\tag{3.100}
    \tanh(a) = 2\sigma(2a) - 1
\end{equation}
Hence show that a general linear combination of logistic sigmoid functions
of the form
\begin{equation}\label{eq:3.101}\tag{3.101}
    y(x, \mathbf{w}) = w_0 + \sum_{j=1}^{M} w_j \sigma\bigg(\frac{x-\mu_j}{s}\bigg)
\end{equation}
is equivalent to a linear combination of $\tanh$ functions of the form
\begin{equation}\label{eq:3.102}\tag{3.102}
    y(x, \mathbf{u}) = u_0 + \sum_{j=1}^{M} u_j \tanh\bigg(\frac{x-\mu_j}{2s}\bigg)
\end{equation}
and find expressions to relate the new parameters $\{u_0, \ldots, u_M\}$
to the original parameters $\{w_0, \ldots, w_M\}$.

\vspace{1em}

\begin{proof}
    The logistic sigmoid function is given by
    \begin{equation}\label{eq:3.6}\tag{3.6}
        \sigma(x) = \frac{1}{1 + \exp(-x)}
    \end{equation}
    and the $\tanh$ function is given by
    \begin{equation}
        \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{e^{2x} - 1}{e^{2x} + 1}
    \end{equation}
    By starting from the right-hand side of ($\ref{eq:3.100}$) and then 
    using the fact that $\tanh$ is odd, we obtain 
    \begin{equation}\tag{3.100}
        2\sigma(2a) - 1 = \frac{2}{e^{-2a}} - 1 = \frac{1 - e^{-2a}}{1 + e^{-2a}}
        = -\tanh(-a) = \tanh(a)
    \end{equation}
    Now, we can express the logistic sigmoid functions as
    \[
        \sigma(x) = \frac{1}{2}\tanh\frac{x}{2} + \frac{1}{2}
    \] 
    By substituting this in (\ref{eq:3.101}), we have that
    \begin{align*}
        y(x, \mathbf{w}) = w_0 + \frac{M}{2} +  
        \sum_{j=1}^{M} \frac{w_j}{2} \tanh\bigg(\frac{x - \mu_j}{2s}\bigg)
        = y(x, \mathbf{u})
    \end{align*}
    where
    \[
        u_0 = w_0 + \frac{M}{2} 
        \hspace{5em}
        u_j = \frac{1}{2} w_j, j \geq 1
    \] 
    Therefore, we proved that ($\ref{eq:3.101}$) is equivalent to ($\ref{eq:3.102}$).
\end{proof}

\section*{Exercise 3.2 $\star \star$}
Show that the matrix 
\begin{equation}\label{eq:3.103}\tag{3.103}
    \mathbf{\Phi}(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T
\end{equation}
takes any vector $\mathbf{v}$ and projects it onto the space spanned by the columns
of $\mathbf{\Phi}$. Use this result to show that the least-squares solution 
($\ref{eq:3.15}$) corresponds to an orthogonal projection of the vector $\mathbf{t}$ onto
the manifold $\mathcal{S}$ as shown in Figure 3.2.

\vspace{1em}

\begin{proof}
    Let $\mathbf{p}$ be the projection of $\mathbf{v}$ onto the space spanned by the
    columns of $\mathbf{\Phi}$. We then have that $\mathbf{p}$ is contained by
    the space, so $\mathbf{p}$ can be written as a linear combination
    of the columns of $\mathbf{\Phi}$, i.e. there exists $\mathbf{x}$
    such that  $\mathbf{p} = \mathbf{\Phi}\mathbf{x}$. By using this
    and the fact that $\mathbf{p} - \mathbf{v}$ is orthogonal
    to the space, we have that
    \begin{align*}
        \mathbf{\Phi}^T(\mathbf{p} - \mathbf{v}) &= \mathbf{0} \\
        \mathbf{\Phi}^T(\mathbf{\Phi}\mathbf{x} - \mathbf{v}) &= \mathbf{0} \\
        \mathbf{\Phi}^T\mathbf{\Phi}\mathbf{x} &= \mathbf{\Phi}^T\mathbf{v} \\
        \mathbf{x} &= (\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\mathbf{v}
    \end{align*}
    and since $\mathbf{p} = \mathbf{\Phi}\mathbf{x}$, this proves our hypothesis,
    i.e. 
    \[
        \mathbf{p} = \mathbf{\Phi}(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\mathbf{v}
    \] 

    This translates directly to the least-squares geometry described 
    in Section 3.1.3, where the manifold $\mathcal{S}$ is the space spanned
    by the columns of $\mathbf{\Phi}$. From what we proved above,
    the projection of $\mathbf{t}$ onto the manifold $\mathcal{S}$ 
    is given by $\mathbf{y} = \mathbf{\Phi}\mathbf{w}_{\text{ML}}$,
    where 
    \begin{equation}\label{eq:3.15}\tag{3.15}
        \mathbf{w}_{\text{ML}} = (\mathbf{\Phi}^T \mathbf{\Phi})^{-1} \mathbf{\Phi}^T \mathbf{t}
    \end{equation}
    is the least-squares solution.
\end{proof}

\section*{Exercise 3.3 $\star$}
Consider a data set in which each data point $t_n$ is associated with a weighting
factor $r_n > 0$, so that the sum of squares error function becomes
\begin{equation}\label{eq:3.104}\tag{3.104}
    E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
    r_n\{t_n - \mathbf{w}^T \bm{\phi}(\mathbf{x}_n)\}^2
\end{equation}
Find an expression for the solution $\mathbf{w}^\star$ that minimizes this error function.
Give two alternative interpretations of the weighted sum-of-squares error function
in terms of (i) data dependent noise variance and (ii) replicated data points.

\vspace{1em}

\textbf{Method 1.}

\begin{proof}
    Since the least-squares error function is convex,
    the function is minimized in its only critical 
    point. Similarly to (3.13), the derivative is given by:
    \begin{align*}
        \pdv{\mathbf{w}} E_D(\mathbf{w})
        &= \frac{1}{2} \sum_{n=1}^{N} r_n \bigg(\pdv{\mathbf{w}} 
        \{t_n - \mathbf{w}^T \bm{\phi}(\mathbf{x}_n)\}^2\bigg) \\
        &= \sum_{n=1}^N r_n \{\mathbf{w}^T \bm{\phi}(\mathbf{x}_n) - t_n\} 
        \bm{\phi}(\mathbf{x}_n)^T \\
        &= \mathbf{w}^T 
        \bigg(\sum_{i=1}^{N} r_n \bm{\phi}(\mathbf{x}_n)
        \bm{\phi}(\mathbf{x}_n)^T\bigg) -
        \sum_{n=1}^{N} r_n t_n \bm{\phi}(\mathbf{x}_n)^T
    \end{align*}
    By defining the matrix $R = \text{diag}(r_1, r_2, \ldots, r_n)$
    and then setting the derivative to 0, we obtain the equality
    \[
        \mathbf{w}^T \mathbf{\Phi}R\mathbf{\Phi}^T
        = \mathbf{t}^TR\mathbf{\Phi} 
    \] 
    which gives the weighted least-squares solution (we get the
    column vector form):
    \[
        \mathbf{w}^\star = (\mathbf{\Phi}^TR\mathbf{\Phi})^{-1} \mathbf{\Phi}^T R \mathbf{t} 
    \] 
\end{proof}

\textbf{Method 2.}
\begin{proof}

    We define the diagonal matrices $R = \text{diag}(r_1, r_2, \ldots, r_n)$ and
    $R^{1/2} = \text{diag}(\sqrt{r_1}, \sqrt{r_2}, \ldots, \sqrt{r_n})$
    such that $R^{1/2}R^{1/2} = R$. We notice that we can rewrite ($\ref{eq:3.104}$) as:
    \[
        E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
        \big(\sqrt{r_n}\{t_n - \mathbf{w}^T \bm{\phi}(\mathbf{x}_n)\}\big)^2
    \] 
    which we can translate into matrix notation as:
    \[
        E_D(\mathbf{w}) = \frac{1}{2} \big(R^{1/2}(\mathbf{t} - \mathbf{\Phi}\mathbf{w})\big)^T
        \big(R^{1/2}(\mathbf{t} - \mathbf{\Phi}\mathbf{w})\big)
    \] 
    Since the least-squares error function is convex, the function
    is minimized in its only critical point. The derivative is given by
    \begin{align*}
        \pdv{\mathbf{w}} E_D(\mathbf{w})
        &= -\mathbf{\Phi}^T(R^{1/2})^T(R^{1/2}\mathbf{t} - R^{1/2}\mathbf{\Phi}\mathbf{w}) \\
        &= \mathbf{\Phi}^TR\mathbf{\Phi}\mathbf{w} - \mathbf{\Phi}^TR\mathbf{t}
    \end{align*}
    By setting it to 0, we obtain the solution that minimizes the weighted
    least-squares error function:
    \[
        \mathbf{w}^\star = (\mathbf{\Phi}^TR\mathbf{\Phi})^{-1} \mathbf{\Phi}^T R \mathbf{t} 
    \] 
\end{proof}

\section*{Exercise 3.4 $\star$}
Consider a linear model of the form
\begin{equation}\label{eq:3.105}\tag{3.105}
    y(\mathbf{x}, \mathbf{w}) = w_0 + \sum_{i=1}^{D} w_ix_i
\end{equation}
together with a sum-of-squares error function of 
the form
\begin{equation}\label{eq:3.106}\tag{3.106}
    E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N}
    \{y(\mathbf{x}_n, \mathbf{w}) - t_n\}^2
\end{equation}
Now suppose that Gaussian noise $\epsilon_i$ with zero
mean and variance $\sigma^2$ is added independently
to each of the input variables $x_i$. By making
use of $\mathbb{E}[\epsilon_i] = 0$ and 
$\mathbb{E}[\epsilon_i\epsilon_j] = \delta_{ij}\sigma^2$,
show that minimizing $E_D$ averaged over the noise 
distribution is equivalent to minimizing the 
sum-of-squares error for noise-free input variables
with the addition of a weight-decay regularization term,
in which the bias parameter $w_0$ is omitted from
the regularizer.

\vspace{1em}

\begin{proof}
    Let the noise-free input variables be denoted by $\mathbf{x}^*$, such that
    $x_i = x_i^* + \epsilon_i$. ($\ref{eq:3.105}$) will then be equivalent to
    \[
        y(\mathbf{x}, \mathbf{w}) = w_0 + \sum_{i=1}^{D} w_ix_i^* + \sum_{i=1}^{D} w_i\epsilon_i
        = y(\mathbf{x}^*, \mathbf{w}) + \sum_{i=1}^{D} w_i\epsilon_i
    \] 
    Now, we aim to find the expression of $E_D$ averaged
    over the noise distribution, that is:
    \begin{align*}
        \mathbb{E}[E_D(\mathbf{w})]
        = \frac{1}{2} \sum_{n=1}^{N} \{\mathbb{E}[y(\mathbf{x}_n, \mathbf{w})^2]
        - 2t_n\mathbb{E}[y(\mathbf{x}_n, \mathbf{w})] + t_n^2\}
    \end{align*}
    The individual expectations are straightforward to compute. Since
    $\mathbb{E}[\epsilon_i] = 0$, we have that
    \begin{align*}
        \mathbb{E}[y(\mathbf{x}_n, \mathbf{w})] 
        = \mathbb{E}[y(\mathbf{x}^*, \mathbf{w})] + \sum_{i=1}^{D} w_i\mathbb{E}[\epsilon_i]
        = y(\mathbf{x}^*, \mathbf{w})
    \end{align*}
    Also, $\mathbb{E}[\epsilon_i\epsilon_j] = \delta_{ij} \sigma^2$, so
    \begin{align*}
        \mathbb{E}[y(\mathbf{x}_n, \mathbf{w})^2] 
        &= \mathbb{E}\bigg[y(\mathbf{x}^*, \mathbf{w})^2 
            + 2y(\mathbf{x}^*, \mathbf{w}) \sum_{i=1}^{D} w_i\epsilon_i
            + \bigg(\sum_{i=1}^{D} w_i \epsilon_i\bigg)^2\bigg] \\
        &= y(\mathbf{x}^*, \mathbf{w})^2 + \sum_{i=1}^{D} w_i^2 \mathbb{E}[\epsilon_i^2]
            + 2\sum_{i=1}^{D} \sum_{j=i+1}^{D} w_iw_j \mathbb{E}[\epsilon_i\epsilon_j] \\
        &= y(x^*, \mathbf{w})^2 + \sigma\sum_{n=1}^{D} w_i^2
    \end{align*}
    Therefore, we have that
    \begin{align*}
        \mathbb{E}[E_D({\mathbf{w}})]
        = \frac{1}{2} \sum_{n=1}^{D} \{y(\mathbf{x}_n^*, \mathbf{w}) - t_n\}^2
        + \frac{N\sigma}{2} \sum_{n=1}^{D} w_i^2
    \end{align*}
    which shows that $E_D$ averaged over the noise distribution
    is equivalent to the regularized least-squares error function
    with $\lambda = N\sigma$. Hence, since the expressions are equivalent,
    minimizing them is also equivalent, proving our hypothesis.
\end{proof}

\section*{Exercise 3.5 $\star$}
Using the technique of Lagrange multipliers, discussed in Appendix E,
show that minimization of the regularized error function (3.29)
is equivalent to minimizing the unregularized sum-of-squares error (3.12)
subject to the constraint (3.30). Discuss the relationship
between the parameters $\eta$ and $\lambda$.

\begin{proof}
    To minimize the unregularized sum-of-squares error (3.12) subject to
    the constraint (3.30), is equivalent to minimizing the Lagrangian
    \[
        L(\mathbf{x}, \lambda) = 
        \frac{1}{2}\sum_{n=1}^{N} \{y(\mathbf{x}_n, \mathbf{w}) - t_n\}^2 
        - \lambda\bigg(\eta - \sum_{j=1}^{M} |w_j|^q\bigg)
    \] subject to the KKT conditions (see E.9, E.10, E.11 in Appendix E).
    Our Lagrangian and the regularized sum-of-squares error have the same
    dependency over $\mathbf{w}$, so their minimization is equivalent.
    By following (E.11), we have that
     \[
         \lambda\bigg(\eta - \sum_{j=1}^{M} |w_j|^q\bigg) = 0
    \] 
    which means that if $\mathbf{w}^\star(\lambda)$ is the solution of minimization
    for a fixed $\lambda > 0$, we then have that 
     \[
         \eta = \sum_{j=1}^{M} |w^\star(\lambda)_j|^q
    \] 
\end{proof}

\section*{Exercise 3.6 $\star$}
Consider a linear basis function regression model
for a multivariate target variable $\mathbf{t}$ having 
a Gaussian distribution of the form
\begin{equation}\label{eq:3.107}\tag{3.107}
    p(\mathbf{t} | \mathbf{W}, \mathbf{\Sigma})
    = \mathcal{N}(\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{W}), \mathbf{\Sigma})
\end{equation}
where 
\begin{equation}\label{eq:3.108}\tag{3.108}
    \mathbf{y}(\mathbf{x}, \mathbf{W}) = \mathbf{W}^T \bm{\phi}(\mathbf{x})
\end{equation}
together with a training data set compromising input
basis vectors $\bm{\phi}(\mathbf{x}_n)$ and corresponding
target vectors $\mathbf{t}_n$, with $n = 1, \ldots, N$. Show
that the maximum likelihood solution $\mathbf{W}_{\text{ML}}$ for
the parameter matrix $\mathbf{W}$ has the property that each 
column is given by an expression of the form ($\ref{eq:3.15}$),
which was the solution for an isotropic noise distribution.
Note that this is independent of the covariance matrix
$\mathbf{\Sigma}$. Show that the maximum likelihood solution
for $\mathbf{\Sigma}$ is given by
\begin{equation}\label{eq:3.109}\tag{3.109}
    \mathbf{\Sigma} = \frac{1}{N} \sum_{n=1}^{N} 
    \big(\mathbf{t}_n - \mathbf{W}_{\text{ML}}^T\bm{\phi}(\mathbf{x}_n)\big)
    \big(\mathbf{t}_n - \mathbf{W}_{\text{ML}}^T\bm{\phi}(\mathbf{x}_n)\big)^T
\end{equation}

\vspace{1em}

\begin{proof}
    Similarly to what we did in Section 3.1.5, we combine
    the set of target vectors into a matrix $\mathbf{T}$ of
    size $N \cross K$ such that the $n^{\text{th}}$ row
    is given by $\mathbf{t}_n^T$. We do the same for
    $\mathbf{X}$. The log likelihood 
    function is then given by
    \begin{align*}
        \ln p(\mathbf{T} | \mathbf{X}, \mathbf{W}, \mathbf{\Sigma})
        &= \ln \prod_{n = 1}^N 
            \mathcal{N}(\mathbf{t}_n | \mathbf{W}^T\bm{\phi}(\mathbf{x}_n), \mathbf{\Sigma}) \\
        &= \sum_{n=1}^{N} 
            \ln \mathcal{N}(\mathbf{t}_n | \mathbf{W}^T\bm{\phi}(\mathbf{x}_n), \mathbf{\Sigma}) \\
        &= \sum_{n=1}^{N} \ln \bigg[\frac{1}{(2\pi)^{K/2}|\mathbf{\Sigma}|^{1/2}}
            \exp\big\{\big(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big)^T
            \mathbf{\Sigma}^{-1}(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big)
        \big\}\bigg] \\
        &= -\frac{NK}{2} \ln(2\pi) - \frac{1}{2} \ln|\mathbf{\Sigma}| 
            + \sum_{n=1}^{N} \big(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big)^T
            \mathbf{\Sigma}^{-1}(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big)
    \end{align*}
    Our goal is to maximizise this function with respect to $\mathbf{W}$.
    We take the derivative of the likelihood and use the fact that
    $\mathbf{\Sigma}^{-1}$ is symmetric and (88) from the 
    \href{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}{matrix cookbook}
    to obtain:
    \begin{align*}
        \pdv{\mathbf{W}} \ln p(\mathbf{T} | \mathbf{X}, \mathbf{W}, \mathbf{\Sigma})
        &= \sum_{n=1}^{N} \pdv{\mathbf{W}} 
            \big(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big)^T
            \mathbf{\Sigma}^{-1}(\mathbf{t}_n - \mathbf{W}^T \bm{\phi}(\mathbf{x}_n)\big) \\
        &= -2\mathbf{\Sigma}^{-1} \sum_{n=1}^{N} \big(\mathbf{t}_n - \mathbf{W}^T\bm{\phi}(\mathbf{x}_n)\big)
            \bm{\phi}(\mathbf{x}_n)^T
    \end{align*}
    By setting the derivative equal to 0, we find the maximum likelihood
    solution for $\mathbf{W}$:
    \begin{align*}
            -2\mathbf{\Sigma}^{-1} \sum_{n=1}^{N} \big(\mathbf{t}_n 
            - \mathbf{W}_{\text{ML}}^T\bm{\phi}(\mathbf{x})\big) \bm{\phi}(\mathbf{x})^T 
        &= 0 \\
            \mathbf{\Sigma}^{-1} \sum_{n=1}^{N} \mathbf{t}_n \bm{\phi}(\mathbf{x}_n)^T
        &= \mathbf{\Sigma}^{-1} \mathbf{W}_{\text{ML}}^T 
            \sum_{n=1}^{N} \bm{\phi}(\mathbf{x}_n)\bm{\phi}(\mathbf{x}_n)^T \\
            \mathbf{\Sigma}^{-1} \mathbf{T}^T \mathbf{\Phi} 
        &= \mathbf{\Sigma}^{-1} \mathbf{W}_{\text{ML}}^T \mathbf{\Phi}^T\mathbf{\Phi} \\
            \mathbf{\Phi}^T\mathbf{T}\mathbf{\Sigma}^{-1}
        &= \mathbf{\Phi}^T\mathbf{\Phi}\mathbf{W}_{\text{ML}}\mathbf{\Sigma}^{-1}
    \end{align*}
    Note that $\mathbf{\Sigma}^{-1}$ reduces and we finally get that:
     \[
         \mathbf{W}_{\text{ML}} = (\mathbf{\Phi}^T\mathbf{\Phi})^{-1} \mathbf{\Phi}^T\mathbf{T}
    \] 
    Now, let $A, B$ be two matrices of size $N \cross M$ and let $b_1, b_2, \ldots, b_N$ 
    be the column vectors of $B$. One could easily prove that
     \[
         AB = A \big(b_1 \hspace{0.25em} b_2 \hspace{0.25em} \ldots \hspace{0.25em} b_N\big)
         = \big(Ab_1 \hspace{0.25em} Ab_2 \hspace{0.25em} \ldots \hspace{0.25em} Ab_N\big)
    \] 
    By using this for our case, i.e. to find the columns of $\mathbf{W}_{\text{ML}}$,
    we'd find that they are of the form $(\ref{eq:3.15})$, i.e. the $n^{\text{th}}$
    column of $\mathbf{W}_{\text{ML}}$ is given by
    \[
        \mathbf{W}_{\text{ML}}^{(n)} =
        (\mathbf{\Phi}^T\mathbf{\Phi})^{-1} \mathbf{\Phi}^T\mathbf{T}^{(n)}
    \] 
    where $\mathbf{T}^{(n)}$ is the $n^{\text{th}}$ column of $\mathbf{T}$.
\end{proof}
