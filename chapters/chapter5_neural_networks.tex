\chapter{Neural Networks}

\section*{Exercise 5.1 $\star \star$}
Consider a two-layer network function of the form $\eqref{eq:5.7}$ 
in which the hidden-unit nonlinear activation functions $h(\cdot)$ 
are given by logistic sigmoid functions of the form
\begin{equation}\label{eq:5.191}\tag{5.191}
    \sigma(a) = \frac{1}{1 + \exp(-a)}.
\end{equation}
Show that there exists an equivalent network, which computes 
exactly the same function, but with hidden activation functions
given by $\tanh(a)$ where the $\tanh$ function is defined by
(5.59). Hint: first find the relation between $\sigma(a)$ 
and $\tanh(a)$, and then show that the parameters of the two
networks differ by linear transformations.

\vspace{1em}

\begin{proof}
    The considered two-layer network has the form
    \begin{equation}\label{eq:5.7}\tag{5.7}
        y_k(\mathbf{x}, \mathbf{w}) 
        = \sigma\bigg(\sum_{j=1}^{M} w_{kj}^{(2)} 
        h\bigg(\sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)}\bigg) + w_{k0}^{(2)}\bigg)
    \end{equation}
    Now, we've proved in Exercise 3.1 that 
    \[
        \sigma(x) = \frac{1}{2} \tanh \frac{x}{2} + \frac{1}{2}
    \] 
    Therefore, we can rewrite $y_k$ as
    \begin{align*}
        y_k(\mathbf{x}, \mathbf{w})
        &= \sigma\bigg(\frac{1}{2}\sum_{j=1}^{M} w_{kj}^{(2)} 
            \tanh\bigg(\frac{1}{2}\sum_{i=1}^{D} w_{ji}^{(1)}x_i + \frac{1}{2} w_{j0}^{(1)}\bigg)
            + \frac{1}{2} \sum_{j=1}^{M} w_{kj}^{(2)} + w_{k0}^{(2)}\bigg) \\
        &= \sigma\bigg(\sum_{j=1}^{M} \omega_{kj}^{(2)} 
        h\bigg(\sum_{i=1}^{D} \omega_{ji}^{(1)}x_i + \omega_{j0}^{(1)}\bigg) + \omega_{k0}^{(2)}\bigg)
    \end{align*}
    where 
    \newline
    \begin{minipage}[b]{0.25\textwidth}
    \[
       \omega_{ji}^{(1)} = \frac{1}{2} w_{ji}^{(1)}
    \]
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
    \[
        \omega_{j0}^{(1)} = \frac{1}{2} w_{j0}^{(1)}
    \] 
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
    \[
        \omega_{kj}^{(2)} = \frac{1}{2} w_{kj}^{(2)}
    \]
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
        \[
            \omega_{k0}^{(2)} = \frac{1}{2}\sum_{j = 1}^M w_{kj}^{(2)} + w_{k0}^{(2)}
        \] 
    \end{minipage}
    Both new parameter sets can be obtained as linear transformations of the old ones,
    so there exists an equivalent two-layer network using $\tanh$ hidden activation functions,
    but different parameters.
\end{proof}

\section*{Exercise 5.2 $\star$}
Show that maximizing the likelihood function under the conditional 
distribution \eqref{eq:5.16} for a multioutput network is equivalent to minimizing
the sum-of-squares error function (5.11).

\vspace{1em}

\begin{proof}
    The likelihood function is given by 
    \[
        p(\mathbf{T} | \mathbf{X}, \mathbf{w}, \beta)
        = \prod_{n = 1}^N p(\mathbf{t}_n | \mathbf{x}_n, \mathbf{w}, \beta)
    \] 
    The target variables are assumed to be distributed normally
    \begin{equation}\label{eq:5.16}\tag{5.16}
        p(\mathbf{t}_n | \mathbf{x}_n, \mathbf{w}, \beta) 
        = \mathcal{N}(\mathbf{t}_n | \mathbf{y}(\mathbf{x}_n, \mathbf{w}), \beta^{-1}\mathbf{I})
    \end{equation}
    and since
    \[
        \ln \mathcal{N}(\mathbf{t}_n | y(\mathbf{x}_n, \mathbf{w}), \beta^{-1}\mathbf{I})
        = -\frac{N}{2} \ln \beta - \frac{NK}{2} \ln(2\pi) - \frac{\beta}{2} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2
    \] 
    the negative log-likelihood is given by
    \[
        -\ln p(\mathbf{t} | \mathbf{X}, \mathbf{w}, \beta)
        = \frac{\beta}{2}\sum_{n=1}^{N} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2 + \text{const}
    \] 
    where we grouped the terms that don't depend on $\mathbf{w}$ under the constant term.
    Maximization of the likelihood function is equivalent to minimizing the negative 
    log-likelihood.
    Therefore, one can easily find that this is equivalent to minimizing the error function
    \begin{equation}\label{eq:5.11}\tag{5.11}
        E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2
    \end{equation}
\end{proof}

\section*{Exercise 5.3 $\star \star$}
Consider a regression problem involving multiple target variables in which
it is assumed that the distribution of the targets, conditioned on the input
vector $\mathbf{x}$, is a Gaussian of the form
\begin{equation}\label{eq:5.192}\tag{5.192}
    p(\mathbf{t} | \mathbf{x}, \mathbf{w}) 
    = \mathcal{N}(\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{w}), \mathbf{\Sigma})
\end{equation}
where $\mathbf{y(x, w)}$ is the output of a neural network with input vector
$\mathbf{x}$ and weight vector $\mathbf{w}$, and $ \mathbf{\Sigma}$ is the
covariance of the assumed Gaussian noise on the targets. Given a set
of independent observations of $\mathbf{x}$ and $\mathbf{t}$, write down
the error function that must be minimized in order to find the maximum
likelihood solution for $\mathbf{w}$, if we assume that $ \mathbf{\Sigma}$
is fixed and known. Now assume that $ \mathbf{\Sigma}$ is also to be determined
from the data, and write down an expression for the maximum likelihood
solution for $ \mathbf{\Sigma}$. Note that the optimizations of
$\mathbf{w}$ and $\mathbf{\Sigma}$ are now coupled, in contrast to the
case of independent target variables discussed in Section 5.2.

\vspace{1em}

\begin{proof}
    The negative log-likelihood is given by
    \begin{align*}
    -\ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
    &= -\sum_{i=1}^{N} \ln \mathcal{N}(\mathbf{t}_n | \mathbf{y}(\mathbf{x}_n, \mathbf{w}), \mathbf{\Sigma}) \\
    &= \frac{NK}{2} \ln(2\pi) + \frac{N}{2} \ln|\mathbf{\Sigma}| 
    + \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
    \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \end{align*}
    Maximizing the likelihood is equivalent to minimizing the negative
    log-likelihood. Therefore, the error function that must be minimized
    to obtain maximum likelihood is given by
    \[
        E(\mathbf{w}, \mathbf{\Sigma}) 
        = \frac{N}{2} \ln|\mathbf{\Sigma}|
        + \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
    In the case when $\mathbf{\Sigma}$ is known, we can simply treat
    the determinant term as a constant, so minimizing
    the error function
    \[
        E(\mathbf{w}) = 
        \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
    would yield the maximum likelihood solution $\mathbf{w}_\text{ML}$.
    If $ \mathbf{\Sigma}$ is unknown, we can't do that and the
    determination of $\mathbf{w}_{\text{ML}}$ would use $\mathbf{\Sigma}$,
    so that's why this time the optimizations of $\mathbf{w}$
    and $\mathbf{\Sigma}$ are coupled. The MLE for
    the covariance matrix is obtained by taking the
    derivative of the negative log-likelihood wrt. $\mathbf{\Sigma}^{-1}$, equalizing
    it to 0 and then solving for $\mathbf{\Sigma}$. Taking the
    derivative of the negative log-likelihood yields
    \begin{align*}
        \pdv{\mathbf{\Sigma}^{-1}} \ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
        &= \frac{N}{2} \pdv{\mathbf{\Sigma}^{-1}} \ln |\mathbf{\Sigma}| +
        \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big) \\
        &= -\frac{N}{2} \pdv{\mathbf{\Sigma}^{-1}} \ln |\mathbf{\Sigma}^{-1}| +
        \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \Trace\big\{\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)\big\} \\
        &= -\frac{N}{2} \mathbf{\Sigma} 
        + \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \Trace\big\{\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big) \mathbf{\Sigma}^{-1}\big\}\\
        &= -\frac{N}{2} \mathbf{\Sigma} + \frac{1}{2} \sum_{n=1}^{N} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \end{align*}
    where we've used the cyclic property of the trace operator and the fact that
    \[
        \pdv{\mathbf{A}} \ln |A| = A^{-T}
    \]
    Now, equalizing
    the derivative with 0 and solving for $\mathbf{\Sigma}$ gives
    the MLE for the covariance matrix:
    \[
        \mathbf{\Sigma}_\text{ML}
        = \frac{1}{N} \sum_{n=1}^{N} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
\end{proof}

\section*{Exercise 5.4 $\star \star$}
Consider a binary classification problem where the target values
are $t \in \{0, 1\}$, with a network output $y(\mathbf{x}, \mathbf{w})$
that represents $p(t = 1 | \mathbf{x})$, and suppose that
there is a probability $\epsiilon$ that the class label
on a trainining data point has been incorrectly set.
Assuming independent and identically distributed data,
write down the error function corresponding to the negative
log likelihood. Verify that the error function $\eqref{eq:5.21}$
is obtained when $\epsilon = 0$. Note that this error function
makes the model robust to incorrectly labelled data, in contrast
to the usual error function.

\vspace{1em}

\begin{proof}
    We're going to model the problem similarly with what we've done in
    Section 4.2.2, but this time taking into account the mislabelled training data
    probability. As a result, let $r \in \{0, 1\}$ the real target values, considering
    mislabelling. Therefore, we can find the label probabilities by weighting in the error
    chance:
    \begin{align*}
        p(r = 1 | \mathbf{x}, \mathbf{w}) 
        &= (1 - \epsilon)p(t = 1 | \mathbf{x}, \mathbf{w}) 
        + \epsilon p(t = 0 | \mathbf{x}, \mathbf{w})
        = (1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) 
        + \epsilon \big(1 - y(\mathbf{x}_n, \mathbf{w})\big)
    \end{align*}
    \vspace{-2em}
    \begin{align*}
        p(r = 0 | \mathbf{x}, \mathbf{w}) 
        &= (1 - \epsilon)p(t = 0 | \mathbf{x}, \mathbf{w}) 
        + \epsilon p(t = 1 | \mathbf{x}, \mathbf{w})
        = (1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big) 
        + \epsilon y(\mathbf{x}_n, \mathbf{w})
    \end{align*}
    We can combine both of these into
    \begin{align*}
        p(r | \mathbf{x}, \mathbf{w}) 
        &= p(r = 1 | \mathbf{x}, \mathbf{w})^r p(r = 0 | \mathbf{x}, \mathbf{w})^{1 - r} \\
        &= \big[(1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) + \epsilon \big(1 - y(\mathbf{x}_n, \mathbf{w})\big)\big]^r
        \big[(1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big) 
        + \epsilon y(\mathbf{x}_n, \mathbf{w})\big]^{1 - r}
    \end{align*}
    Therefore, the negative log-likelihood is given by
    \begin{align*}
        - \ln p(\mathbf{r} | \mathbf{\mathbf{X}}, \mathbf{w})
        = - \ln \prod_{i=1}^N p(r_n | \mathbf{x}_n, \mathbf{w})
        = - \sum_{i=1}^{N} \{r_n \ln p(r_n = 1 | \mathbf{x}_n, \mathbf{w}) 
            + (1 - r_n) \ln p(r_n = 0 | \mathbf{x}, \mathbf{w})\} 
    \end{align*}
    As a result, this is equivalent to minimizing the error function
    \[
        E(\mathbf{w}) = -\sum_{i=1}^{N} 
        \big[r_n \ln\big\{(1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) 
            + \epsilon\big(1 - y(\mathbf{x}_n, \mathbf{w})\big)\big\}
            + (1 - r_n) \ln\big\{(1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big)
            + \epsilon y(\mathbf{x}_n, \mathbf{w})\big]
    \] 
    which for $\epsilon = 0$ is equivalent to $\eqref{eq:5.21}$.
\end{proof}

\section*{Exercise 5.5 $\star$}
Show that maximizing likelihood for a multiclass neural network
model in which the network outputs have the interpretation
$y_k(\mathbf{x}, \mathbf{w}) = p(t_k = 1 | \mathbf{x})$ is equivalent to minimization
of the cross-entropy function \eqref{eq:5.24}.

\vspace{1em}

\begin{proof}
    Let's consider the binary target variables $t_k \in \{0, 1\}$ have a 1-of-$K$ 
    coding scheme indicating the class. If we assume the class labels are independent, 
    given the input vector, the conditional distribution of the targets is
    \[
        p(t_k | \mathbf{x}) = \prod_{k=1}^K p(t_k = 1 | \mathbf{x})^{t_k}
    \] 
    As a result, the corresponding negative log likelihood is given by
    \[
        -\ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
        = -\ln \prod_{n=1}^N \prod_{k=1}^K p(t_{nk} = 1 | \mathbf{x}_n)^{t_{nk}}
        = -\ln \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln p(t_{nk} = 1 | \mathbf{x}_n) 
    \]
    Therefore, maximizing the likelihood of the model is equivalent to minimization
    of the cross entropy function
    \begin{equation}\label{eq:5.24}\tag{5.24}
        E(\mathbf{w}) 
        = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln p(t_{nk} = 1 | \mathbf{x}_n) 
    \end{equation}
\end{proof}

\section*{Exercise 5.6 $\star$}
Show the derivative of the error function (5.21) with respect
to the activation $a_k$ for output units having a softmax activation
function satisfies \eqref{eq:5.18}.

\vspace{1em}

\begin{proof}
    The general result for the derivative of the softmax function with respect to the 
    activation $a_k$ was proved in Exercise 4.17 and is given by $\eqref{eq:4.106}$.
    Therefore, we have that
    \[
        \pdv{y_k}{a_k} = y_k(1 - y_k)
    \] 
    Taking the derivative of 
    \begin{equation}\label{eq:5.21}\tag{5.21}
        E(\mathbf{w}) = -\sum_{n=1}^{N} \{t_n \ln y_n + (1 - t_n) \ln(1 - y_n)\}
    \end{equation}
    with respect to $a_k$ yields
    \begin{align*}
        \pdv{a_k} E(\mathbf{w}) &= -t_k \pdv{a_k} \ln y_k - (1 - t_k) \pdv{a_k} \ln(1 - y_k) 
        = -t_k(1 - y_k) + y_k(1 - t_k)y_k 
        = y_k - t_k
    \end{align*}
    As a result,
    \begin{equation}\label{eq:5.18}\tag{5.18}
        \pdv{E}{a_k} = y_k - t_k
    \end{equation}
\end{proof}

\section*{Exercise 5.7 $\star$}
Show the derivative of the error function $\eqref{eq:5.21}$ with respect
to the activation $a_k$ for an output unit having a logistic sigmoid activation
function satisfies $\eqref{eq:5.18}$.

\vspace{1em}

\begin{proof}
    We've seen in Exercise 4.12 that
    \begin{equation}\tag{4.88}
        \pdv{a} \sigma(a) = \sigma(a)(1 - \sigma(a)
    \end{equation}
    Since the output unit has a logistic sigmoid activation function, then
    \[
        y_k = \sigma(a_k)
    \] 
    Therefore, using $\eqref{eq:4.88}$ gives
     \[
         \pdv{y_k}{a_k} = \sigma(a_k)\big(1 - \sigma(a_k)\big) = y_k(1 - y_k)
    \] 
    Analogously to Exercise 5.6, one can quickly reach that $\eqref{eq:5.18}$ holds.
\end{proof}

\section*{Exercise 5.8 $\star$}
We saw in $\eqref{eq:4.88}$ that the derivative of the logistic sigmoid
activation function can be expressed in terms of the function value itself.
Derive the corresponding result for the '$\tanh$' activation function defined
by (5.59).

\vspace{1em}

\begin{proof}
    Taking the derivative of the '$\tanh$' function is straightforward:
    \[
        \pdv{a} \tanh(a) 
        = \pdv{a} \bigg(\frac{e^a - e^{-a}}{e^a + e^{-a}}\bigg)
        = \frac{\big(e^a + e^{-a}\big)^2 - \big(e^a - e^{-a}\big)^2}{\big(e^a + e^{-a}\big)^2}
        = 1 - \bigg(\frac{e^a - e^{-a}}{e^a + e^{-a}}\bigg)^2
        = 1 - \tanh(a)^2
    \] 
    Notice that the derivative of the `$\tanh$` function can also be expressed
    as a function of itself.
\end{proof}

\section*{Exercise 5.9 $\star$}
The error function $\eqref{eq:5.21}$ for binary classification problems
was derived for a network having a logistic-sigmoid output activation
function, so that $0 \leq y(\mathbf{x}, \mathbf{w}) \leq 1$, and data having
target values $t \in \{0, 1\}$. Derive the corresponding error function
if we consider a network having an output $-1 \leq y(\mathbf{x}, \mathbf{w}) \leq 1$
and target values $t = 1$ for class $\mathcal{C}_1$ and $t = -1$ for
class  $\mathcal{C}_2$. What would be the appropiate choice of output
unit activation function?

\vspace{1em}

\begin{proof}
    The hyperbolic tangent is the appropiate choice for the ouput unit
    activation function, because `$\tanh$` is a sigmoid function and its
    values range between $-1$ and $1$. Let's consider the case of binary 
    classification in which we interpret the network output $y(\mathbf{x}, \mathbf{w})$
    as the conditional probability $p(\mathcal{C}_1 | \mathbf{x})$, with
    $p(\mathcal{C}_2 | \mathbf{x})$ given by $1 - y(\mathbf{x}, \mathbf{w})$. The
    conditional distribution of targets given inputs is then of the form
    \[
        p(t | \mathbf{x}, \mathbf{w}) 
        = y(\mathbf{x}, \mathbf{w})^{\frac{1+t}{2}} 
        \big\{1 - y(\mathbf{x}, \mathbf{w})\big\}^{\frac{1-t}{2}}
    \] 
    Taking the negative log-likelihood then yields
    \[
        -\ln p(\mathbf{t} | \mathbf{X}, \mathbf{w})
        = -\ln \prod_{n = 1}^N p(t_n | \mathbf{x}_n, \mathbf{w})
        = -\sum_{n=1}^{N} \bigg\{\frac{1+t}{2} \ln y(\mathbf{x}, \mathbf{w}) 
        + \frac{1-t}{2} \ln\big(1 - y(\mathbf{x}, \mathbf{w})\big)\bigg\}
    \] 
    As a result, maximizing the likelihood is equivalent to minimizing 
    the error function
    \[
        E(\mathbf{w}) = -\sum_{n=1}^{N} \bigg\{\frac{1+t}{2} \ln y_n + \frac{1-t}{2} \ln(1 -  y_n)\bigg\}
    \] 
    where $y_n$ denotes $y(\mathbf{x}_n, \mathbf{w})$.
\end{proof}

\section*{Exercise 5.10 $\star$}
Consider a Hessian matrix $\mathbf{H}$ with eigenvector equation $\eqref{eq:5.33}$. By
setting the vector $\mathbf{v}$ in (5.39) equal to each of the eigenvectors
$\mathbf{u}_i$ in turn, show that $\mathbf{H}$ is positive definite if,
and only if, all of its eigenvalues are positive.

\vspace{1em}

\begin{proof}
    Consider the eigenvector equation
    \begin{equation}\label{eq:5.33}\tag{5.33}
        \mathbf{H}\mathbf{u}_i = \lambda_i \mathbf{u}_i
    \end{equation}
    \begin{enumerate}
        \item [\to] Assume that $\mathbf{H}$ is positive definite. Then,
    \[
        \mathbf{u}_i^T \mathbf{H} \mathbf{u}_i = \lambda_i ||\mathbf{u}_i||^2 > 0
    \] 
    which happens only if the eigenvalues $\lambda_i$ are positive.
    \vspace{1em}
    \item [\leftarrow] Suppose that the eigenvalues $\lambda_i$ are positive.
        Since the eigenvectors form an orthonormal basis, an arbitrary
        vector $\mathbf{v}$ can be written in the form
        \begin{equation}\label{eq:5.38}\tag{5.38}
            \mathbf{v} = \sum_{i} c_i \mathbf{u}_i 
        \end{equation}
        Therefore, 
        \begin{align*}
            \mathbf{v}^T\mathbf{H}\mathbf{v}
            &= \bigg(\sum_{i} c_i \mathbf{u}_i\bigg)^T \mathbf{H}
            \bigg(\sum_{i} c_i \mathbf{u}_i\bigg)
            = \bigg(\sum_{i} c_i \mathbf{u}_i\bigg)^T
            \bigg(\sum_{i} c_i \lambda_i \mathbf{u}_i\bigg) \\
            &= \sum_{i} \sum_{j} \lambda_j c_i c_j \mathbf{u}_i^T \mathbf{u}_j
            = \sum_{i} \lambda_i c_i^2
        \end{align*}
        Since the eigenvalues $\lambda_i$ are positive,
        \[
            \mathbf{v}^T\mathbf{H}\mathbf{v} = \sum_{i} \lambda_i c_i^2 > 0
        \] 
        for all $\mathbf{v}$, which proves that $\mathbf{H}$ is positive definite.
    \end{enumerate}
\end{proof}

\section*{Exercise 5.11 $\star \star$}
Consider a quadratic error function defined by $\eqref{eq:5.32}$, in which the
Hessian matrix $\mathbf{H}$ has an eigenvalue equation given by $\eqref{eq:5.33}$.
Show that the contours of constant error are ellipses whose axes are aligned
with eigenvectors $\mathbf{u}_i$ with lengths that are inversly proportional
to the square root of the corresponding eigenvalues $\lambda_i$.

\vspace{1em}

\begin{proof}
    Analogously to what we've seen in Section 5.3.2, we're going
    to rewrite 
    \begin{equation}\label{eq:5.32}\tag{5.32}
        E(\mathbf{w}) \simeq E(\mathbf{w}^\star) + 
        \frac{1}{2}(\mathbf{w} - \mathbf{w}^\star)^T\mathbf{H}(\mathbf{w} - \mathbf{w}^\star)
    \end{equation}
    as 
    \begin{equation}\label{eq:5.36}\tag{5.36}
        E(\mathbf{w}) \simeq E(\mathbf{w}^\star) + \frac{1}{2} \sum_{i} \lambda_i \alpha_i^2 
    \end{equation}
    where we've expanded $(\mathbf{w} - \mathbf{w}^\star)$ as a linear combination
    of $\mathbf{H}$'s eigenvectors:
    \begin{equation}\label{eq:5.35}\tag{5.35}
       \mathbf{w} - \mathbf{w}^\star = \sum_{i} \alpha_i \mathbf{u}_i  
    \end{equation}
    Now, since $\mathbf{w}$ and $\mathbf{w}^\star$ are fixed, let 
    $\xi = 2E(\mathbf{w}) - 2E(\mathbf{w}^\star)$.
    Therefore, one can rewrite $\eqref{eq:5.36}$ as
     \[
         \xi \simeq \sum_{i} \lambda_i\alpha_i^2 
         = \sum_{i} \bigg(\frac{\alpha_i}{\lambda_i^{-1/2}} \bigg)^2
    \] 
    This equation describes an $N$-dimensional ellipsoid. Since
    the coordinates $\alpha_i$ that define it are using the orthonormal
    basis formed by $\{\mathbf{u}_i\}$, its axis are aligned with
    the eigenvectors $\mathbf{u}_i$. The axis length of an
    ellipse can be obtained by taking $\alpha_i = 0$, for
    $i \neq j$ such that
     \[
         \xi \simeq \bigg(\frac{\alpha_j}{\lambda_j^{-1/2}}\bigg)^2
    \] 
    and respectively
    \[
        \alpha_j \simeq \bigg(\frac{\xi}{\lambda_j}\bigg)^{1/2}
    \] 
    Therefore, the lenghts of the ellipses are inversly proportional
    to the square root of the corresponding eigenvalues $\lambda_i$.
\end{proof}

\section*{Exercise 5.12 $\star \star$}
By considering the local Taylor expansion ($\ref{eq:5.32}$) of
an error function about a stationary point $\mathbf{w}^\star$, show
that the necessary and sufficient condition for the stationary point to be 
a local minimum of the error function is that the Hessian matrix
$\mathbf{H}$, defined by $(5.30)$ with $\widehat{\mathbf{w}} = \mathbf{w}^\star$,
be positive definite.

\vspace{1em}

\begin{proof}
    $ $
    \begin{enumerate}
        \item [\to] Suppose that $\mathbf{H}$ is positive definite. From
            $\eqref{eq:5.32}$ one could then find that
             \[
                 E(\mathbf{w}) - E(\mathbf{w}^\star) > 0
            \] 
            for $\mathbf{w} \neq  \mathbf{w}^\star$. Therefore, $E(\mathbf{w}^\star)$
            would be the minimum value of E.
        \vspace{1em}
        \item [\leftarrow] Assume that $\mathbf{w}^\star$
            is a local minimum of E. Then,
            \[
                E(\mathbf{w}) -E(\mathbf{w}^\star) > 0
            \]
            which would mean that 
            \[
                \frac{1}{2} (\mathbf{w} - \mathbf{w}^\star)^T \mathbf{H}
                (\mathbf{w} - \mathbf{w}^\star) > 0
            \] 
            for $\mathbf{w} \neq \mathbf{w}^\star$, i.e.
            $\mathbf{H}$ is positive definite, since
            $\mathbf{w}$ respectively $\mathbf{w} - \mathbf{w}^\star$
            can be chosen arbitrarily.
    \end{enumerate}
\end{proof}

\section*{Exercise 5.13 $\star$}
Show that as a consequence of the symmetry of the Hessian matrix $\mathbf{H}$,
the number of independent elements in the quadratic error function $\eqref{eq:5.28}$ 
is given by $W(W + 3)/2$.

\vspace{1em}

\begin{proof}
    The independent elements in the
    \begin{equation}\label{eq:5.28}\tag{5.28}
        E(\mathbf{w}) \simeq E(\mathbf{\widehat{w}}) + 
        (\mathbf{w} - \widehat{\mathbf{w}})^T\mathbf{b}
        + \frac{1}{2}(\mathbf{w} - \mathbf{\widehat{w}})^T \mathbf{H} (\mathbf{w} - \mathbf{\widehat{w}})
    \end{equation}
    are given by the terms containing $\mathbf{b}$ and $\mathbf{H}$.
    Since $\mathbf{b}$ has $W$ elements and $\mathbf{H}$ is a symmetric matrix
    with $W(W + 1) / 2$ independent elements (see Exercise 2.21), one has a total of
    $$W + \frac{W(W + 1)}{2} = \frac{W(W + 3)}{2}$$ independent elements,
    where $W$ is the dimensionality of $\mathbf{w}$.
\end{proof}

\section*{Exercise 5.14 $\star$}
By making a Taylor expansion, verify that the terms that are $O(\epsilon)$ 
cancel on the right-hand side of $\eqref{eq:5.69}$.

\vspace{1em}

\begin{proof}
    Taking the Taylor expansion around $w_{ji}$ of the terms on the right hand side of
    \begin{equation}\label{eq:5.69}\tag{5.69}
        \pdv{E_n}{w_{ji}} = \frac{E_n(w_{ji} + \epsilon) - E_n(w_{ji} - \epsilon)}{2\epsilon}
        + O(\epsilon^2)
    \end{equation}
    yields
    \[
        E_n(w_{ji} + \epsilon) \simeq E_n(w_{ji}) + \epsilon E_n'(w_{ji}) 
        + \frac{\epsilon^2}{2} E_n''(w_{ji}) + O(\epsilon^3)
    \] 
    \[
        E_n(w_{ji} - \epsilon) \simeq E_n(w_{ji}) - \epsilon E_n'(w_{ji}) 
        + \frac{\epsilon^2}{2} E_n''(w_{ji}) + O(\epsilon^3)
    \] 
    Substituting these results into $\eqref{eq:5.69}$ cancels
    the $O(\epsilon)$ terms and gives

    \[
        \pdv{E_n}{w_{ji}} \simeq E_n'(w_{ji}) + O(\epsilon^2)
    \] 
\end{proof}

\section*{Exercise 5.15 $\star \star$}
In Section 5.3.4, we derived a procedure for evaluation the Jacobian
matrix of a neural network using a backpropagation procedure.
Derive an alternative formalism for finding the Jacobian based on 
$\emph{forward propagation}$ equations.

\vspace{1em}

\begin{proof}
    The Jacobian can be obtained by using the $\emph{forward propagation}$ technique.
    This is similar to what we've seen in Section 5.3.4, but this time the computations
    will start from the output end of the network. We have that
    \[
        J_{ki} = \pdv{y_k}{x_i} = \pdv{y_k}{a_k} \pdv{a_k}{x_i}
    \] 
    Summing over the $j$ hidden units that have links to $k$ units yields
    \[
        \pdv{a_k}{x_i} = \sum_{j} \pdv{a_k}{a_j} \pdv{a_j}{x_i}
    \] 
    From (5.48), it's obvious that
    \[
        \pdv{a_j}{x_i} = w_{ji}
    \] 
    As a result,
    \[
        J_{ki} = \pdv{y_k}{a_k} \sum_{j} \pdv{a_k}{a_j} w_{ji}
        = \pdv{y_k}{a_k} \sum_{j} \pdv{a_k}{z_j} \pdv{z_j}{a_j} w_{ji}
        = \pdv{y_k}{a_k} \sum_{j} \pdv{z_j}{a_j} w_{kj} w_{ji}
    \] 
    Suppose that $h$ is the activation function for the ouput layer,
    respectively $g$ for the hidden layer. Then,
    \[
        J_{ki} = h'(a_k) \sum_{j} g'(a_j) w_{kj} w_{ji}
    \] 
    Since the main steps are computing $a_j$ and $a_k$ (in this order), the
    process of evaluating the Jacobian can be tought of as a 
    forward propagation process.
\end{proof}

\section*{Exercise 5.16 $\star$}
The outer product approximation to the Hessian matrix for
a neural network using a sum-of-squares error function is given
by  $(5.84)$. Extend this result to the case of 
multiple outputs.

\vspace{1em}

\begin{proof}
    The sum-of-square error function for multiple outputs is given by
    \[
        E = \frac{1}{2} \sum_{n=1}^n ||\mathbf{y}_n - \mathbf{t}_n||^2
    \] 
    Similarly to Section 5.4.2, our goal is to obtain the outer product
    approximation for the Hessian matrix of the error. Hence,
    computing the Hessian yields:
    \begin{align*}
        \mathbf{H} 
        &= \nabla \nabla E
        = \nabla\bigg(\frac{1}{2} \sum_{n=1}^{N} \nabla ||\mathbf{y}_n - \mathbf{t}_n||^2\bigg)
        = \nabla \bigg(\sum_{n=1}^{N} (\mathbf{y}_n - \mathbf{t}_n)^T \nabla \mathbf{y}_n\bigg) \\
        &= \sum_{n=1}^{N} \nabla \mathbf{y}_n \nabla \mathbf{y}_n^T
        + \sum_{n=1}^{N} (\mathbf{y}_n - \mathbf{t}_n)^T \nabla \nabla \mathbf{y}_n
    \end{align*}
    Neglecting the second term yields the outer product approximation
    for the Hessian matrix:
    \[
        \mathbf{H} \simeq \sum_{n=1}^{N} \nabla \mathbf{y}_n \nabla \mathbf{y}_n^T
    \] 
    which is analogous to $(5.84)$ for $\mathbf{b}_n = \nabla \mathbf{y_n}$
    in the multiple output case.
    Note that, for simplicity all the $\nabla$ symbols refer to $\nabla_\mathbf{w}$ 
\end{proof}

\section*{Exercise 5.17 $\star$}
Consider a squared loss function of the form
\begin{equation}\label{eq:5.193}\tag{5.193}
    E = \frac{1}{2} \iint \{y(\mathbf{x}, \mathbf{w}) - t\}^2 p(\mathbf{x}, t) \diff \mathbf{x} \diff t
\end{equation}
where $y(\mathbf{x}, \mathbf{w})$ is a parametric function such as a neural
network. The result (1.89) shows that the function $y(\mathbf{x}, \mathbf{w})$
that minimizes this error is given by the conditional
expectation of $t$ given $\mathbf{x}$. Use this result to show
that the second derivative of $E$ with respect to two elements
$w_r$ and $w_s$ of the vector $\mathbf{w}$, is given by
\begin{equation}\label{eq:5.194}\tag{5.194}
    \pdv{E}{w_r}{w_s} = \int \pdv{y}{w_r} \pdv{y}{w_s} p(\mathbf{x}) \diff \mathbf{x}
\end{equation}
Note that, for a finite sample form $p(\mathbf{x})$, we obtain
(5.84).

\vspace{1em}

\begin{proof}
    To simplify the notation, we'll denote $y(\mathbf{x}, \mathbf{w})$ as $y$.
    Taking the second derivative of $E$ yields
    \begin{align*}
         \pdv{E}{w_s}{w_r}
         &= \pdv{}{w_s}{w_r} \bigg(\frac{1}{2} \iint \{y - t\}^2 p(\mathbf{x}, t) \diff \mathbf{x} \diff t\bigg) \\
         &= \frac{1}{2} \pdv{w_s} \iint 2(y - t) \pdv{y}{w_r} p(\mathbf{x}, t) \diff \mathbf{x} \diff t \\
         &= \iint \pdv{y}{w_s} \pdv{y}{w_r} p(\mathbf{x}, t) \diff \mathbf{x} \diff t + 
         \iint (y - t)\pdv{y}{w_s}{w_r} p(\mathbf{x}, t) \diff \mathbf{x} \diff t
    \end{align*}
    Using (1.89), i.e. that $y = \mathbb{E}_t[t | \mathbf{x}]$
    minimizes the error, proves that the second integral term is null:
    \begin{align*}
         \iint (y - t)\pdv{y}{w_s}{w_r} p(\mathbf{x}, t) \diff \mathbf{x} \diff t
         &= \int y p(\mathbf{x}) \pdv{y}{w_s}{w_r} \diff \mathbf{x}
         - \int \bigg(\int t p(t | x) \diff t\bigg) p(\mathbf{x}) \pdv{y}{w_s}{w_r} \diff \mathbf{x} \\
         &= \int (y - \mathbb{E}_t[t | \mathbf{x}]) p(\mathbf{x}) \pdv{y}{w_s}{w_r} \diff \mathbf{x} \\
         &= 0
    \end{align*}
    As a result, the second derivative can be written as
    \begin{equation*} \tag{5.194}
        \pdv{E}{w_s}{w_r}
        = \iint \pdv{y}{w_s} \pdv{y}{w_r} p(\mathbf{x}) p(t | \mathbf{x}) \diff \mathbf{x} \diff t
        = \int  \pdv{y}{w_s} \pdv{y}{w_r} p(\mathbf{x}) \diff \mathbf{x}
    \end{equation*}
\end{proof}

\section*{Exercise 5.18 $\star$}
Consider a two-layer network of the form shown in Figure 5.1 with the addition
of extra parameters corresponding to skip-layer connections
that go directly from inputs to the outputs. By extending the discussion of Section 5.3.2,
write down the equations for the derivatives of the error function with respect
to these additional parameters.

\vspace{1em}

\begin{proof}
    Let the weight corresponding to the skip-layer connections be denoted by $w_{ki}^{(s)}$. 
    The outputs will gain an extra sum corresponding to those connections:
    \[
        y_k = \sum_{j} w_{kj}^{(2)} z_j + \sum_{i} w_{ki}^{(s)} x_i
    \] 
    Since $\delta_k$'s functional form remains unchanged, the derivatives
    with respect to the first-layer and second-layer weights remain the same as
    before, i.e. (5.67). The derivative with respect to the skip-layer is now given by
    \[
        \pdv{E_n}{w_{ki}^{(s)}} = \pdv{E_n}{a_k} \pdv{a_k}{w_{ki}^{(s)}}
        = \delta_k x_i
    \] 
    the same as the one of the second-layer.
\end{proof}

\section*{Exercise 5.19 $\star$}
Derive the expression (5.85) for the outer product approximation
to the Hessian matrix for a network having a single output
with a logistic sigmoid output-unit activation function and a cross-entropy
error function, corresponding to the result (5.84) for the
sum-of-squares error function.

\vspace{1em}

\begin{proof}
    For simplicity, let $y_n$ denote $y(\mathbf{x}_n, \mathbf{w)}$.
    Consider a network with the cross-entropy error function
    \begin{equation}\label{eq:5.21}\tag{5.21}
        E(\mathbf{w}) = -\sum_{n=1}^{N} \{t_n \ln y_n + (1 - t_n) \ln(1 - y_n)\}
    \end{equation}
    and a single output with activation
    \[
        y_n = \sigma(a_n)
    \] 
    From $\eqref{eq:4.88}$ one has that
    \[
        \nabla y_n
        = \sigma(a_n)[1 - \sigma(a_n)]\nabla a_n
        = y_n(1 - y_n) \nabla a_n
    \] 
    Using the chain rule of differential calculus,
    \begin{align*}
        \mathbf{H} 
        = \nabla \nabla E(\mathbf{w})
        = \nabla \sum_{n=1}^{N} \pdv{E}{a_n} \pdv{a_n}{\mathbf{w}}
        = \nabla \sum_{n=1}^{N} \pdv{E}{a_n} \nabla a_n
        = \nabla \sum_{n=1}^{N} \pdv{E}{a_n} \nabla a_n
    \end{align*}
    Computing the derivative term is straightforward:
    \[
        \pdv{E}{a_n} 
        = -\bigg\{\frac{t_n}{y_n} \pdv{y_n}{a_n}
        -\frac{1 - t_n}{1 - y_n} \pdv{y_n}{a_n} \bigg\}
        = \frac{y_n(1 - t_n) - t_n(1 - y_n)}{y_n(1 - y_n)} 
        \pdv{y_n}{a_n}
        = y_n - t_n
    \] 
    Substituting this into the initial expression gives
    \begin{align*}
        \mathbf{H} 
        = \nabla  \sum_{n=1}^{N} (y_n - t_n) \nabla a_n
        = \sum_{n=1}^{N} \big\{\nabla y_n \nabla a_n^T + 
        (y_n - t_n) \nabla \nabla a_n\big\}
    \end{align*}
    The second term inside the sum contains the term ($y_n - t_n$), so
    we can neglect it as seen in Section 5.4.2 and arrive at the $\emph{outer 
    approximation}$ for the Hessian matrix by expanding $\nabla y_n$:
    \[
    \mathbf{H} \simeq \sum_{n=1}^{N} y_n(1 - y_n) \nabla a_n \nabla a_n^T
    \] 
    which is equivalent to $(5.85)$ for $\mathbf{b}_n = \nabla a_n$.
    Note that, for simplicity all the $\nabla$ symbols refer to $\nabla_\mathbf{w}$ 
\end{proof}

\section*{Exercise 5.20 $\star$}
Derive an expression for the outer product approximation
to the Hessian matrix for a network having $K$ outputs with a softmax
output-unit activation function and a cross-entropy error function,
corresponding to the result (5.84) for the sum-of-squares error
function.

\vspace{1em}

\begin{proof}
    We'll take a similar approach to the previous exercises. For simplicity,
    let $y_{nk}$ denote $y_{k}(\mathbf{x}_n, \mathbf{w}_k)$. 
    Consider a network with the cross-entropy error function
    \[
        E(\mathbf{w}) = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln y_{nk}  
    \] 
    and $K$ outputs with the softmax activation function
     \[
         y_{nk} = \frac{\exp{a_{nk}}}{\sum_{j=1}^{K} \exp{a_{nj}}} 
    \] 
    Note that both $\mathbf{y}_n$ and $\mathbf{a}_n$ are vectors 
    of size $1 \times K$. Using the chain rule of calculus yields
    \[
        \mathbf{H} 
        = \nabla \nabla E(\mathbf{w})
        = \nabla \sum_{n=1}^{N} \pdv{E}{\mathbf{a}_n} \pdv{\mathbf{a}_n}{\mathbf{w}}
        = \nabla \sum_{n=1}^{N} \pdv{E}{\mathbf{a_n}} \nabla \mathbf{a}_n 
    \] 
    Now, the derivative term will be of size $1 \times K$. The value of the $i$-th element
    will be given by
    \begin{align*}
        \bigg(\pdv{E}{\mathbf{a}_n}\bigg)_i
        = \pdv{E}{a_{ni}}
        = -\sum_{k=1}^{K} t_{nk} \pdv{\ln y_{nk}}{a_{ni}}
        = -\sum_{k=1}^{K} \frac{t_{nk}}{y_{nk}} \pdv{y_{nk}}{a_{ni}}
    \end{align*}
    From Exercise 4.17, respectively $\eqref{eq:4.106}$, we have that
     \[
         \pdv{y_{nk}}{a_{ni}} = y_{nk}(\delta_{ik} - y_{ni})
    \] 
    Therefore,
    \[
        \bigg(\pdv{E}{\mathbf{a}_n}\bigg)_i
        = -\sum_{k=1}^{K} \frac{t_{nk}}{y_{nk}} y_{nk} (\delta_{ik} - y_{ni})
        = -\sum_{k=1}^{K} t_{nk} (\delta_{ik} - y_{ni})
        = y_{ni} \sum_{k=1}^{K} t_{nk} - t_{ni}
        = y_{ni} - t_{ni}
    \] 
    Hence, it's obvious that
    \[
        \pdv{E}{\mathbf{a}_n} = \mathbf{y}_n - \mathbf{t}_n
    \] 
    Substituting this back into the Hessian,
    \begin{align*}
        \mathbf{H} 
        = \nabla \sum_{n=1}^{N} \big(\mathbf{y}_n - \mathbf{t}_n\big) \nabla \mathbf{a}_n
        = \sum_{n=1}^{N} \big\{\nabla \mathbf{y}_n \nabla \mathbf{a}_n 
            + (\mathbf{y}_n - \mathbf{t}_n) \nabla \nabla \mathbf{a}_n\}
    \end{align*}
    As before, the second term inside the sum contains the term $(\mathbf{y}_n - \mathbf{t}_n)$,
    so we can neglect it similarly to what we do in the previous exercises or Section 5.4.2. By ignoring
    the term, one arrives at the $\emph{outer approximation}$ of the Hessian matrix:
    \[
        \mathbf{H} \simeq 
        \sum_{n=1}^{N} \nabla \mathbf{y}_n \nabla \mathbf{a}_n
        = \sum_{n=1}^{N} \pdv{\mathbf{y}_n}{\mathbf{a}_n} \nabla \mathbf{a}_n \nabla \mathbf{a}_n
    \] 
    where the derivative term is a $K \times K$ matrix with the elements
    \[
        \bigg(\pdv{\mathbf{y}_n}{\mathbf{a}_n}\bigg)_{ij}
        = \pdv{y_{ni}}{a_{nj}}
        = y_{ni} (\delta_{ij} - y_{ni})
    \] 
    Note that for notation simplicity, all $\nabla$ symbols refer to $\nabla_\mathbf{w}$.
\end{proof}

\section*{Exercise 5.21 $\star \star \star$ TODO}
Extend the expression (5.86) for the outer product approximation
of the Hessian matrix to the case of $K > 1$ output units. Hence,
derive a recursive expression analogous to (5.87) for incrementing
the number $N$ of patterns and a similar expression for incrementing
the number $K$ of outputs. Use these results, together with the identity
(5.88), to find sequential update expressions analogous to (5.89) for
finding the inverse of the Hessian by incrementally including both extra patterns
and extra outputs.
\vspace{1em}

\begin{proof}
    
\end{proof}

\section*{Exercise 5.22 $\star \star$}
Derive the results $\eqref{eq:5.93}$, $\eqref{eq:5.94}$, and $\eqref{eq:5.95}$ for
the elements of the Hessian matrix of a two-layer feed-forward network by application of
the chain rule of calculus.

\vspace{1em}

\begin{proof}
    As seen in Section 5.4.5, we consider the three separate blocks of the Hessian:
    \begin{enumerate}
        \item Both weights are in the second layer:
            \begin{align*}
                \pdv{E_n}{w_{kj}^{(2)}}{w_{k'j'}^{(2)}} 
                &= \pdv{w_{kj}^{(2)}} \bigg(\pdv{E_n}{w_{k'j'}^{(2)}}\bigg) \\
                &= \pdv{w_{kj}^{(2)}} \bigg(\pdv{E_n}{a_{k'}} \pdv{a_{k'}}{w_{k'j'}^{(2)}}\bigg) \\
                &= \pdv{w_{kj}^{(2)}} \bigg(\pdv{E_n}{a_{k'}} z_{j'}\bigg) \\
                &= z_{j'} \pdv{a_{k'}} \bigg(\pdv{E_n}{a_k} \pdv{a_k}{w_{kj}^{(2)}}\bigg) \\
                &= z_{j}z_{j'} \pdv{E_n}{a_k}{a_{k'}} \\
                &= z_j z_{j'} M_{kk'} \label{eq:5.93}\tag{5.93}
            \end{align*}
        \item Both weights are in the first layer:
            \begin{align*}
                \pdv{E_n}{w_{ji}^{(1)}}{w_{j'i'}^{(1)}}
                &= \pdv{w_{ji}^{(1)}} \bigg(\pdv{E_n}{w_{j'i'}^{(1)}}\bigg) \\
                &= \pdv{w_{ji}^{(1)}} \bigg(\pdv{E_n}{a_{j'}} \pdv{a_{j'}}{w_{j'i'}^{(1)}}\bigg) \\
                &= \pdv{w_{ji}^{(1)}} \big(x_{i'} \delta_{j'}\big)
            \end{align*}
        Using the backpropagation formula $(5.56)$, we have that
        \begin{align*}
                \pdv{E_n}{w_{ji}^{(1)}}{w_{j'i'}^{(1)}}
                &= \pdv{w_{ji}^{(1)}} \bigg(x_{i'} h'(a_{j'}) \sum_{k} w_{kj'}^{(2)} \delta_k\bigg) \\
                &= x_{i'} \pdv{w_{ji}^{(1)}} \big(h'(a_{j'})\big)\sum_{k} \big(w_{kj'}^{(2)} \delta_k\big) +
                    x_{i'} h'(a_{j'}) \sum_{k} w_{kj'}^{(2)} \pdv{\delta_k}{w_{ji}^{(1)}}
        \end{align*}
        For $j \neq j'$ the derivative in the first term is null. As a result, the first term
        can be written as
        \begin{align*}
            x_{i'} \pdv{w_{ji}^{(1)}} \big(h'(a_{j'})\big) \sum_{k} w_{kj'}^{(2)} \delta_
            &= \mathbf{I}_{jj'} x_{i'} \pdv{w_{j'i}^{(1)}} \big(h'(a_{j'})\big) \sum_{k} w_{kj'}^{(2)} \delta_k \\
            &= \mathbf{I}_{jj'} x_i x_{i'} h''(a_{j'}) \sum_{k} w_{kj'}^{(2)} \delta_k
        \end{align*}
        Now, let's compute the derivative in the second term:
        \begin{align*}
            \pdv{\delta_k}{w_{ji}^{(1)}} 
            = \sum_{k'} \pdv{\delta_k}{a_{k'}} \pdv{a_{k'}}{w_{ji}^{(1)}}
            = \sum_{k'} \pdv{E_n}{a_k}{a_k'} \pdv{a_k'}{w_{ji}^{(1)}}
            &= \sum_{k'} M_{kk'} \pdv{w_{ji}^{(1)}}\bigg(\sum_{j} w_{k'j}^{(2)} h(x_i w_{ji}^{(1)})\bigg) \\
            &= x_i h'(a_j) \sum_{k'} M_{kk'} w_{k'j}^{(2)}
        \end{align*}
        Putting everything together yields the desired result:
        \begin{align*}
            \pdv{E_n}{w_{ji}^{(1)}}{w_{j'i'}^{(1)}}
            = \mathbf{I}_{jj'} x_i x_{i'} h''(a_{j'}) \sum_{k} w_{kj'}^{(2)} \delta_k +
            x_i x_{i'} h'(a_j) h'(a_{j'}) \sum_{k} \sum_{k'} w_{kj'}^{(2)} w_{k'j}^{(2)} M_{kk'}
            \label{eq:5.94}\tag{5.94}
        \end{align*}
        Note that this result is equivalent with the one in the book even if the $k$ and $k'$ are
        interchanged in the second term. This is because the sum ranges are chosen arbitrarily.
    \item One weight in each layer:
        \begin{align*}
            \pdv{E_n}{w_{ji}^{(1)}}{w_{kj'}^{(2)}}
            &= \pdv{w_{ji}^{(1)}} \bigg(\pdv{E_n}{w_{kj'}^{(2)}}\bigg) \\
            &= \pdv{w_{ji}^{(1)}} \bigg(\pdv{E_n}{a_{k}} \pdv{a_k}{w_{kj'}^{(2)}}\bigg) \\
            &= \pdv{w_{ji}^{(1)}} \big(\delta_{k} z_{j'}\big) \\ 
            &= z_{j'} \pdv{\delta_k}{w_{ji}^{(1)}} + \delta_k \pdv{z_{j'}}{w_{ji}^{(1)}}
        \end{align*}
        We found the value of the first term in the previous case. Also, the derivative
        in the second term is null for $j \neq j'$. Therefore, the above expression becomes
        \begin{align*}
            \pdv{E_n}{w_{ji}^{(1)}}{w_{kj'}^{(2)}}
            &= z_{j'} x_i h'(a_j) \sum_{k'} M_{kk'} w_{k'j}^{(2)} + 
            \mathbf{I}_{jj'} \delta_k x_i h'(a_j) \\
            &= x_i h'(a_j) \bigg\{\delta_k \mathbf{I}_{jj'} + z_{j'} \sum_{k'} w_{k'j}^{(2)} M_{kk'}\bigg\} 
            \label{eq:5.95}\tag{5.95}
        \end{align*}
    \end{enumerate}
\end{proof}
