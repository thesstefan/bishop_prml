\chapter{Neural Networks}

\section*{Exercise 5.1 $\star \star$}
Consider a two-layer network function of the form $\eqref{eq:5.7}$ 
in which the hidden-unit nonlinear activation functions $h(\cdot)$ 
are given by logistic sigmoid functions of the form
\begin{equation}\label{eq:5.191}\tag{5.191}
    \sigma(a) = \frac{1}{1 + \exp(-a)}.
\end{equation}
Show that there exists an equivalent network, which computes 
exactly the same function, but with hidden activation functions
given by $\tanh(a)$ where the $\tanh$ function is defined by
(5.59). Hint: first find the relation between $\sigma(a)$ 
and $\tanh(a)$, and then show that the parameters of the two
networks differ by linear transformations.

\vspace{1em}

\begin{proof}
    The considered two-layer network has the form
    \begin{equation}\label{eq:5.7}\tag{5.7}
        y_k(\mathbf{x}, \mathbf{w}) 
        = \sigma\bigg(\sum_{j=1}^{M} w_{kj}^{(2)} 
        h\bigg(\sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)}\bigg) + w_{k0}^{(2)}\bigg)
    \end{equation}
    Now, we've proved in Exercise 3.1 that 
    \[
        \sigma(x) = \frac{1}{2} \tanh \frac{x}{2} + \frac{1}{2}
    \] 
    Therefore, we can rewrite $y_k$ as
    \begin{align*}
        y_k(\mathbf{x}, \mathbf{w})
        &= \sigma\bigg(\frac{1}{2}\sum_{j=1}^{M} w_{kj}^{(2)} 
            \tanh\bigg(\frac{1}{2}\sum_{i=1}^{D} w_{ji}^{(1)}x_i + \frac{1}{2} w_{j0}^{(1)}\bigg)
            + \frac{1}{2} \sum_{j=1}^{M} w_{kj}^{(2)} + w_{k0}^{(2)}\bigg) \\
        &= \sigma\bigg(\sum_{j=1}^{M} \omega_{kj}^{(2)} 
        h\bigg(\sum_{i=1}^{D} \omega_{ji}^{(1)}x_i + \omega_{j0}^{(1)}\bigg) + \omega_{k0}^{(2)}\bigg)
    \end{align*}
    where 
    \newline
    \begin{minipage}[b]{0.25\textwidth}
    \[
       \omega_{ji}^{(1)} = \frac{1}{2} w_{ji}^{(1)}
    \]
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
    \[
        \omega_{j0}^{(1)} = \frac{1}{2} w_{j0}^{(1)}
    \] 
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
    \[
        \omega_{kj}^{(2)} = \frac{1}{2} w_{kj}^{(2)}
    \]
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
        \[
            \omega_{k0}^{(2)} = \frac{1}{2}\sum_{j = 1}^M w_{kj}^{(2)} + w_{k0}^{(2)}
        \] 
    \end{minipage}
    Both new parameter sets can be obtained as linear transformations of the old ones,
    so there exists an equivalent two-layer network using $\tanh$ hidden activation functions,
    but different parameters.
\end{proof}

\section*{Exercise 5.2 $\star$}
Show that maximizing the likelihood function under the conditional 
distribution \eqref{eq:5.16} for a multioutput network is equivalent to minimizing
the sum-of-squares error function (5.11).

\vspace{1em}

\begin{proof}
    The likelihood function is given by 
    \[
        p(\mathbf{T} | \mathbf{X}, \mathbf{w}, \beta)
        = \prod_{n = 1}^N p(\mathbf{t}_n | \mathbf{x}_n, \mathbf{w}, \beta)
    \] 
    The target variables are assumed to be distributed normally
    \begin{equation}\label{eq:5.16}\tag{5.16}
        p(\mathbf{t}_n | \mathbf{x}_n, \mathbf{w}, \beta) 
        = \mathcal{N}(\mathbf{t}_n | \mathbf{y}(\mathbf{x}_n, \mathbf{w}), \beta^{-1}\mathbf{I})
    \end{equation}
    and since
    \[
        \ln \mathcal{N}(\mathbf{t}_n | y(\mathbf{x}_n, \mathbf{w}), \beta^{-1}\mathbf{I})
        = -\frac{N}{2} \ln \beta - \frac{NK}{2} \ln(2\pi) - \frac{\beta}{2} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2
    \] 
    the negative log-likelihood is given by
    \[
        -\ln p(\mathbf{t} | \mathbf{X}, \mathbf{w}, \beta)
        = \frac{\beta}{2}\sum_{n=1}^{N} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2 + \text{const}
    \] 
    where we grouped the terms that don't depend on $\mathbf{w}$ under the constant term.
    Maximization of the likelihood function is equivalent to minimizing the negative 
    log-likelihood.
    Therefore, one can easily find that this is equivalent to minimizing the error function
    \begin{equation}\label{eq:5.11}\tag{5.11}
        E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2
    \end{equation}
\end{proof}

\section*{Exercise 5.3 $\star \star$}
Consider a regression problem involving multiple target variables in which
it is assumed that the distribution of the targets, conditioned on the input
vector $\mathbf{x}$, is a Gaussian of the form
\begin{equation}\label{eq:5.192}\tag{5.192}
    p(\mathbf{t} | \mathbf{x}, \mathbf{w}) 
    = \mathcal{N}(\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{w}), \mathbf{\Sigma})
\end{equation}
where $\mathbf{y(x, w)}$ is the output of a neural network with input vector
$\mathbf{x}$ and weight vector $\mathbf{w}$, and $ \mathbf{\Sigma}$ is the
covariance of the assumed Gaussian noise on the targets. Given a set
of independent observations of $\mathbf{x}$ and $\mathbf{t}$, write down
the error function that must be minimized in order to find the maximum
likelihood solution for $\mathbf{w}$, if we assume that $ \mathbf{\Sigma}$
is fixed and known. Now assume that $ \mathbf{\Sigma}$ is also to be determined
from the data, and write down an expression for the maximum likelihood
solution for $ \mathbf{\Sigma}$. Note that the optimizations of
$\mathbf{w}$ and $\mathbf{\Sigma}$ are now coupled, in contrast to the
case of independent target variables discussed in Section 5.2.

\vspace{1em}

\begin{proof}
    The negative log-likelihood is given by
    \begin{align*}
    -\ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
    &= -\sum_{i=1}^{N} \ln \mathcal{N}(\mathbf{t}_n | \mathbf{y}(\mathbf{x}_n, \mathbf{w}), \mathbf{\Sigma}) \\
    &= \frac{NK}{2} \ln(2\pi) + \frac{N}{2} \ln|\mathbf{\Sigma}| 
    + \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
    \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \end{align*}
    Maximizing the likelihood is equivalent to minimizing the negative
    log-likelihood. Therefore, the error function that must be minimized
    to obtain maximum likelihood is given by
    \[
        E(\mathbf{w}, \mathbf{\Sigma}) 
        = \frac{N}{2} \ln|\mathbf{\Sigma}|
        + \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
    In the case when $\mathbf{\Sigma}$ is known, we can simply treat
    the determinant term as a constant, so minimizing
    the error function
    \[
        E(\mathbf{w}) = 
        \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
    would yield the maximum likelihood solution $\mathbf{w}_\text{ML}$.
    If $ \mathbf{\Sigma}$ is unknown, we can't do that and the
    determination of $\mathbf{w}_{\text{ML}}$ would use $\mathbf{\Sigma}$,
    so that's why this time the optimizations of $\mathbf{w}$
    and $\mathbf{\Sigma}$ are coupled. The MLE for
    the covariance matrix is obtained by taking the
    derivative of the negative log-likelihood wrt. $\mathbf{\Sigma}^{-1}$, equalizing
    it to 0 and then solving for $\mathbf{\Sigma}$. Taking the
    derivative of the negative log-likelihood yields
    \begin{align*}
        \pdv{\mathbf{\Sigma}^{-1}} \ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
        &= \frac{N}{2} \pdv{\mathbf{\Sigma}^{-1}} \ln |\mathbf{\Sigma}| +
        \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big) \\
        &= -\frac{N}{2} \pdv{\mathbf{\Sigma}^{-1}} \ln |\mathbf{\Sigma}^{-1}| +
        \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \Trace\big\{\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)\big\} \\
        &= -\frac{N}{2} \mathbf{\Sigma} 
        + \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \Trace\big\{\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big) \mathbf{\Sigma}^{-1}\big\}\\
        &= -\frac{N}{2} \mathbf{\Sigma} + \frac{1}{2} \sum_{n=1}^{N} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \end{align*}
    where we've used the cyclic property of the trace operator and the fact that
    \[
        \pdv{\mathbf{A}} \ln |A| = A^{-T}
    \]
    Now, equalizing
    the derivative with 0 and solving for $\mathbf{\Sigma}$ gives
    the MLE for the covariance matrix:
    \[
        \mathbf{\Sigma}_\text{ML}
        = \frac{1}{N} \sum_{n=1}^{N} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
\end{proof}

\section*{Exercise 5.4 $\star \star$}
Consider a binary classification problem where the target values
are $t \in \{0, 1\}$, with a network output $y(\mathbf{x}, \mathbf{w})$
that represents $p(t = 1 | \mathbf{x})$, and suppose that
there is a probability $\epsiilon$ that the class label
on a trainining data point has been incorrectly set.
Assuming independent and identically distributed data,
write down the error function corresponding to the negative
log likelihood. Verify that the error function $\eqref{eq:5.21}$
is obtained when $\epsilon = 0$. Note that this error function
makes the model robust to incorrectly labelled data, in contrast
to the usual error function.

\vspace{1em}

\begin{proof}
    We're going to model the problem similarly with what we've done in
    Section 4.2.2, but this time taking into account the mislabelled training data
    probability. As a result, let $r \in \{0, 1\}$ the real target values, considering
    mislabelling. Therefore, we can find the label probabilities by weighting in the error
    chance:
    \begin{align*}
        p(r = 1 | \mathbf{x}, \mathbf{w}) 
        &= (1 - \epsilon)p(t = 1 | \mathbf{x}, \mathbf{w}) 
        + \epsilon p(t = 0 | \mathbf{x}, \mathbf{w})
        = (1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) 
        + \epsilon \big(1 - y(\mathbf{x}_n, \mathbf{w})\big)
    \end{align*}
    \vspace{-2em}
    \begin{align*}
        p(r = 0 | \mathbf{x}, \mathbf{w}) 
        &= (1 - \epsilon)p(t = 0 | \mathbf{x}, \mathbf{w}) 
        + \epsilon p(t = 1 | \mathbf{x}, \mathbf{w})
        = (1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big) 
        + \epsilon y(\mathbf{x}_n, \mathbf{w})
    \end{align*}
    We can combine both of these into
    \begin{align*}
        p(r | \mathbf{x}, \mathbf{w}) 
        &= p(r = 1 | \mathbf{x}, \mathbf{w})^r p(r = 0 | \mathbf{x}, \mathbf{w})^{1 - r} \\
        &= \big[(1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) + \epsilon \big(1 - y(\mathbf{x}_n, \mathbf{w})\big)\big]^r
        \big[(1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big) 
        + \epsilon y(\mathbf{x}_n, \mathbf{w})\big]^{1 - r}
    \end{align*}
    Therefore, the negative log-likelihood is given by
    \begin{align*}
        - \ln p(\mathbf{r} | \mathbf{\mathbf{X}}, \mathbf{w})
        = - \ln \prod_{i=1}^N p(r_n | \mathbf{x}_n, \mathbf{w})
        = - \sum_{i=1}^{N} \{r_n \ln p(r_n = 1 | \mathbf{x}_n, \mathbf{w}) 
            + (1 - r_n) \ln p(r_n = 0 | \mathbf{x}, \mathbf{w})\} 
    \end{align*}
    As a result, this is equivalent to minimizing the error function
    \[
        E(\mathbf{w}) = -\sum_{i=1}^{N} 
        \big[r_n \ln\big\{(1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) 
            + \epsilon\big(1 - y(\mathbf{x}_n, \mathbf{w})\big)\big\}
            + (1 - r_n) \ln\big\{(1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big)
            + \epsilon y(\mathbf{x}_n, \mathbf{w})\big]
    \] 
    which for $\epsilon = 0$ is equivalent to $\eqref{eq:5.21}$.
\end{proof}

\section*{Exercise 5.5 $\star$}
Show that maximizing likelihood for a multiclass neural network
model in which the network outputs have the interpretation
$y_k(\mathbf{x}, \mathbf{w}) = p(t_k = 1 | \mathbf{x})$ is equivalent to minimization
of the cross-entropy function \eqref{eq:5.24}.

\vspace{1em}

\begin{proof}
    Let's consider the binary target variables $t_k \in \{0, 1\}$ have a 1-of-$K$ 
    coding scheme indicating the class. If we assume the class labels are independent, 
    given the input vector, the conditional distribution of the targets is
    \[
        p(t_k | \mathbf{x}) = \prod_{k=1}^K p(t_k = 1 | \mathbf{x})^{t_k}
    \] 
    As a result, the corresponding negative log likelihood is given by
    \[
        -\ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
        = -\ln \prod_{n=1}^N \prod_{k=1}^K p(t_{nk} = 1 | \mathbf{x}_n)^{t_{nk}}
        = -\ln \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln p(t_{nk} = 1 | \mathbf{x}_n) 
    \]
    Therefore, maximizing the likelihood of the model is equivalent to minimization
    of the cross entropy function
    \begin{equation}\label{eq:5.24}\tag{5.24}
        E(\mathbf{w}) 
        = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln p(t_{nk} = 1 | \mathbf{x}_n) 
    \end{equation}
\end{proof}

\section*{Exercise 5.6 $\star$}
Show the derivative of the error function (5.21) with respect
to the activation $a_k$ for output units having a softmax activation
function satisfies \eqref{eq:5.18}.

\vspace{1em}

\begin{proof}
    The general result for the derivative of the softmax function with respect to the 
    activation $a_k$ was proved in Exercise 4.17 and is given by $\eqref{eq:4.106}$.
    Therefore, we have that
    \[
        \pdv{y_k}{a_k} = y_k(1 - y_k)
    \] 
    Taking the derivative of 
    \begin{equation}\label{eq:5.21}\tag{5.21}
        E(\mathbf{w}) = -\sum_{n=1}^{N} \{t_n \ln y_n + (1 - t_n) \ln(1 - y_n)\}
    \end{equation}
    with respect to $a_k$ yields
    \begin{align*}
        \pdv{a_k} E(\mathbf{w}) &= -t_k \pdv{a_k} \ln y_k - (1 - t_k) \pdv{a_k} \ln(1 - y_k) 
        = -t_k(1 - y_k) + y_k(1 - t_k)y_k 
        = y_k - t_k
    \end{align*}
    As a result,
    \begin{equation}\label{eq:5.18}\tag{5.18}
        \pdv{E}{a_k} = y_k - t_k
    \end{equation}
\end{proof}

\section*{Exercise 5.7 $\star$}
Show the derivative of the error function $\eqref{eq:5.21}$ with respect
to the activation $a_k$ for an output unit having a logistic sigmoid activation
function satisfies $\eqref{eq:5.18}$.

\vspace{1em}

\begin{proof}
    We've seen in Exercise 4.12 that
    \begin{equation}\tag{4.88}
        \pdv{a} \sigma(a) = \sigma(a)(1 - \sigma(a)
    \end{equation}
    Since the output unit has a logistic sigmoid activation function, then
    \[
        y_k = \sigma(a_k)
    \] 
    Therefore, using $\eqref{eq:4.88}$ gives
     \[
         \pdv{y_k}{a_k} = \sigma(a_k)\big(1 - \sigma(a_k)\big) = y_k(1 - y_k)
    \] 
    Analogously to Exercise 5.6, one can quickly reach that $\eqref{eq:5.18}$ holds.
\end{proof}

\section*{Exercise 5.8 $\star$}
We saw in $\eqref{eq:4.88}$ that the derivative of the logistic sigmoid
activation function can be expressed in terms of the function value itself.
Derive the corresponding result for the '$\tanh$' activation function defined
by (5.59).

\vspace{1em}

\begin{proof}
    Taking the derivative of the '$\tanh$' function is straightforward:
    \[
        \pdv{a} \tanh(a) 
        = \pdv{a} \bigg(\frac{e^a - e^{-a}}{e^a + e^{-a}}\bigg)
        = \frac{\big(e^a + e^{-a}\big)^2 - \big(e^a - e^{-a}\big)^2}{\big(e^a + e^{-a}\big)^2}
        = 1 - \bigg(\frac{e^a - e^{-a}}{e^a + e^{-a}}\bigg)^2
        = 1 - \tanh(a)^2
    \] 
    Notice that the derivative of the `$\tanh$` function can also be expressed
    as a function of itself.
\end{proof}

\section*{Exercise 5.9 $\star$}
The error function $\eqref{eq:5.21}$ for binary classification problems
was derived for a network having a logistic-sigmoid output activation
function, so that $0 \leq y(\mathbf{x}, \mathbf{w}) \leq 1$, and data having
target values $t \in \{0, 1\}$. Derive the corresponding error function
if we consider a network having an output $-1 \leq y(\mathbf{x}, \mathbf{w}) \leq 1$
and target values $t = 1$ for class $\mathcal{C}_1$ and $t = -1$ for
class  $\mathcal{C}_2$. What would be the appropiate choice of output
unit activation function?

\vspace{1em}

\begin{proof}
    The hyperbolic tangent is the appropiate choice for the ouput unit
    activation function, because `$\tanh$` is a sigmoid function and its
    values range between $-1$ and $1$. Let's consider the case of binary 
    classification in which we interpret the network output $y(\mathbf{x}, \mathbf{w})$
    as the conditional probability $p(\mathcal{C}_1 | \mathbf{x})$, with
    $p(\mathcal{C}_2 | \mathbf{x})$ given by $1 - y(\mathbf{x}, \mathbf{w})$. The
    conditional distribution of targets given inputs is then of the form
    \[
        p(t | \mathbf{x}, \mathbf{w}) 
        = y(\mathbf{x}, \mathbf{w})^{\frac{1+t}{2}} 
        \big\{1 - y(\mathbf{x}, \mathbf{w})\big\}^{\frac{1-t}{2}}
    \] 
    Taking the negative log-likelihood then yields
    \[
        -\ln p(\mathbf{t} | \mathbf{X}, \mathbf{w})
        = -\ln \prod_{n = 1}^N p(t_n | \mathbf{x}_n, \mathbf{w})
        = -\sum_{n=1}^{N} \bigg\{\frac{1+t}{2} \ln y(\mathbf{x}, \mathbf{w}) 
        + \frac{1-t}{2} \ln\big(1 - y(\mathbf{x}, \mathbf{w})\big)\bigg\}
    \] 
    As a result, maximizing the likelihood is equivalent to minimizing 
    the error function
    \[
        E(\mathbf{w}) = -\sum_{n=1}^{N} \bigg\{\frac{1+t}{2} \ln y_n + \frac{1-t}{2} \ln(1 -  y_n)\bigg\}
    \] 
    where $y_n$ denotes $y(\mathbf{x}_n, \mathbf{w})$.
\end{proof}

\section*{Exercise 5.10 $\star$}
Consider a Hessian matrix $\mathbf{H}$ with eigenvector equation $\eqref{eq:5.33}$. By
setting the vector $\mathbf{v}$ in (5.39) equal to each of the eigenvectors
$\mathbf{u}_i$ in turn, show that $\mathbf{H}$ is positive definite if,
and only if, all of its eigenvalues are positive.

\vspace{1em}

\begin{proof}
    Consider the eigenvector equation
    \begin{equation}\label{eq:5.33}\tag{5.33}
        \mathbf{H}\mathbf{u}_i = \lambda_i \mathbf{u}_i
    \end{equation}
    \begin{enumerate}
        \item [\to] Assume that $\mathbf{H}$ is positive definite. Then,
    \[
        \mathbf{u}_i^T \mathbf{H} \mathbf{u}_i = \lambda_i ||\mathbf{u}_i||^2 > 0
    \] 
    which happens only if the eigenvalues $\lambda_i$ are positive.
    \vspace{1em}
    \item [\leftarrow] Suppose that the eigenvalues $\lambda_i$ are positive.
        Since the eigenvectors form an orthonormal basis, an arbitrary
        vector $\mathbf{v}$ can be written in the form
        \begin{equation}\label{eq:5.38}\tag{5.38}
            \mathbf{v} = \sum_{i} c_i \mathbf{u}_i 
        \end{equation}
        Therefore, 
        \[
            \mathbf{v}^T\mathbf{H}\mathbf{v}
            = \bigg(\sum_{i} c_i \mathbf{u}_i\bigg)^T \mathbf{H}
            \bigg(\sum_{i} c_i \mathbf{u}_i\bigg)
            = \bigg(\sum_{i} c_i \mathbf{u}_i\bigg)^T
            \bigg(\sum_{i} c_i \lambda_i \mathbf{u}_i\bigg)
            = \sum_{i} \sum_{j} \lambda_j c_i c_j \mathbf{u}_i^T \mathbf{u}_j
            = \sum_{i} \lambda_i c_i^2
        \] 
        Since the eigenvalues $\lambda_i$ are positive,
        \[
            \mathbf{v}^T\mathbf{H}\mathbf{v} = \sum_{i} \lambda_i c_i^2 > 0
        \] 
        for all $\mathbf{v}$, which proves that $\mathbf{H}$ is positive definite.
    \end{enumerate}
\end{proof}

\section*{Exercise 5.11 $\star \star ++++++++++++$}
Consider a quadratic error function defined by $\eqref{eq:5.32}$, in which the
Hessian matrix $\mathbf{H}$ has an eigenvalue equation given by $\eqref{eq:5.33}$.
Show that the contours of constant error are ellipses whose axes are aligned
with eigenvectors $\mathbf{u}_i$ with lengths that are inversly proportional
to the square root of the corresponding eigenvalues $\lambda_i$.

\vspace{1em}

\begin{proof}
    Analogously to what we've seen in Section 5.3.2, we're going
    to rewrite 
    \begin{equation}\label{eq:5.32}\tag{5.32}
        E(\mathbf{w}) \approxeq E(\mathbf{w}^\star) + 
        \frac{1}{2}(\mathbf{w} - \mathbf{w}^\star)^T\mathbf{H}(\mathbf{w} - \mathbf{w}^\star)
    \end{equation}
    as 
    \begin{equation}\label{eq:5.36}\tag{5.36}
        E(\mathbf{w}) \approxeq E(\mathbf{w}^\star) + \frac{1}{2} \sum_{i} \lambda_i \alpha_i^2 
    \end{equation}
    where we've expanded $(\mathbf{w} - \mathbf{w}^\star)$ as a linear combination
    of $\mathbf{H}$'s eigenvectors:
    \begin{equation}\label{eq:5.35}\tag{5.35}
       \mathbf{w} - \mathbf{w}^\star = \sum_{i} \alpha_i \mathbf{u}_i  
    \end{equation}
    Now, since $\mathbf{w}$ and $\mathbf{w}^\star$ are fixed, let 
    $\xi = 2E(\mathbf{w}) - 2E(\mathbf{w}^\star)$.
    Therefore, one can rewrite $\eqref{eq:5.36}$ as
     \[
         \xi \approxeq \sum_{i} \lambda_i\alpha_i^2 
        \sum_{i} \frac{\alpha_i^2}{\lambda_i^{-1/2}} 
    \] 
    This equation describes an ellipsoid with axes aligned to
    the eigenvectors $\mathbf{u}_i$ (since we use the coordinate system
    defined in $\eqref{eq:5.35}$) with axes lengths that are inversly
    proportional to the square root of the corresponding eigenvalues
    $\lambda_i$.
\end{proof}

\section*{Exercise 5.12 $\star \star +++++++++++++$}
By considering the local Taylor expansion ($\ref{eq:5.32}$) of
an error function about a stationary point $\mathbf{w}^\star$, show
that the necessary and sufficient condition for the stationary point to be 
a local minimum of the error function is that the Hessian matrix
$\mathbf{H}$, defined by $\eqref{eq:5.30}$ with $\widehat{\mathbf{w}} = \mathbf{w}^\star$,
be positive definite.

\vspace{1em}

\begin{proof}
    $ $
    \begin{enumerate}
        \item [\to] Suppose that $\mathbf{H}$ is positive definite. From
            $\eqref{eq:5.32}$ one could then find that
             \[
                 E(\mathbf{w}) - E(\mathbf{w}^\star) > 0
            \] 
            for $\mathbf{w} \neq  \mathbf{w}^\star$. Therefore, $E(\mathbf{w}^\star)$
            would be the minimum value of E.
        \vspace{1em}
        \item [\leftarrow] Assume that $\mathbf{w}^\star$
            is a local minimum of E. Then,
            \[
                E(\mathbf{w}) -E(\mathbf{w}^\star) > 0
            \]
            which would mean that 
            \[
                \frac{1}{2} (\mathbf{w} - \mathbf{w}^\star)^T \mathbf{H}
                (\mathbf{w} - \mathbf{w}^\star) > 0
            \] 
            for $\mathbf{w} \neq \mathbf{w}^\star$, i.e.
            $\mathbf{H}$ is positive definite, since
            $\mathbf{w}$ respectively $\mathbf{w} - \mathbf{w}^\star$
            can be chosen arbitrarily.
    \end{enumerate}
\end{proof}
