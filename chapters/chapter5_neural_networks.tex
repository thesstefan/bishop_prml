\chapter{Neural Networks}

\section*{Exercise 5.1 $\star \star$}
Consider a two-layer network function of the form $\eqref{eq:5.7}$ 
in which the hidden-unit nonlinear activation functions $h(\cdot)$ 
are given by logistic sigmoid functions of the form
\begin{equation}\label{eq:5.191}\tag{5.191}
    \sigma(a) = \frac{1}{1 + \exp(-a)}.
\end{equation}
Show that there exists an equivalent network, which computes 
exactly the same function, but with hidden activation functions
given by $\tanh(a)$ where the $\tanh$ function is defined by
(5.59). Hint: first find the relation between $\sigma(a)$ 
and $\tanh(a)$, and then show that the parameters of the two
networks differ by linear transformations.

\vspace{1em}

\begin{proof}
    The considered two-layer network has the form
    \begin{equation}\label{eq:5.7}\tag{5.7}
        y_k(\mathbf{x}, \mathbf{w}) 
        = \sigma\bigg(\sum_{j=1}^{M} w_{kj}^{(2)} 
        h\bigg(\sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)}\bigg) + w_{k0}^{(2)}\bigg)
    \end{equation}
    Now, we've proved in Exercise 3.1 that 
    \[
        \sigma(x) = \frac{1}{2} \tanh \frac{x}{2} + \frac{1}{2}
    \] 
    Therefore, we can rewrite $y_k$ as
    \begin{align*}
        y_k(\mathbf{x}, \mathbf{w})
        &= \sigma\bigg(\frac{1}{2}\sum_{j=1}^{M} w_{kj}^{(2)} 
            \tanh\bigg(\frac{1}{2}\sum_{i=1}^{D} w_{ji}^{(1)}x_i + \frac{1}{2} w_{j0}^{(1)}\bigg)
            + \frac{1}{2} \sum_{j=1}^{M} w_{kj}^{(2)} + w_{k0}^{(2)}\bigg) \\
        &= \sigma\bigg(\sum_{j=1}^{M} \omega_{kj}^{(2)} 
        h\bigg(\sum_{i=1}^{D} \omega_{ji}^{(1)}x_i + \omega_{j0}^{(1)}\bigg) + \omega_{k0}^{(2)}\bigg)
    \end{align*}
    where 
    \newline
    \begin{minipage}[b]{0.25\textwidth}
    \[
       \omega_{ji}^{(1)} = \frac{1}{2} w_{ji}^{(1)}
    \]
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
    \[
        \omega_{j0}^{(1)} = \frac{1}{2} w_{j0}^{(1)}
    \] 
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
    \[
        \omega_{kj}^{(2)} = \frac{1}{2} w_{kj}^{(2)}
    \]
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
        \[
            \omega_{k0}^{(2)} = \frac{1}{2}\sum_{j = 1}^M w_{kj}^{(2)} + w_{k0}^{(2)}
        \] 
    \end{minipage}
    Both new parameter sets can be obtained as linear transformations of the old ones,
    so there exists an equivalent two-layer network using $\tanh$ hidden activation functions,
    but different parameters.
\end{proof}

\section*{Exercise 5.2 $\star$}
Show that maximizing the likelihood function under the conditional 
distribution \eqref{eq:5.16} for a multioutput network is equivalent to minimizing
the sum-of-squares error function (5.11).

\vspace{1em}

\begin{proof}
    The likelihood function is given by 
    \[
        p(\mathbf{T} | \mathbf{X}, \mathbf{w}, \beta)
        = \prod_{n = 1}^N p(\mathbf{t}_n | \mathbf{x}_n, \mathbf{w}, \beta)
    \] 
    The target variables are assumed to be distributed normally
    \begin{equation}\label{eq:5.16}\tag{5.16}
        p(\mathbf{t}_n | \mathbf{x}_n, \mathbf{w}, \beta) 
        = \mathcal{N}(\mathbf{t}_n | \mathbf{y}(\mathbf{x}_n, \mathbf{w}), \beta^{-1}\mathbf{I})
    \end{equation}
    and since
    \[
        \ln \mathcal{N}(\mathbf{t}_n | y(\mathbf{x}_n, \mathbf{w}), \beta^{-1}\mathbf{I})
        = -\frac{N}{2} \ln \beta - \frac{NK}{2} \ln(2\pi) - \frac{\beta}{2} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2
    \] 
    the negative log-likelihood is given by
    \[
        -\ln p(\mathbf{t} | \mathbf{X}, \mathbf{w}, \beta)
        = \frac{\beta}{2}\sum_{n=1}^{N} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2 + \text{const}
    \] 
    where we grouped the terms that don't depend on $\mathbf{w}$ under the constant term.
    Maximization of the likelihood function is equivalent to minimizing the negative 
    log-likelihood.
    Therefore, one can easily find that this is equivalent to minimizing the error function
    \begin{equation}\label{eq:5.11}\tag{5.11}
        E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2
    \end{equation}
\end{proof}

\section*{Exercise 5.3 $\star \star$}
Consider a regression problem involving multiple target variables in which
it is assumed that the distribution of the targets, conditioned on the input
vector $\mathbf{x}$, is a Gaussian of the form
\begin{equation}\label{eq:5.192}\tag{5.192}
    p(\mathbf{t} | \mathbf{x}, \mathbf{w}) 
    = \mathcal{N}(\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{w}), \mathbf{\Sigma})
\end{equation}
where $\mathbf{y(x, w)}$ is the output of a neural network with input vector
$\mathbf{x}$ and weight vector $\mathbf{w}$, and $ \mathbf{\Sigma}$ is the
covariance of the assumed Gaussian noise on the targets. Given a set
of independent observations of $\mathbf{x}$ and $\mathbf{t}$, write down
the error function that must be minimized in order to find the maximum
likelihood solution for $\mathbf{w}$, if we assume that $ \mathbf{\Sigma}$
is fixed and known. Now assume that $ \mathbf{\Sigma}$ is also to be determined
from the data, and write down an expression for the maximum likelihood
solution for $ \mathbf{\Sigma}$. Note that the optimizations of
$\mathbf{w}$ and $\mathbf{\Sigma}$ are now coupled, in contrast to the
case of independent target variables discussed in Section 5.2.

\vspace{1em}

\begin{proof}
    The negative log-likelihood is given by
    \begin{align*}
    -\ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
    &= -\sum_{i=1}^{N} \ln \mathcal{N}(\mathbf{t}_n | \mathbf{y}(\mathbf{x}_n, \mathbf{w}), \mathbf{\Sigma}) \\
    &= \frac{NK}{2} \ln(2\pi) + \frac{N}{2} \ln|\mathbf{\Sigma}| 
    + \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
    \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \end{align*}
    Maximizing the likelihood is equivalent to minimizing the negative
    log-likelihood. Therefore, the error function that must be minimized
    to obtain maximum likelihood is given by
    \[
        E(\mathbf{w}, \mathbf{\Sigma}) 
        = \frac{N}{2} \ln|\mathbf{\Sigma}|
        + \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
    In the case when $\mathbf{\Sigma}$ is known, we can simply treat
    the determinant term as a constant, so minimizing
    the error function
    \[
        E(\mathbf{w}) = 
        \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
    would yield the maximum likelihood solution $\mathbf{w}_\text{ML}$.
    If $ \mathbf{\Sigma}$ is unknown, we can't do that and the
    determination of $\mathbf{w}_{\text{ML}}$ would use $\mathbf{\Sigma}$,
    so that's why this time the optimizations of $\mathbf{w}$
    and $\mathbf{\Sigma}$ are coupled. The MLE for
    the covariance matrix is obtained by taking the
    derivative of the negative log-likelihood wrt. $\mathbf{\Sigma}^{-1}$, equalizing
    it to 0 and then solving for $\mathbf{\Sigma}$. Taking the
    derivative of the negative log-likelihood yields
    \begin{align*}
        \pdv{\mathbf{\Sigma}^{-1}} \ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
        &= \frac{N}{2} \pdv{\mathbf{\Sigma}^{-1}} \ln |\mathbf{\Sigma}| +
        \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big) \\
        &= -\frac{N}{2} \pdv{\mathbf{\Sigma}^{-1}} \ln |\mathbf{\Sigma}^{-1}| +
        \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \Trace\big\{\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)\big\} \\
        &= -\frac{N}{2} \mathbf{\Sigma} 
        + \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \Trace\big\{\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big) \mathbf{\Sigma}^{-1}\big\}\\
        &= -\frac{N}{2} \mathbf{\Sigma} + \frac{1}{2} \sum_{n=1}^{N} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \end{align*}
    where we've used the cyclic property of the trace operator and the fact that
    \[
        \pdv{\mathbf{A}} \ln |A| = A^{-T}
    \]
    Now, equalizing
    the derivative with 0 and solving for $\mathbf{\Sigma}$ gives
    the MLE for the covariance matrix:
    \[
        \mathbf{\Sigma}_\text{ML}
        = \frac{1}{N} \sum_{n=1}^{N} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
\end{proof}

\section*{Exercise 5.4 $\star \star$}
Consider a binary classification problem where the target values
are $t \in \{0, 1\}$, with a network output $y(\mathbf{x}, \mathbf{w})$
that represents $p(t = 1 | \mathbf{x})$, and suppose that
there is a probability $\epsiilon$ that the class label
on a trainining data point has been incorrectly set.
Assuming independent and identically distributed data,
write down the error function corresponding to the negative
log likelihood. Verify that the error function $\eqref{eq:5.21}$
is obtained when $\epsilon = 0$. Note that this error function
makes the model robust to incorrectly labelled data, in contrast
to the usual error function.

\vspace{1em}

\begin{proof}
    We're going to model the problem similarly with what we've done in
    Section 4.2.2, but this time taking into account the mislabelled training data
    probability. As a result, let $r \in \{0, 1\}$ the real target values, considering
    mislabelling. Therefore, we can find the label probabilities by weighting in the error
    chance:
    \begin{align*}
        p(r = 1 | \mathbf{x}, \mathbf{w}) 
        &= (1 - \epsilon)p(t = 1 | \mathbf{x}, \mathbf{w}) 
        + \epsilon p(t = 0 | \mathbf{x}, \mathbf{w})
        = (1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) 
        + \epsilon \big(1 - y(\mathbf{x}_n, \mathbf{w})\big)
    \end{align*}
    \vspace{-2em}
    \begin{align*}
        p(r = 0 | \mathbf{x}, \mathbf{w}) 
        &= (1 - \epsilon)p(t = 0 | \mathbf{x}, \mathbf{w}) 
        + \epsilon p(t = 1 | \mathbf{x}, \mathbf{w})
        = (1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big) 
        + \epsilon y(\mathbf{x}_n, \mathbf{w})
    \end{align*}
    We can combine both of these into
    \begin{align*}
        p(r | \mathbf{x}, \mathbf{w}) 
        &= p(r = 1 | \mathbf{x}, \mathbf{w})^r p(r = 0 | \mathbf{x}, \mathbf{w})^{1 - r} \\
        &= \big[(1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) + \epsilon \big(1 - y(\mathbf{x}_n, \mathbf{w})\big)\big]^r
        \big[(1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big) 
        + \epsilon y(\mathbf{x}_n, \mathbf{w})\big]^{1 - r}
    \end{align*}
    Therefore, the negative log-likelihood is given by
    \begin{align*}
        - \ln p(\mathbf{r} | \mathbf{\mathbf{X}}, \mathbf{w})
        = - \ln \prod_{i=1}^N p(r_n | \mathbf{x}_n, \mathbf{w})
        = - \sum_{i=1}^{N} \{r_n \ln p(r_n = 1 | \mathbf{x}_n, \mathbf{w}) 
            + (1 - r_n) \ln p(r_n = 0 | \mathbf{x}, \mathbf{w})\} 
    \end{align*}
    As a result, this is equivalent to minimizing the error function
    \[
        E(\mathbf{w}) = -\sum_{i=1}^{N} 
        \big[r_n \ln\big\{(1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) 
            + \epsilon\big(1 - y(\mathbf{x}_n, \mathbf{w})\big)\big\}
            + (1 - r_n) \ln\big\{(1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big)
            + \epsilon y(\mathbf{x}_n, \mathbf{w})\big]
    \] 
    which for $\epsilon = 0$ is equivalent to $\eqref{eq:5.21}$.
\end{proof}

\section*{Exercise 5.5 $\star$}
Show that maximizing likelihood for a multiclass neural network
model in which the network outputs have the interpretation
$y_k(\mathbf{x}, \mathbf{w}) = p(t_k = 1 | \mathbf{x})$ is equivalent to minimization
of the cross-entropy function \eqref{eq:5.24}.

\vspace{1em}

\begin{proof}
    Let's consider the binary target variables $t_k \in \{0, 1\}$ have a 1-of-$K$ 
    coding scheme indicating the class. If we assume the class labels are independent, 
    given the input vector, the conditional distribution of the targets is
    \[
        p(t_k | \mathbf{x}) = \prod_{k=1}^K p(t_k = 1 | \mathbf{x})^{t_k}
    \] 
    As a result, the corresponding negative log likelihood is given by
    \[
        -\ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
        = -\ln \prod_{n=1}^N \prod_{k=1}^K p(t_{nk} = 1 | \mathbf{x}_n)^{t_{nk}}
        = -\ln \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln p(t_{nk} = 1 | \mathbf{x}_n) 
    \]
    Therefore, maximizing the likelihood of the model is equivalent to minimization
    of the cross entropy function
    \begin{equation}\label{eq:5.24}\tag{5.24}
        E(\mathbf{w}) 
        = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln p(t_{nk} = 1 | \mathbf{x}_n) 
    \end{equation}
\end{proof}

\section*{Exercise 5.6 $\star$}
Show the derivative of the error function (5.21) with respect
to the activation $a_k$ for output units having a softmax activation
function satisfies \eqref{eq:5.18}.

\vspace{1em}

\begin{proof}
    The general result for the derivative of the softmax function with respect to the 
    activation $a_k$ was proved in Exercise 4.17 and is given by $\eqref{eq:4.106}$.
    Therefore, we have that
    \[
        \pdv{y_k}{a_k} = y_k(1 - y_k)
    \] 
    Taking the derivative of 
    \begin{equation}\label{eq:5.21}\tag{5.21}
        E(\mathbf{w}) = -\sum_{n=1}^{N} \{t_n \ln y_n + (1 - t_n) \ln(1 - y_n)\}
    \end{equation}
    with respect to $a_k$ yields
    \begin{align*}
        \pdv{a_k} E(\mathbf{w}) &= -t_k \pdv{a_k} \ln y_k - (1 - t_k) \pdv{a_k} \ln(1 - y_k) 
        = -t_k(1 - y_k) + y_k(1 - t_k)y_k 
        = y_k - t_k
    \end{align*}
    As a result,
    \begin{equation}\label{eq:5.18}\tag{5.18}
        \pdv{E}{a_k} = y_k - t_k
    \end{equation}
\end{proof}

\section*{Exercise 5.7 $\star$}
Show the derivative of the error function $\eqref{eq:5.21}$ with respect
to the activation $a_k$ for an output unit having a logistic sigmoid activation
function satisfies $\eqref{eq:5.18}$.

\vspace{1em}

\begin{proof}
    We've seen in Exercise 4.12 that
    \begin{equation}\tag{4.88}
        \pdv{a} \sigma(a) = \sigma(a)(1 - \sigma(a)
    \end{equation}
    Since the output unit has a logistic sigmoid activation function, then
    \[
        y_k = \sigma(a_k)
    \] 
    Therefore, using $\eqref{eq:4.88}$ gives
     \[
         \pdv{y_k}{a_k} = \sigma(a_k)\big(1 - \sigma(a_k)\big) = y_k(1 - y_k)
    \] 
    Analogously to Exercise 5.6, one can quickly reach that $\eqref{eq:5.18}$ holds.
\end{proof}

\section*{Exercise 5.8 $\star$}
We saw in $\eqref{eq:4.88}$ that the derivative of the logistic sigmoid
activation function can be expressed in terms of the function value itself.
Derive the corresponding result for the '$\tanh$' activation function defined
by (5.59).

\vspace{1em}

\begin{proof}
    Taking the derivative of the '$\tanh$' function is straightforward:
    \[
        \pdv{a} \tanh(a) 
        = \pdv{a} \bigg(\frac{e^a - e^{-a}}{e^a + e^{-a}}\bigg)
        = \frac{\big(e^a + e^{-a}\big)^2 - \big(e^a - e^{-a}\big)^2}{\big(e^a + e^{-a}\big)^2}
        = 1 - \bigg(\frac{e^a - e^{-a}}{e^a + e^{-a}}\bigg)^2
        = 1 - \tanh(a)^2
    \] 
    Notice that the derivative of the `$\tanh$` function can also be expressed
    as a function of itself.
\end{proof}

\section*{Exercise 5.9 $\star$}
The error function $\eqref{eq:5.21}$ for binary classification problems
was derived for a network having a logistic-sigmoid output activation
function, so that $0 \leq y(\mathbf{x}, \mathbf{w}) \leq 1$, and data having
target values $t \in \{0, 1\}$. Derive the corresponding error function
if we consider a network having an output $-1 \leq y(\mathbf{x}, \mathbf{w}) \leq 1$
and target values $t = 1$ for class $\mathcal{C}_1$ and $t = -1$ for
class  $\mathcal{C}_2$. What would be the appropiate choice of output
unit activation function?

\vspace{1em}

\begin{proof}
    The hyperbolic tangent is the appropiate choice for the ouput unit
    activation function, because `$\tanh$` is a sigmoid function and its
    values range between $-1$ and $1$. Let's consider the case of binary 
    classification in which we interpret the network output $y(\mathbf{x}, \mathbf{w})$
    as the conditional probability $p(\mathcal{C}_1 | \mathbf{x})$, with
    $p(\mathcal{C}_2 | \mathbf{x})$ given by $1 - y(\mathbf{x}, \mathbf{w})$. The
    conditional distribution of targets given inputs is then of the form
    \[
        p(t | \mathbf{x}, \mathbf{w}) 
        = y(\mathbf{x}, \mathbf{w})^{\frac{1+t}{2}} 
        \big\{1 - y(\mathbf{x}, \mathbf{w})\big\}^{\frac{1-t}{2}}
    \] 
    Taking the negative log-likelihood then yields
    \[
        -\ln p(\mathbf{t} | \mathbf{X}, \mathbf{w})
        = -\ln \prod_{n = 1}^N p(t_n | \mathbf{x}_n, \mathbf{w})
        = -\sum_{n=1}^{N} \bigg\{\frac{1+t}{2} \ln y(\mathbf{x}, \mathbf{w}) 
        + \frac{1-t}{2} \ln\big(1 - y(\mathbf{x}, \mathbf{w})\big)\bigg\}
    \] 
    As a result, maximizing the likelihood is equivalent to minimizing 
    the error function
    \[
        E(\mathbf{w}) = -\sum_{n=1}^{N} \bigg\{\frac{1+t}{2} \ln y_n + \frac{1-t}{2} \ln(1 -  y_n)\bigg\}
    \] 
    where $y_n$ denotes $y(\mathbf{x}_n, \mathbf{w})$.
\end{proof}

\section*{Exercise 5.10 $\star$}
Consider a Hessian matrix $\mathbf{H}$ with eigenvector equation $\eqref{eq:5.33}$. By
setting the vector $\mathbf{v}$ in (5.39) equal to each of the eigenvectors
$\mathbf{u}_i$ in turn, show that $\mathbf{H}$ is positive definite if,
and only if, all of its eigenvalues are positive.

\vspace{1em}

\begin{proof}
    Consider the eigenvector equation
    \begin{equation}\label{eq:5.33}\tag{5.33}
        \mathbf{H}\mathbf{u}_i = \lambda_i \mathbf{u}_i
    \end{equation}
    \begin{enumerate}
        \item [\to] Assume that $\mathbf{H}$ is positive definite. Then,
    \[
        \mathbf{u}_i^T \mathbf{H} \mathbf{u}_i = \lambda_i ||\mathbf{u}_i||^2 > 0
    \] 
    which happens only if the eigenvalues $\lambda_i$ are positive.
    \vspace{1em}
    \item [\leftarrow] Suppose that the eigenvalues $\lambda_i$ are positive.
        Since the eigenvectors form an orthonormal basis, an arbitrary
        vector $\mathbf{v}$ can be written in the form
        \begin{equation}\label{eq:5.38}\tag{5.38}
            \mathbf{v} = \sum_{i} c_i \mathbf{u}_i 
        \end{equation}
        Therefore, 
        \begin{align*}
            \mathbf{v}^T\mathbf{H}\mathbf{v}
            &= \bigg(\sum_{i} c_i \mathbf{u}_i\bigg)^T \mathbf{H}
            \bigg(\sum_{i} c_i \mathbf{u}_i\bigg)
            = \bigg(\sum_{i} c_i \mathbf{u}_i\bigg)^T
            \bigg(\sum_{i} c_i \lambda_i \mathbf{u}_i\bigg) \\
            &= \sum_{i} \sum_{j} \lambda_j c_i c_j \mathbf{u}_i^T \mathbf{u}_j
            = \sum_{i} \lambda_i c_i^2
        \end{align*}
        Since the eigenvalues $\lambda_i$ are positive,
        \[
            \mathbf{v}^T\mathbf{H}\mathbf{v} = \sum_{i} \lambda_i c_i^2 > 0
        \] 
        for all $\mathbf{v}$, which proves that $\mathbf{H}$ is positive definite.
    \end{enumerate}
\end{proof}

\section*{Exercise 5.11 $\star \star$}
Consider a quadratic error function defined by $\eqref{eq:5.32}$, in which the
Hessian matrix $\mathbf{H}$ has an eigenvalue equation given by $\eqref{eq:5.33}$.
Show that the contours of constant error are ellipses whose axes are aligned
with eigenvectors $\mathbf{u}_i$ with lengths that are inversly proportional
to the square root of the corresponding eigenvalues $\lambda_i$.

\vspace{1em}

\begin{proof}
    Analogously to what we've seen in Section 5.3.2, we're going
    to rewrite 
    \begin{equation}\label{eq:5.32}\tag{5.32}
        E(\mathbf{w}) \simeq E(\mathbf{w}^\star) + 
        \frac{1}{2}(\mathbf{w} - \mathbf{w}^\star)^T\mathbf{H}(\mathbf{w} - \mathbf{w}^\star)
    \end{equation}
    as 
    \begin{equation}\label{eq:5.36}\tag{5.36}
        E(\mathbf{w}) \simeq E(\mathbf{w}^\star) + \frac{1}{2} \sum_{i} \lambda_i \alpha_i^2 
    \end{equation}
    where we've expanded $(\mathbf{w} - \mathbf{w}^\star)$ as a linear combination
    of $\mathbf{H}$'s eigenvectors:
    \begin{equation}\label{eq:5.35}\tag{5.35}
       \mathbf{w} - \mathbf{w}^\star = \sum_{i} \alpha_i \mathbf{u}_i  
    \end{equation}
    Now, since $\mathbf{w}$ and $\mathbf{w}^\star$ are fixed, let 
    $\xi = 2E(\mathbf{w}) - 2E(\mathbf{w}^\star)$.
    Therefore, one can rewrite $\eqref{eq:5.36}$ as
     \[
         \xi \simeq \sum_{i} \lambda_i\alpha_i^2 
         = \sum_{i} \bigg(\frac{\alpha_i}{\lambda_i^{-1/2}} \bigg)^2
    \] 
    This equation describes an $N$-dimensional ellipsoid. Since
    the coordinates $\alpha_i$ that define it are using the orthonormal
    basis formed by $\{\mathbf{u}_i\}$, its axis are aligned with
    the eigenvectors $\mathbf{u}_i$. The axis length of an
    ellipse can be obtained by taking $\alpha_i = 0$, for
    $i \neq j$ such that
     \[
         \xi \simeq \bigg(\frac{\alpha_j}{\lambda_j^{-1/2}}\bigg)^2
    \] 
    and respectively
    \[
        \alpha_j \simeq \bigg(\frac{\xi}{\lambda_j}\bigg)^{1/2}
    \] 
    Therefore, the lenghts of the ellipses are inversly proportional
    to the square root of the corresponding eigenvalues $\lambda_i$.
\end{proof}

\section*{Exercise 5.12 $\star \star$}
By considering the local Taylor expansion ($\ref{eq:5.32}$) of
an error function about a stationary point $\mathbf{w}^\star$, show
that the necessary and sufficient condition for the stationary point to be 
a local minimum of the error function is that the Hessian matrix
$\mathbf{H}$, defined by $(5.30)$ with $\widehat{\mathbf{w}} = \mathbf{w}^\star$,
be positive definite.

\vspace{1em}

\begin{proof}
    $ $
    \begin{enumerate}
        \item [\to] Suppose that $\mathbf{H}$ is positive definite. From
            $\eqref{eq:5.32}$ one could then find that
             \[
                 E(\mathbf{w}) - E(\mathbf{w}^\star) > 0
            \] 
            for $\mathbf{w} \neq  \mathbf{w}^\star$. Therefore, $E(\mathbf{w}^\star)$
            would be the minimum value of E.
        \vspace{1em}
        \item [\leftarrow] Assume that $\mathbf{w}^\star$
            is a local minimum of E. Then,
            \[
                E(\mathbf{w}) -E(\mathbf{w}^\star) > 0
            \]
            which would mean that 
            \[
                \frac{1}{2} (\mathbf{w} - \mathbf{w}^\star)^T \mathbf{H}
                (\mathbf{w} - \mathbf{w}^\star) > 0
            \] 
            for $\mathbf{w} \neq \mathbf{w}^\star$, i.e.
            $\mathbf{H}$ is positive definite, since
            $\mathbf{w}$ respectively $\mathbf{w} - \mathbf{w}^\star$
            can be chosen arbitrarily.
    \end{enumerate}
\end{proof}

\section*{Exercise 5.13 $\star$}
Show that as a consequence of the symmetry of the Hessian matrix $\mathbf{H}$,
the number of independent elements in the quadratic error function $\eqref{eq:5.28}$ 
is given by $W(W + 3)/2$.

\vspace{1em}

\begin{proof}
    The independent elements in the
    \begin{equation}\label{eq:5.28}\tag{5.28}
        E(\mathbf{w}) \simeq E(\mathbf{\widehat{w}}) + 
        (\mathbf{w} - \widehat{\mathbf{w}})^T\mathbf{b}
        + \frac{1}{2}(\mathbf{w} - \mathbf{\widehat{w}})^T \mathbf{H} (\mathbf{w} - \mathbf{\widehat{w}})
    \end{equation}
    are given by the terms containing $\mathbf{b}$ and $\mathbf{H}$.
    Since $\mathbf{b}$ has $W$ elements and $\mathbf{H}$ is a symmetric matrix
    with $W(W + 1) / 2$ independent elements (see Exercise 2.21), one has a total of
    $$W + \frac{W(W + 1)}{2} = \frac{W(W + 3)}{2}$$ independent elements,
    where $W$ is the dimensionality of $\mathbf{w}$.
\end{proof}

\section*{Exercise 5.14 $\star$}
By making a Taylor expansion, verify that the terms that are $O(\epsilon)$ 
cancel on the right-hand side of $\eqref{eq:5.69}$.

\vspace{1em}

\begin{proof}
    Taking the Taylor expansion around $w_{ji}$ of the terms on the right hand side of
    \begin{equation}\label{eq:5.69}\tag{5.69}
        \pdv{E_n}{w_{ji}} = \frac{E_n(w_{ji} + \epsilon) - E_n(w_{ji} - \epsilon)}{2\epsilon}
        + O(\epsilon^2)
    \end{equation}
    yields
    \[
        E_n(w_{ji} + \epsilon) \simeq E_n(w_{ji}) + \epsilon E_n'(w_{ji}) 
        + \frac{\epsilon^2}{2} E_n''(w_{ji}) + O(\epsilon^3)
    \] 
    \[
        E_n(w_{ji} - \epsilon) \simeq E_n(w_{ji}) - \epsilon E_n'(w_{ji}) 
        + \frac{\epsilon^2}{2} E_n''(w_{ji}) + O(\epsilon^3)
    \] 
    Substituting these results into $\eqref{eq:5.69}$ cancels
    the $O(\epsilon)$ terms and gives

    \[
        \pdv{E_n}{w_{ji}} \simeq E_n'(w_{ji}) + O(\epsilon^2)
    \] 
\end{proof}

\section*{Exercise 5.15 $\star \star$}
In Section 5.3.4, we derived a procedure for evaluation the Jacobian
matrix of a neural network using a backpropagation procedure.
Derive an alternative formalism for finding the Jacobian based on 
$\emph{forward propagation}$ equations.

\vspace{1em}

\begin{proof}
    The Jacobian can be obtained by using the $\emph{forward propagation}$ technique.
    This is similar to what we've seen in Section 5.3.4, but this time the computations
    will start from the output end of the network. We have that
    \[
        J_{ki} = \pdv{y_k}{x_i} = \pdv{y_k}{a_k} \pdv{a_k}{x_i}
    \] 
    Summing over the $j$ hidden units that have links to $k$ units yields
    \[
        \pdv{a_k}{x_i} = \sum_{j} \pdv{a_k}{a_j} \pdv{a_j}{x_i}
    \] 
    From (5.48), it's obvious that
    \[
        \pdv{a_j}{x_i} = w_{ji}
    \] 
    As a result,
    \[
        J_{ki} = \pdv{y_k}{a_k} \sum_{j} \pdv{a_k}{a_j} w_{ji}
        = \pdv{y_k}{a_k} \sum_{j} \pdv{a_k}{z_j} \pdv{z_j}{a_j} w_{ji}
        = \pdv{y_k}{a_k} \sum_{j} \pdv{z_j}{a_j} w_{kj} w_{ji}
    \] 
    Suppose that $h$ is the activation function for the ouput layer,
    respectively $g$ for the hidden layer. Then,
    \[
        J_{ki} = h'(a_k) \sum_{j} g'(a_j) w_{kj} w_{ji}
    \] 
    Since the main steps are computing $a_j$ and $a_k$ (in this order), the
    process of evaluating the Jacobian can be tought of as a 
    forward propagation process.
\end{proof}

\section*{Exercise 5.16 $\star$}
The outer product approximation to the Hessian matrix for
a neural network using a sum-of-squares error function is given
by  $(5.84)$. Extend this result to the case of 
multiple outputs.

\vspace{1em}

\begin{proof}
    The sum-of-square error function for multiple outputs is given by
    \[
        E = \frac{1}{2} \sum_{n=1}^n ||\mathbf{y}_n - \mathbf{t}_n||^2
    \] 
    Similarly to Section 5.4.2, our goal is to obtain the outer product
    approximation for the Hessian matrix of the error. Hence,
    computing the Hessian yields:
    \begin{align*}
        \mathbf{H} 
        &= \nabla \nabla E
        = \nabla\bigg(\frac{1}{2} \sum_{n=1}^{N} \nabla ||\mathbf{y}_n - \mathbf{t}_n||^2\bigg)
        = \nabla \bigg(\sum_{n=1}^{N} (\mathbf{y}_n - \mathbf{t}_n)^T \nabla \mathbf{y}_n\bigg) \\
        &= \sum_{n=1}^{N} \nabla \mathbf{y}_n \nabla \mathbf{y}_n^T
        + \sum_{n=1}^{N} (\mathbf{y}_n - \mathbf{t}_n)^T \nabla \nabla \mathbf{y}_n
    \end{align*}
    Neglecting the second term yields the outer product approximation
    for the Hessian matrix:
    \[
        \mathbf{H} \simeq \sum_{n=1}^{N} \nabla \mathbf{y}_n \nabla \mathbf{y}_n^T
    \] 
    which is analogous to $(5.84)$ for $\mathbf{b}_n = \nabla \mathbf{y_n}$
    in the multiple output case.
    Note that, for simplicity all the $\nabla$ symbols refer to $\nabla_\mathbf{w}$ 
\end{proof}

\section*{Exercise 5.17 $\star$}
Consider a squared loss function of the form
\begin{equation}\label{eq:5.193}\tag{5.193}
    E = \frac{1}{2} \iint \{y(\mathbf{x}, \mathbf{w}) - t\}^2 p(\mathbf{x}, t) \diff \mathbf{x} \diff t
\end{equation}
where $y(\mathbf{x}, \mathbf{w})$ is a parametric function such as a neural
network. The result (1.89) shows that the function $y(\mathbf{x}, \mathbf{w})$
that minimizes this error is given by the conditional
expectation of $t$ given $\mathbf{x}$. Use this result to show
that the second derivative of $E$ with respect to two elements
$w_r$ and $w_s$ of the vector $\mathbf{w}$, is given by
\begin{equation}\label{eq:5.194}\tag{5.194}
    \pdv{E}{w_r}{w_s} = \int \pdv{y}{w_r} \pdv{y}{w_s} p(\mathbf{x}) \diff \mathbf{x}
\end{equation}
Note that, for a finite sample form $p(\mathbf{x})$, we obtain
(5.84).

\vspace{1em}

\begin{proof}
    To simplify the notation, we'll denote $y(\mathbf{x}, \mathbf{w})$ as $y$.
    Taking the second derivative of $E$ yields
    \begin{align*}
         \pdv{E}{w_s}{w_r}
         &= \pdv{}{w_s}{w_r} \bigg(\frac{1}{2} \iint \{y - t\}^2 p(\mathbf{x}, t) \diff \mathbf{x} \diff t\bigg) \\
         &= \frac{1}{2} \pdv{w_s} \iint 2(y - t) \pdv{y}{w_r} p(\mathbf{x}, t) \diff \mathbf{x} \diff t \\
         &= \iint \pdv{y}{w_s} \pdv{y}{w_r} p(\mathbf{x}, t) \diff \mathbf{x} \diff t + 
         \iint (y - t)\pdv{y}{w_s}{w_r} p(\mathbf{x}, t) \diff \mathbf{x} \diff t
    \end{align*}
    Using (1.89), i.e. that $y = \mathbb{E}_t[t | \mathbf{x}]$
    minimizes the error, proves that the second integral term is null:
    \begin{align*}
         \iint (y - t)\pdv{y}{w_s}{w_r} p(\mathbf{x}, t) \diff \mathbf{x} \diff t
         &= \int y p(\mathbf{x}) \pdv{y}{w_s}{w_r} \diff \mathbf{x}
         - \int \bigg(\int t p(t | x) \diff t\bigg) p(\mathbf{x}) \pdv{y}{w_s}{w_r} \diff \mathbf{x} \\
         &= \int (y - \mathbb{E}_t[t | \mathbf{x}]) p(\mathbf{x}) \pdv{y}{w_s}{w_r} \diff \mathbf{x} \\
         &= 0
    \end{align*}
    As a result, the second derivative can be written as
    \begin{equation*} \tag{5.194}
        \pdv{E}{w_s}{w_r}
        = \iint \pdv{y}{w_s} \pdv{y}{w_r} p(\mathbf{x}) p(t | \mathbf{x}) \diff \mathbf{x} \diff t
        = \int  \pdv{y}{w_s} \pdv{y}{w_r} p(\mathbf{x}) \diff \mathbf{x}
    \end{equation*}
\end{proof}

\section*{Exercise 5.18 $\star$}
Consider a two-layer network of the form shown in Figure 5.1 with the addition
of extra parameters corresponding to skip-layer connections
that go directly from inputs to the outputs. By extending the discussion of Section 5.3.2,
write down the equations for the derivatives of the error function with respect
to these additional parameters.

\vspace{1em}

\begin{proof}
    Let the weight corresponding to the skip-layer connections be denoted by $w_{ki}^{(s)}$. 
    The outputs will gain an extra sum corresponding to those connections:
    \[
        y_k = \sum_{j} w_{kj}^{(2)} z_j + \sum_{i} w_{ki}^{(s)} x_i
    \] 
    Since $\delta_k$'s functional form remains unchanged, the derivatives
    with respect to the first-layer and second-layer weights remain the same as
    before, i.e. (5.67). The derivative with respect to the skip-layer is now given by
    \[
        \pdv{E_n}{w_{ki}^{(s)}} = \pdv{E_n}{a_k} \pdv{a_k}{w_{ki}^{(s)}}
        = \delta_k x_i
    \] 
    the same as the one of the second-layer.
\end{proof}

\section*{Exercise 5.19 $\star$}
Derive the expression (5.85) for the outer product approximation
to the Hessian matrix for a network having a single output
with a logistic sigmoid output-unit activation function and a cross-entropy
error function, corresponding to the result (5.84) for the
sum-of-squares error function.

\vspace{1em}

\begin{proof}
    For simplicity, let $y_n$ denote $y(\mathbf{x}_n, \mathbf{w)}$.
    Consider a network with the cross-entropy error function
    \begin{equation}\label{eq:5.21}\tag{5.21}
        E(\mathbf{w}) = -\sum_{n=1}^{N} \{t_n \ln y_n + (1 - t_n) \ln(1 - y_n)\}
    \end{equation}
    and a single output with activation
    \[
        y_n = \sigma(a_n)
    \] 
    From $\eqref{eq:4.88}$ one has that
    \[
        \nabla y_n
        = \sigma(a_n)[1 - \sigma(a_n)]\nabla a_n
        = y_n(1 - y_n) \nabla a_n
    \] 
    Using the chain rule of differential calculus,
    \begin{align*}
        \mathbf{H} 
        = \nabla \nabla E(\mathbf{w})
        = \nabla \sum_{n=1}^{N} \pdv{E}{a_n} \pdv{a_n}{\mathbf{w}}
        = \nabla \sum_{n=1}^{N} \pdv{E}{a_n} \nabla a_n
        = \nabla \sum_{n=1}^{N} \pdv{E}{a_n} \nabla a_n
    \end{align*}
    Computing the derivative term is straightforward:
    \[
        \pdv{E}{a_n} 
        = -\bigg\{\frac{t_n}{y_n} \pdv{y_n}{a_n}
        -\frac{1 - t_n}{1 - y_n} \pdv{y_n}{a_n} \bigg\}
        = \frac{y_n(1 - t_n) - t_n(1 - y_n)}{y_n(1 - y_n)} 
        \pdv{y_n}{a_n}
        = y_n - t_n
    \] 
    Substituting this into the initial expression gives
    \begin{align*}
        \mathbf{H} 
        = \nabla  \sum_{n=1}^{N} (y_n - t_n) \nabla a_n
        = \sum_{n=1}^{N} \big\{\nabla y_n \nabla a_n^T + 
        (y_n - t_n) \nabla \nabla a_n\big\}
    \end{align*}
    The second term inside the sum contains the term ($y_n - t_n$), so
    we can neglect it as seen in Section 5.4.2 and arrive at the $\emph{outer 
    approximation}$ for the Hessian matrix by expanding $\nabla y_n$:
    \[
    \mathbf{H} \simeq \sum_{n=1}^{N} y_n(1 - y_n) \nabla a_n \nabla a_n^T
    \] 
    which is equivalent to $(5.85)$ for $\mathbf{b}_n = \nabla a_n$.
    Note that, for simplicity all the $\nabla$ symbols refer to $\nabla_\mathbf{w}$ 
\end{proof}

\section*{Exercise 5.20 $\star$}
Derive an expression for the outer product approximation
to the Hessian matrix for a network having $K$ outputs with a softmax
output-unit activation function and a cross-entropy error function,
corresponding to the result (5.84) for the sum-of-squares error
function.

\vspace{1em}

\begin{proof}
    We'll take a similar approach to the previous exercises. For simplicity,
    let $y_{nk}$ denote $y_{k}(\mathbf{x}_n, \mathbf{w}_k)$. 
    Consider a network with the cross-entropy error function
    \[
        E(\mathbf{w}) = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln y_{nk}  
    \] 
    and $K$ outputs with the softmax activation function
     \[
         y_{nk} = \frac{\exp{a_{nk}}}{\sum_{j=1}^{K} \exp{a_{nj}}} 
    \] 
    Note that both $\mathbf{y}_n$ and $\mathbf{a}_n$ are vectors 
    of size $1 \times K$. Using the chain rule of calculus yields
    \[
        \mathbf{H} 
        = \nabla \nabla E(\mathbf{w})
        = \nabla \sum_{n=1}^{N} \pdv{E}{\mathbf{a}_n} \pdv{\mathbf{a}_n}{\mathbf{w}}
        = \nabla \sum_{n=1}^{N} \pdv{E}{\mathbf{a_n}} \nabla \mathbf{a}_n 
    \] 
    Now, the derivative term will be of size $1 \times K$. The value of the $i$-th element
    will be given by
    \begin{align*}
        \bigg(\pdv{E}{\mathbf{a}_n}\bigg)_i
        = \pdv{E}{a_{ni}}
        = -\sum_{k=1}^{K} t_{nk} \pdv{\ln y_{nk}}{a_{ni}}
        = -\sum_{k=1}^{K} \frac{t_{nk}}{y_{nk}} \pdv{y_{nk}}{a_{ni}}
    \end{align*}
    From Exercise 4.17, respectively $\eqref{eq:4.106}$, we have that
     \[
         \pdv{y_{nk}}{a_{ni}} = y_{nk}(\delta_{ik} - y_{ni})
    \] 
    Therefore,
    \[
        \bigg(\pdv{E}{\mathbf{a}_n}\bigg)_i
        = -\sum_{k=1}^{K} \frac{t_{nk}}{y_{nk}} y_{nk} (\delta_{ik} - y_{ni})
        = -\sum_{k=1}^{K} t_{nk} (\delta_{ik} - y_{ni})
        = y_{ni} \sum_{k=1}^{K} t_{nk} - t_{ni}
        = y_{ni} - t_{ni}
    \] 
    Hence, it's obvious that
    \[
        \pdv{E}{\mathbf{a}_n} = \mathbf{y}_n - \mathbf{t}_n
    \] 
    Substituting this back into the Hessian,
    \begin{align*}
        \mathbf{H} 
        = \nabla \sum_{n=1}^{N} \big(\mathbf{y}_n - \mathbf{t}_n\big) \nabla \mathbf{a}_n
        = \sum_{n=1}^{N} \big\{\nabla \mathbf{y}_n \nabla \mathbf{a}_n 
            + (\mathbf{y}_n - \mathbf{t}_n) \nabla \nabla \mathbf{a}_n\}
    \end{align*}
    As before, the second term inside the sum contains the term $(\mathbf{y}_n - \mathbf{t}_n)$,
    so we can neglect it similarly to what we do in the previous exercises or Section 5.4.2. By ignoring
    the term, one arrives at the $\emph{outer approximation}$ of the Hessian matrix:
    \[
        \mathbf{H} \simeq 
        \sum_{n=1}^{N} \nabla \mathbf{y}_n \nabla \mathbf{a}_n
        = \sum_{n=1}^{N} \pdv{\mathbf{y}_n}{\mathbf{a}_n} \nabla \mathbf{a}_n \nabla \mathbf{a}_n
    \] 
    where the derivative term is a $K \times K$ matrix with the elements
    \[
        \bigg(\pdv{\mathbf{y}_n}{\mathbf{a}_n}\bigg)_{ij}
        = \pdv{y_{ni}}{a_{nj}}
        = y_{ni} (\delta_{ij} - y_{ni})
    \] 
    Note that for notation simplicity, all $\nabla$ symbols refer to $\nabla_\mathbf{w}$.
\end{proof}

\section*{Exercise 5.21 $\star \star \star$ TODO}
Extend the expression (5.86) for the outer product approximation
of the Hessian matrix to the case of $K > 1$ output units. Hence,
derive a recursive expression analogous to (5.87) for incrementing
the number $N$ of patterns and a similar expression for incrementing
the number $K$ of outputs. Use these results, together with the identity
(5.88), to find sequential update expressions analogous to (5.89) for
finding the inverse of the Hessian by incrementally including both extra patterns
and extra outputs.
\vspace{1em}

\begin{proof}
    
\end{proof}

\section*{Exercise 5.22 $\star \star$}
Derive the results $\eqref{eq:5.93}$, $\eqref{eq:5.94}$, and $\eqref{eq:5.95}$ for
the elements of the Hessian matrix of a two-layer feed-forward network by application of
the chain rule of calculus.

\vspace{1em}

\begin{proof}
    As seen in Section 5.4.5, we consider the three separate blocks of the Hessian:
    \begin{enumerate}
        \item Both weights are in the second layer:
            \begin{align*}
                \pdv{E_n}{w_{kj}^{(2)}}{w_{k'j'}^{(2)}} 
                &= \pdv{w_{kj}^{(2)}} \bigg(\pdv{E_n}{w_{k'j'}^{(2)}}\bigg) \\
                &= \pdv{w_{kj}^{(2)}} \bigg(\pdv{E_n}{a_{k'}} \pdv{a_{k'}}{w_{k'j'}^{(2)}}\bigg) \\
                &= \pdv{w_{kj}^{(2)}} \bigg(\pdv{E_n}{a_{k'}} z_{j'}\bigg) \\
                &= z_{j'} \pdv{a_{k'}} \bigg(\pdv{E_n}{a_k} \pdv{a_k}{w_{kj}^{(2)}}\bigg) \\
                &= z_{j}z_{j'} \pdv{E_n}{a_k}{a_{k'}} \\
                &= z_j z_{j'} M_{kk'} \label{eq:5.93}\tag{5.93}
            \end{align*}
        \item Both weights are in the first layer:
            \begin{align*}
                \pdv{E_n}{w_{ji}^{(1)}}{w_{j'i'}^{(1)}}
                &= \pdv{w_{ji}^{(1)}} \bigg(\pdv{E_n}{w_{j'i'}^{(1)}}\bigg) \\
                &= \pdv{w_{ji}^{(1)}} \bigg(\pdv{E_n}{a_{j'}} \pdv{a_{j'}}{w_{j'i'}^{(1)}}\bigg) \\
                &= \pdv{w_{ji}^{(1)}} \big(x_{i'} \delta_{j'}\big)
            \end{align*}
        Using the backpropagation formula $(5.56)$, we have that
        \begin{align*}
                \pdv{E_n}{w_{ji}^{(1)}}{w_{j'i'}^{(1)}}
                &= \pdv{w_{ji}^{(1)}} \bigg(x_{i'} h'(a_{j'}) \sum_{k} w_{kj'}^{(2)} \delta_k\bigg) \\
                &= x_{i'} \pdv{w_{ji}^{(1)}} \big(h'(a_{j'})\big)\sum_{k} \big(w_{kj'}^{(2)} \delta_k\big) +
                    x_{i'} h'(a_{j'}) \sum_{k} w_{kj'}^{(2)} \pdv{\delta_k}{w_{ji}^{(1)}}
        \end{align*}
        For $j \neq j'$ the derivative in the first term is null. As a result, the first term
        can be written as
        \begin{align*}
            x_{i'} \pdv{w_{ji}^{(1)}} \big(h'(a_{j'})\big) \sum_{k} w_{kj'}^{(2)} \delta_
            &= \mathbf{I}_{jj'} x_{i'} \pdv{w_{j'i}^{(1)}} \big(h'(a_{j'})\big) \sum_{k} w_{kj'}^{(2)} \delta_k \\
            &= \mathbf{I}_{jj'} x_i x_{i'} h''(a_{j'}) \sum_{k} w_{kj'}^{(2)} \delta_k
        \end{align*}
        Now, let's compute the derivative in the second term:
        \begin{align*}
            \pdv{\delta_k}{w_{ji}^{(1)}} 
            = \sum_{k'} \pdv{\delta_k}{a_{k'}} \pdv{a_{k'}}{w_{ji}^{(1)}}
            = \sum_{k'} \pdv{E_n}{a_k}{a_k'} \pdv{a_k'}{w_{ji}^{(1)}}
            &= \sum_{k'} M_{kk'} \pdv{w_{ji}^{(1)}}\bigg(\sum_{j} w_{k'j}^{(2)} h(x_i w_{ji}^{(1)})\bigg) \\
            &= x_i h'(a_j) \sum_{k'} M_{kk'} w_{k'j}^{(2)}
        \end{align*}
        Putting everything together yields the desired result:
        \begin{align*}
            \pdv{E_n}{w_{ji}^{(1)}}{w_{j'i'}^{(1)}}
            = \mathbf{I}_{jj'} x_i x_{i'} h''(a_{j'}) \sum_{k} w_{kj'}^{(2)} \delta_k +
            x_i x_{i'} h'(a_j) h'(a_{j'}) \sum_{k} \sum_{k'} w_{kj'}^{(2)} w_{k'j}^{(2)} M_{kk'}
            \label{eq:5.94}\tag{5.94}
        \end{align*}
        Note that this result is equivalent with the one in the book even if the $k$ and $k'$ are
        interchanged in the second term. This is because the sum ranges are chosen arbitrarily.
    \item One weight in each layer:
        \begin{align*}
            \pdv{E_n}{w_{ji}^{(1)}}{w_{kj'}^{(2)}}
            &= \pdv{w_{ji}^{(1)}} \bigg(\pdv{E_n}{w_{kj'}^{(2)}}\bigg) \\
            &= \pdv{w_{ji}^{(1)}} \bigg(\pdv{E_n}{a_{k}} \pdv{a_k}{w_{kj'}^{(2)}}\bigg) \\
            &= \pdv{w_{ji}^{(1)}} \big(\delta_{k} z_{j'}\big) \\ 
            &= z_{j'} \pdv{\delta_k}{w_{ji}^{(1)}} + \delta_k \pdv{z_{j'}}{w_{ji}^{(1)}}
        \end{align*}
        We found the value of the first term in the previous case. Also, the derivative
        in the second term is null for $j \neq j'$. Therefore, the above expression becomes
        \begin{align*}
            \pdv{E_n}{w_{ji}^{(1)}}{w_{kj'}^{(2)}}
            &= z_{j'} x_i h'(a_j) \sum_{k'} M_{kk'} w_{k'j}^{(2)} + 
            \mathbf{I}_{jj'} \delta_k x_i h'(a_j) \\
            &= x_i h'(a_j) \bigg\{\delta_k \mathbf{I}_{jj'} + z_{j'} \sum_{k'} w_{k'j}^{(2)} M_{kk'}\bigg\} 
            \label{eq:5.95}\tag{5.95}
        \end{align*}
    \end{enumerate}
\end{proof}

\section*{Exercise 5.23 $\star \star$}
Extend the results of Section 5.4.5 for the exact Hessian of two-layer network
to include skip-layer connections that go directly from input to outputs.

\vspace{1em}

\begin{proof}
    Let's denote the skip layer by the $(s)$ superscript. One has 3 cases for
    the weight combinations that include skip weights:
    \begin{enumerate}
        \item The non-skip activation is in the second layer:
            \begin{align*}
                \pdv{E_n}{w_{ki}^{(s)}}{w_{k'j}^{(2)}}
                &= \pdv{w_{ki}^{(s)}} \bigg(\pdv{E_n}{w_{k'j}^{(2)}}\bigg) \\
                &= \pdv{w_{ki}^{(s)}}\bigg(\pdv{E_n}{a_{k'}} \pdv{a_{k'}}{w_{k'j}^{(2)}}\bigg) \\
                &= \pdv{w_{ki}^{(s)}}\big(\delta_{k'} z_j\big) \\
                &= z_j \pdv{\delta_{k'}}{w_{ki}^{(s)}}
            \end{align*}
            Computing the derivative term separately yields
            \begin{align*}
                \pdv{\delta_{k'}}{w_{ki}^{(s)}}
                = \pdv{\delta_{k'}}{a_k} \pdv{a_k}{w_{ki}^{(s)}}
                = \pdv{E_n}{a_k}{a_{k'}} x_i
                = M_{kk'} x_i
            \end{align*}
            Hence,
            \begin{align*}
                \pdv{E_n}{w_{ki}^{(s)}}{w_{k'j}^{(2)}}
                = x_i z_j M_{kk'}
            \end{align*}
        \item The non-skip activation is in the first layer:
            \begin{align*}
                \pdv{E_n}{w_{ki}^{(s)}}{w_{ji'}^{(1)}}
                &= \pdv{w_{ki}^{(s)}} \bigg(\pdv{E_n}{w_{ji'}^{(1)}}\bigg) \\
                &= \pdv{w_{ki}^{(s)}} \bigg(\pdv{E_n}{a_j} \pdv{a_j}{w_{ji'}^{(1)}}\bigg) \\
                &= \pdv{w_{ki}^{(s)}} \big(x_{i'} \delta_{j}\big) \\
                &= x_{i'} \pdv{\delta_j}{w_{ki}^{(s)}} 
            \end{align*}
            Using the back-propagation formula $(5.56)$ gives
            \begin{align*}
                \pdv{E_n}{w_{ki}^{(s)}}{w_{ji'}^{(1)}}
                &= x_{i'} \pdv{\delta_j}{w_{ki}^{(s)}}\bigg(h'(a_j) \sum_{k'} w_{k'j}^{(2)} \delta_{k'}\bigg)  \\
                &= x_{i'} h'(a_j) \sum_{k'} w_{k'j}^{(2)} \pdv{\delta_{k'}}{w_{ki}^{(s)}} 
            \end{align*}
            We've already computed the derivative term in the last case. Therefore,
            \[
                \pdv{E_n}{w_{ki}^{(s)}}{w_{ji'}^{(1)}}
                = x_i x_{i'} h'(a_j) \sum_{k'} w_{k' j}^{(2)} M_{kk'}
            \] 
        \item Both weights are skip weights:
            \begin{align*}
                \pdv{E_n}{w_{ki}^{(s)}}{w_{k'i'}^{(s)}}
                &= \pdv{w_{ki}^{(s)}} \bigg(\pdv{E_n}{w_{k'i'}^{(s)}}\bigg) \\
                &= \pdv{w_{ki}^{(s)}} \bigg(\pdv{E_n}{a_{k'}} \pdv{a_{k'}}{w_{k'i'}^{(s)}}\bigg) \\
                &= \pdv{w_{ki}^{(s)}} \big(\delta_{k'} x_{i'}\big) \\
                &= x_{i'} \pdv{\delta_{k'}}{w_{ki}^{(s)}} 
            \end{align*}
            We've already computed the derivative term in the first case. As a result,
            \[
                \pdv{E_n}{w_{ki}^{(s)}}{w_{k'i'}^{(s)}}
                = x_{i} x_{i'} M_{kk'}
            \] 
    \end{enumerate}
\end{proof}

\section*{Exercise 5.24 $\star \star$}
Verify that the network function defined by (5.113) and (5.114) is invariant
under the transformation (5.115) applied to the inputs,
provided the weights and biases are simultaneously transformed using
(5.116) and (5.117). Similarly, show that the network outputs can be
transformed according to (5.118) by applying the transformation (5.119)
and (5.120) to the second-layer weights and biases.

\vspace{1em}

\begin{proof}
    Let's make the transformations (5.115), (5.116) and (5.117) and check the new
    value of $\widetilde{a_j}$.
    \[
        \widetilde{a_j} 
        = \sum_{i} \widetilde{w}_{ji} \widetilde{x}_i + \widetilde{w}_{j0} 
        = \sum_{i} \frac{1}{a} w_{ji} \big(ax_i + b) + w_{j0} - \frac{b}{a} \sum_{i} w_{ji} 
        = \sum_{i} w_{ji} x_i + w_{j0} 
        = a_j
    \] 
    Since the activations of the hidden layer remain the same after the transformation,
    we can conclude that the outputs are invariant under the above transformations.
    Now, let's apply the transformations (5.119) and (5.120) to the second-layer weights
    and biases. The new outputs look like
    \[
        \widetilde{y}_k 
        = \sum_{j} \widetilde{w}_{kj} z_j + \widetilde{w}_{k0}
        = \sum_{j} c w_{kj} z_j + cw_{k0} + d
        = c\bigg(\sum_{j} w_{kj}z_j + w_{k0}\bigg) + d
        = cy_k + d
    \] 
    which proves that the network outputs can be transformed as in (5.118).
\end{proof}

\section*{Exercise 5.25 $\star \star \star$}
Consider a quadratic error function of the form
 \begin{equation}\label{eq:5.195}\tag{5.195}
     E = E_0 + \frac{1}{2}(\mathbf{w} - \mathbf{w}^\star)^T \mathbf{H} (\mathbf{w} - \mathbf{w}^\star)
\end{equation}
where $\mathbf{w}^*$ represents the minimum, and the Hessian matrix $\mathbf{H}$ is positive
definite and constant. Suppose the initial weight vector $\mathbf{w}^{(0)}$ is chosen to be at the
origin and is updated using simple gradient descent
\begin{equation}\label{eq:5.196}\tag{5.196}
    \mathbf{w}^{(\tau)} = \mathbf{w}^{(\tau - 1)} - \rho \nabla E
\end{equation}
where $\tau$ denotes the step number, and $\rho$ is the learning
rate (which is assumed to be small). Show that, after $\tau$ steps,
the components of the weight vector parallel to the eigenvectors of 
 $\mathbf{H}$ can be written
 \begin{equation}\label{eq:5.197}\tag{5.197}
     w_j^{(\tau)} = \{1 - (1 - \rho \eta_j)^\tau\} w_j^\star
 \end{equation}
 where $w_j = \mathbf{w}^T \mathbf{u}_j$, $\mathbf{u}_j$ and $\eta_j$ are the eigenvectors and eigenvalues,
 respectively, of $\mathbf{H}$ so that
 \begin{equation}\label{eq:5.198}\tag{5.198}
     \mathbf{H}\mathbf{u}_j = \eta_j \mathbf{u}_j
 \end{equation}
 Show that as $\tau \to \infty$, this gives $\mathbf{w}^{(\tau)} \to \mathbf{w}^\star$ as expected,
 provided $|1 - \rho \eta_j| < 1$. Now suppose that training is halted after a finite number $\tau$ 
 of steps. Show that the components of the weight vector parallel to the eigenvectors of the
 Hessian satisfy 
 \begin{equation}\label{eq:5.199}\tag{5.199}
     w_j^{(\tau)} \simeq w_j^\star \text{ when } \eta_j \gg (\rho \tau)^{-1}
\end{equation}
\begin{equation}\label{eq:5.200}\tag{5.200}
     |w_j^{(\tau)}| \ll |w_j^\star| \text{ when } \eta_j \ll (\rho \tau)^{-1}
\end{equation}
Compare this result with the discussion in Section 3.5.3 of regularization with simple
weight decay, and hence show that $(\rho \tau)^{-1}$ is analogous to the regularization
parameter $\lambda$. The above results also show that the effective number of parameters in the 
network, as defined by (3.91), grows as the training progresses.

\vspace{1em}

\begin{proof}
    Taking the gradient of the error with respect to $\mathbf{w}$ yields
    \[
        \nabla E = \nabla\bigg(E_0 
            + \frac{1}{2}(\mathbf{w} - \mathbf{w}^\star)^T\mathbf{H} (\mathbf{w} - \mathbf{w}^\star)\bigg)
        = \mathbf{H}(\mathbf{w} - \mathbf{w}^\star)
    \] 
    Substituting this into $\eqref{eq:5.196}$ gives
    \[
        \mathbf{w}^{(\tau)} 
        = \mathbf{w}^{(\tau - 1)} - \rho \mathbf{H} (\mathbf{w}^{(\tau - 1)} - \mathbf{w}^\star)
    \] 
    Now, we left-multiply by $\mathbf{u}_j^T$ both sides of the expression and use the
    fact that $w_j = \mathbf{w}^T\mathbf{u}_j$, along with $\eqref{eq:5.198}$ to obtain that
    \[
        w_j^{(\tau)} = w_j^{(\tau - 1)} - \rho \eta_j w^{(\tau - 1)}_j + \rho \eta_j w_j^\star
        = (1 - \rho \eta_j) w_j^{(\tau - 1)} + \rho \eta_j w_j^\star
    \] 
    Let's prove $\eqref{eq:5.197}$ by induction. The base case for $\tau = 1$ is obviously holding since
    $\mathbf{w}^{(0)}$ is the origin:
    \[
        w_j^{(1)} 
        = (1 - \rho \eta_j) w_j^{(0)} + \rho \eta_j w_j^\star
        = \rho \eta_j w_j^\star
        = \{1 - (1 - \rho \eta_j)\} w_j^\star
    \] 
    For the general case, let $\tau = t \in \mathbb{N}$ and assume that
    \eqref{eq:5.197} holds:
    \[
        w_j^{(t)} = \{1 - (1 - \rho \eta_j)^t\} w_j^\star
    \] 
    Substituting the value of $w_j^{(t)}$ into
    \begin{align*}
        w_j^{(t + 1)} = (1 - \rho \eta_j)w_j^{(t)} + \rho \eta_j w_j^\star
    \end{align*}
    gives
    \begin{align*}
        w_j^{(t + 1)} 
        &= (1 - \rho \eta_j) \{1 - (1 - \rho \eta_j)^t\} 
            w_j^\star + \rho \eta_j w_j^\star \\
        &= \big\{(1 - \rho \eta_j) - (1 - \rho \eta_j)^{t + 1} + \rho \eta_j\big\} w_j^\star \\
        &= \{1 - (1 - \rho \eta_j)^{(t + 1)}\} w_j^\star
    \end{align*}
    Since the base case and general recursive implication holds, we proved by induction that 
    \eqref{eq:5.197} holds. Now, taking the number of steps $\tau$ to infinity yields
    \[
        \lim_{\tau \to \infty} w_j^{(\tau)}
        = \lim_{\tau \to \infty} \{1 - (1 - \rho \eta_j)^\tau\} w_j^\star
        = w_j^\star
    \] 
    for $|1 - \rho \eta_j| < 1$ since as $\tau \to \infty$, one has that $(1 - \rho \eta_j)^\tau \to 0$.
    Since $\eta_j \rho \tau \gg 1$ and $|1 - \rho \eta_j| < 1, \tau$ must be large. Therefore,
    as proved above, $w_j^{(\tau)} \simeq w_j^\star$. Now, since $\eta_j \rho \tau \ll 1$ and
     $\tau$ is finite, $\rho \tau$ must be very small. We use this fact by expanding the polynomial and 
     ignoring the higher order terms:
    \begin{align*}
        |w_j^{(\tau)}| 
        &= |1 - (1 - \rho \eta_j)^\tau| |w_j^\star| \\
        &= |1 - \big(1 - \tau \rho \eta_j + O(\rho^2 \eta_j^2)\big)| |w_j^\star| \\
        &\simeq |\tau \rho \eta_j| |w_j^\star| \\
        &\ll |w_j^\star|
    \end{align*}
\end{proof}

\section*{Exercise 5.26 $\star \star$}
Consider a multilayer perceptron with arbitrary feed-forward topology, 
which is to be trained by minimzing the \emph{tangent propagation} error
function in which the regularizing function is given by
$\eqref{eq:5.128}$. Show that the regularization term $\Omega$ can
be written as a sum over patterns of terms of the form
\begin{equation}\label{eq:5.201}\tag{5.201}
    \Omega_n = \frac{1}{2} \sum_{k} (\mathcal{G}y_{nk})^2
\end{equation}
where $\mathcal{G}$ is a differential operator defined by
 \begin{equation}\label{eq:5.202}\tag{5.202}
     \mathcal{G} \equiv \sum_{i} \tau_i \pdv{x_i}
\end{equation}
By acting on the forward propagation equations
\begin{equation}\label{eq:5.203}\tag{5.203}
    z_j = h(a_j),
    \hspace{3em}
    a_j = \sum_{i} w_{ji} z_i
\end{equation}
with the operator $\mathcal{G}$, show that $\Omega_n$ 
can be evaluated by forward propagation using the following equations:
\begin{equation}\label{eq:5.204}\tag{5.204}
    \alpha_j = h'(a_j) \beta_j,
    \hspace{3em}
    \beta_j = \sum_{i} w_{ji} \alpha_i
\end{equation}
where we have defined the new variables
\begin{equation}\label{eq:5.205}\tag{5.205}
    \alpha_j \equiv \mathcal{G} z_j,
    \hspace{3em}
    \beta_j \equiv \mathcal{G} a_j
\end{equation}
Now show that the derivatives of $\Omega_n$ with respect to a weight $w_{rs}$ in the network 
can be written in the form  
\begin{equation}\label{eq:5.206}\tag{5.206}
    \pdv{\Omega_n}{w_{rs}} = \sum_{k} \alpha_k \{\phi_{kr} z_s + \delta_{kr} \alpha_s\}
\end{equation}
where we have defined 
\begin{equation}\label{eq:5.207}\tag{5.207}
    \delta_{kr} \equiv \pdv{y_k}{a_r},
    \hspace{3em}
    \phi_{kr} = \mathcal{G} \delta_{kr}.
\end{equation}
Write down the backpropagation equations for $\delta_{kr}$, and hence derive
the set of backpropagation equations for the evaluation of $\phi_{kr}$.

\vspace{1em}

\begin{proof}
    Simply evaluating \eqref{eq:5.201} using \eqref{eq:5.202} gives
    \[
        \Omega_n 
        = \frac{1}{2}\sum_{k}(\mathcal{G} y_{nk})^2
        = \frac{1}{2}\sum_{k}\bigg(\sum_{i} \tau_{ni} \pdv{y_{nk}}{x_{ni}}\bigg)^2
        = \frac{1}{2}\sum_{k}\bigg(\sum_{i} \tau_{ni} J_{nki}\bigg)^2
    \] 
    where 
    $J_{nki}$ is the $(k, i)$-th element of the Jacobian for the $n$-th observation.
    Summing the above expression over $n$ yields the regularization function:
    \begin{equation}\label{eq:5.128}\tag{5.128}
        \sum_{n} \Omega_n = \frac{1}{2} \sum_{n} \bigg(\sum_{k} \tau_{ni} J_{nki}\bigg)^2 = \Omega
    \end{equation}
    By applying the differential operator $\mathcal{G}$ on the forward propagation equations
    \eqref{eq:5.203}, one obtains the same results as \eqref{eq:5.204}:
    \[
        \alpha_j 
        = \mathcal{G}z_j 
        = \sum_{i} \tau_i \pdv{z_j}{x_i}
        = \sum_{i} \tau_i \pdv{z_j}{a_j} \pdv{a_j}{x_i}
        = h'(a_j) \sum_{i} \tau_i \pdv{a_j}{x_i}
        = h'(a_j) \mathcal{G} a_j
        = h'(a_j) \beta_j
    \] 
    \[
        \beta_j
        = \mathcal{G}a_j
        = \sum_{i} \tau_i \pdv{x_i}\bigg(\sum_{i'} w_{ji'} z_{i'}\bigg)
        = \sum_{i} \tau_i \sum_{i'} w_{ji'} \pdv{z_{i'}}{x_i}
        = \sum_{i'} w_{ji'} \mathcal{G} z_{i'}
        = \sum_{i'} w_{ji'} \alpha_{i'}
    \] 
\end{proof}
We notice that we can rewrite $\mathcal{G} y_{k}$ as
\begin{align*}
    \mathcal{G}y_k 
    &= \sum_{i}^{} \tau_i \pdv{y_k}{x_i} \\
    &= \sum_{i}^{} \tau_i \pdv{y_k}{a_k} \sum_{j} \pdv{a_k}{z_j} \pdv{z_j}{x_i} \\
    &= h'(a_k) \sum_{i}^{} \tau_i \sum_{j}^{} w_{kj} \pdv{z_j}{x_i} \\
    &= h'(a_k) \sum_{j}^{} w_{kj} \sum_{i}^{} \tau_i \pdv{z_j}{x_i} \\
    &= h'(a_k) \sum_{j}^{} w_{kj} \mathcal{G} z_j \\
    &= h'(a_k) \beta_k \\
    &= \alpha_k
\end{align*}
Since $\alpha_j$ can be obtained from $a_j$ and $\beta_j$ (see \eqref{eq:5.204}),
this is proof that $\Omega_n$ can be evaluated by forward propagation using the 
equations \eqref{eq:5.204} successively. We can see that the derivative
of the differential operator can be written as
\[
    \pdv{\mathcal{G} f}{\gamma}
    = \pdv{\gamma} \sum_{i}^{} \tau_i \pdv{f}{x_i}
    = \sum_{i}^{} \tau_i \pdv{x_i} \pdv{f}{\gamma}
    = \mathcal{G}\bigg(\pdv{f}{\gamma}\bigg)
\] 
Therefore, the derivatives of the regularizers $\Omega_n$ with respect
to a weight $w_{rs}$ can be written as
\begin{align*}
    \pdv{\Omega_n}{w_{rs}} 
    = \pdv{w_{rs}} \bigg(\frac{1}{2} \sum_{k}^{} \big(\mathcal{G} y_k\big)^2\bigg)
    = \sum_{k}^{} \mathcal{G} y_k \pdv{\mathcal{G}y_k}{w_{rs}}
    = \sum_{k}^{} \alpha_k \mathcal{G}\bigg(\pdv{y_k}{w_{rs}}\bigg)
    &= \sum_{k}^{} \alpha_k \mathcal{G}\bigg(\pdv{y_k}{a_r} \pdv{a_r}{w_{rs}}\bigg) \\
    &= \sum_{k}^{} \alpha_k \mathcal{G}\bigg(\pdv{y_k}{a_r} z_s\bigg)
\end{align*}
Applying the product rule on the obtained expression and substituting the variables
defined in \eqref{eq:5.207} yields
\begin{align*}
    \pdv{\Omega_n}{w_{rs}} 
    = \sum_{k}^{} \alpha_k \{z_s \mathcal{G} \delta_{kr} + \delta_{kr} \mathcal{G} z_s\}
    = \sum_{k}^{} \alpha_k \{\phi_{kr} z_s + \delta_{kr} \alpha_s\} \tag{5.206}
\end{align*}
The backpropagation formula for $\delta_{kr}$ is obtained similarly to the one in Section 5.3.1.
Suppose that the units $l$ come after the units $r$. Then,
\begin{align*}
    \delta_{kr} 
    = \sum_{l}^{} \pdv{y_k}{a_l} \pdv{a_l}{a_r} 
    = \sum_{l}^{} \pdv{y_k}{a_l} \pdv{a_l}{z_r} \pdv{z_r}{a_r} 
    = h'(a_r) \sum_{l}^{} w_{lr} \delta_{kl}
\end{align*}
As a result, the backpropagation equations for $\phi_{kr}$ can be obtained
by applying the differential operator $\mathcal{G}$ on this result:
\begin{align*}
    \phi_{kr} 
    &= \mathcal{G} \delta_{kr} \\
    &= \mathcal{G}\bigg(h'(a_r) \sum_{l}^{} w_{lr} \delta_{kl}\bigg) \\
    &= \mathcal{G} \big(h'(a_r)\big) \sum_{l}^{} w_{lr} \delta_{kl} 
        + h'(a_r) \sum_{l}^{} w_{lr} \mathcal{G}(\delta_{kl})
\end{align*}
\vspace{1em}

\section*{Exercise 5.27 $\star \star$}
Consider the framework for training with transformed data in the special case
in which the transformation consists simply of the addition of random noise 
$\mathbf{x} \to \mathbf{x} + \bm{\xi}$ where $\bm{\xi}$ has a 
Gaussian distribution with zero mean and unit covariance. By following an argument analogous
to that of Section 5.5.5, show that the resulting regularizer reduces to the Tikhonov
from \eqref{eq:5.135}.

\vspace{1em}

\begin{proof}
    Using the same arguments as in Section 5.5.5, we have that the regularizer
    is given by
    \begin{equation}\label{eq:5.134}\tag{5.134}
        \Omega = \frac{1}{2} \int_{}^{} \big(\bm{\tau}^T 
        \nabla y(\mathbf{x})\big)^2 p(\mathbf{x}) \diff \mathbf{x}
    \end{equation}
    Under our specific transformation, we have that
    \[
        \bm{\tau} = \pdv{s(\mathbf{x}, \bm{\xi})}{\bm{\xi}} \bigg|_{\bm{\xi} = \bm{0}}
        = \pdv{\bm{\xi}} \big(\mathbf{x} + \bm{\xi}\big) \bigg|_{\bm{\xi} = \bm{0}}
        = \mathbf{I}
    \] 
    Hence, substituting back into \eqref{eq:5.134} gives the Tikhonov form
    \begin{equation}\label{eq:5.135}\tag{5.135}
        \Omega = \frac{1}{2} \int_{}^{} \norm{\nabla y(\mathbf{x})}^2 p(\mathbf{x}) \diff x
    \end{equation}
\end{proof}

\section*{Exercise 5.29 $\star$}
Verify the result \eqref{eq:5.141}. (See PRML errata for the removal of $\lambda$ factors).

\vspace{1em}

\begin{proof}

    Taking the derivative of the total error function 
    \begin{equation}\label{eq:5.139}\tag{5.139}
        \widetilde{E}(\mathbf{w}) = E(\mathbf{w}) + \Omega(\mathbf{w})
    \end{equation}
    with respect to $w_i$ yields
    \begin{align*}
        \pdv{\widetilde{E}}{w_i} = \pdv{E}{w_i} + \pdv{\Omega}{w_i}
    \end{align*}
    Our goal is to compute the second derivative term. Hence,
    \begin{align*}
        \pdv{\Omega}{w_i} 
        &= \pdv{w_i} \bigg[-\sum_{i}^{} \ln\bigg(\sum_{j}^{} \pi_j \mathcal{N}(w_i | \mu_j, \sigma_j^2)\bigg)\bigg] \\
        &= \bigg(\sum_{k}^{} \pi_k \mathcal{N}(w_i | \mu_k, \sigma_k^2)\bigg)^{-1}
            \sum_{j}^{} \pi_j \pdv{w_i} \mathcal{N}(w_i | \mu_j, \sigma_j^2) \\
        &= \bigg(\sum_{k}^{} \pi_k \mathcal{N}(w_i | \mu_k, \sigma_k^2)\bigg)^{-1}
            \sum_{j}^{} \pi_j \frac{w_i - \mu_j}{\sigma_j^2} \mathcal{N}(w_i | \mu_j, \sigma_j^2) \\
        &= \sum_{j}^{} \frac{\pi_j \mathcal{N}(w_i | \mu_j, \sigma_j^2)} 
            {\sum_{k}^{} \pi_k \mathcal{N}(w_i | \mu_k, \sigma_k^2)} \frac{w_i - \mu_j}{\sigma_j^2} \\
        &= \sum_{j}^{} \gamma_j(w_i) \frac{w_i - \mu_j}{\sigma_j^2}
    \end{align*}
    where $\gamma_j(w)$ is defined by (5.140). 
    Substituting into the expression above, one obtains
    \begin{equation}\label{eq:5.141}\tag{5.141}
        \pdv{\widetilde{E}}{w_i} = \pdv{E}{w_i} + \sum_{j}^{} \gamma_j(w_i) \frac{w_i - \mu_j}{\sigma_j^2}
    \end{equation}
\end{proof}

\section*{Exercise 5.30 $\star$}
Verify the result \eqref{eq:5.142}. (See PRML errata for the removal of $\lambda$ factors).

\vspace{1em}

\begin{proof}
    Similarly to the previous exercise, we're going to compute the derivative
    of $\Omega(\mathbf{w})$ with respect to $\mu_j$:
    \begin{align*}
        \pdv{\Omega}{\mu_j} 
        &= \pdv{\mu_j} \bigg[-\sum_{i}^{} \ln\bigg(\sum_{j}^{} \pi_j \mathcal{N}(w_i | \mu_j, \sigma_j^2)\bigg)\bigg] \\
        &= \sum_{i}^{} \bigg[\bigg(\sum_{k}^{} \pi_k \mathcal{N}(w_i | \mu_k, \sigma_k^2)\bigg)^{-1}
            \pi_j \pdv{\mu_j} \mathcal{N}(w_i | \mu_j, \sigma_j^2)\bigg] \\
        &= \sum_{i}^{} \bigg[\bigg(\sum_{k}^{} \pi_k \mathcal{N}(w_i | \mu_k, \sigma_k^2)\bigg)^{-1}
            \sum_{j}^{} \pi_j \frac{\mu_j - w_i}{\sigma_j^2} \mathcal{N}(w_i | \mu_j, \sigma_j^2)\bigg] \\
        &= \sum_{i}^{} \frac{\pi_j \mathcal{N}(w_i | \mu_j, \sigma_j^2)} 
            {\sum_{k}^{} \pi_k \mathcal{N}(w_i | \mu_k, \sigma_k^2)} \frac{\mu_j - w_i}{\sigma_j^2} \\
        &= \sum_{i}^{} \gamma_j(w_i) \frac{\mu_j - w_i}{\sigma_j^2}
    \end{align*}
    Since $E(\mathbf{w})$ has no dependence on $\mu_j$ the derivative term reduces, so
    \begin{equation}\label{eq:5.142}\tag{5.143}
        \pdv{\widetilde{E}}{\mu_j} = \pdv{E}{\mu_j} + \pdv{\Omega}{\mu_j}
        = \pdv{\Omega}{\mu_j}
        = \sum_{i}^{} \gamma_j(w_i) \frac{\mu_j - w_i}{\sigma_j^2}
    \end{equation}
\end{proof}

\section*{Exercise 5.31 $\star$}
Verify the result \eqref{eq:5.143}. (See PRML errata for the removal of $\lambda$ factors).

\begin{proof}
\vspace{1em}
    Like in the previous exercise, since $E(\mathbf{w})$ does not depend on the distribution of $\mathbf{w}$,
    \[
            \pdv{\widetilde{E}}{\sigma_j} 
            = \pdv{E}{\sigma_j} + \pdv{\Omega}{\sigma_j}
            = \pdv{\Omega}{\sigma_j}
    \] 
    The derivative term is now given by
    \begin{align*}
        \pdv{\Omega}{\sigma_j} 
        &= \pdv{\sigma_j} \bigg[-\sum_{i}^{} \ln\bigg(\sum_{j}^{} \pi_j \mathcal{N}(w_i | \mu_j, \sigma_j^2)\bigg)\bigg] \\
        &= \sum_{i}^{} \bigg[\bigg(\sum_{k}^{} \pi_k \mathcal{N}(w_i | \mu_k, \sigma_k^2)\bigg)^{-1}
        \pi_j \pdv{\sigma_j} \mathcal{N}(w_i | \mu_j, \sigma_j^2)\bigg] \\
        &= \sum_{i}^{} \bigg[\bigg(\sum_{k}^{} \pi_k \mathcal{N}(w_i | \mu_k, \sigma_k^2)\bigg)^{-1}
            \pi_j \bigg(\frac{1}{\sigma_j} - \frac{(w_i - \mu_j)^2}{\sigma_j^3}\bigg) 
        \mathcal{N}(w_i | \mu_j, \sigma_j^2)\bigg] \\
        &= \sum_{i}^{} \frac{\pi_j \mathcal{N}(w_i | \mu_j, \sigma_j^2)} 
            {\sum_{k}^{} \pi_k \mathcal{N}(w_i | \mu_k, \sigma_k^2)}
            \bigg(\frac{1}{\sigma_j} - \frac{(w_i - \mu_j)^2}{\sigma_j^3}\bigg) \\
        &= \sum_{i}^{} \gamma_j(w_i)
            \bigg(\frac{1}{\sigma_j} - \frac{(w_i - \mu_j)^2}{\sigma_j^3}\bigg) 
    \end{align*}
    Therefore,
    \begin{equation}\label{eq:5.143}\tag{5.143}
        \pdv{\widetilde{E}}{\sigma_j}
        = \sum_{i}^{} \gamma_j(w_i)
            \bigg(\frac{1}{\sigma_j} - \frac{(w_i - \mu_j)^2}{\sigma_j^3}\bigg) 
    \end{equation}
\end{proof}

\section*{Exercise 5.32 $\star \star$}
Show that the derivatives of the mixing coefficients $\{\pi_k\}$, defined
by \eqref{eq:5.146}, with respect to the auxiliary parameters $\{\eta_j\}$ are 
given by  
\begin{equation}\label{eq:5.208}\tag{5.208}
    \pdv{\pi_k}{\eta_j} = \delta_{jk} \pi_j - \pi_j \pi_k
\end{equation}
Hence, by making use of the constraint $\sum_{k}^{} \pi_k = 1$, derive
the result \eqref{eq:5.147}.

\vspace{1em}

\begin{proof}
    The mixing coefficients are given by the softmax function
    \begin{equation}\label{eq:5.146}\tag{5.146}
        \pi_j = \frac{\exp{\eta_j}}{\sum_{k}^{} \exp{\eta_k}}
    \end{equation}
    Therefore, as seen in Exercise 4.17, the derivative of $\pi_j$ is 
    given by 
    \begin{equation}\label{eq:5.208}\tag{5.208}
        \pdv{\pi_k}{\eta_j} = \delta_{jk} \pi_j - \pi_j \pi_k
    \end{equation}
    Since $E(\mathbf{w})$ does not depend on the distribution of
    $\mathbf{w}$, the derivative of the total error is given by
    \begin{align*}
        \pdv{\widetilde{E}}{\eta_j}
        = \pdv{\Omega}{\eta_j}
        &= -\sum_{i}^{} \bigg[\bigg(\sum_{l}^{} \pi_l \mathcal{N}(w_i | \mu_l, \sigma_l^2)\bigg)^{-1}
        \sum_{k}^{} \pdv{\pi_k}{\eta_j} \mathcal{N}(w_i | \mu_k, \sigma_k^2)\bigg]\\ 
        &= -\sum_{i}^{} \bigg[\bigg(\sum_{l}^{} \pi_l \mathcal{N}(w_i | \mu_l, \sigma_l^2)\bigg)^{-1}
        \sum_{k}^{} (\delta_{kj} \pi_k - \pi_k \pi_j) \mathcal{N}(w_i | \mu_k, \sigma_k^2)\bigg] \\ 
        &= -\sum_{i}^{} \bigg(\frac{\pi_j \mathcal{N}(w_i | \mu_j, \sigma_j^2)}
            {\sum_{l}^{} \pi_l \mathcal{N}(w_i | \mu_l, \sigma_l^2)}
            - \pi_j \frac{\sum_{k}^{} \pi_k \mathcal{N}(w_i | \mu_k, \sigma_k^2)}
            {\sum_{l} {\pi_l \mathcal{N}(w_i | \mu_l, \sigma_l^2)}}\bigg) \\
        &= \sum_{i}^{} \{\pi_j - \gamma_j(w_i)\} \label{eq:5.147}\tag{5.147}
    \end{align*}
\end{proof}

\section*{Exercise 5.33 $\star$}
Write down a pair of equations that express the Cartesian coordinates
$(x_1, x_2)$ for the robot arm shown in Figure 5.18 in terms of the joint
angles $\theta_1$ and $\theta_2$ and the lengths $L_1$ and $L_2$ of the links.
Assume the origin of the coordinate system is given by the attachment point of
the lower arm. These equations define the `forward kinematics` of the robot arm.

\vspace{1em}
    \begin{figure}[h]
        \begin{tikzpicture}
            \def\figThetaOne{130}
            \def\figThetaTwo{50}

            \def\LOne{6}
            \def\LTwo{4}

            \tkzInit[xmin=-5, xmax=5, ymin=0, ymax=8]
            \tkzDrawX

            \tkzDefPoint(0,0){O}

            \tkzDefShiftPoint[O](\figThetaOne:\LOne){A}
            \tkzDefShiftPoint[A](\figThetaTwo:\LTwo){B}

            \tkzDefPointWith[colinear=at B](A,O)
            \tkzGetPoint{C}

            \tkzDrawSegment[red, line width=1.5pt, vector style](O,A)
            \tkzDrawSegment[red, line width=1.5pt, vector style](A,B)
            \tkzDrawSegment[line width=1pt, vector style](O, C)
            \tkzDrawSegment(B, C)

            \tkzDefRectangle(C,O) \tkzGetPoints{y_C}{E}
            \tkzDefRectangle(A,O) \tkzGetPoints{y_A}{D}

            \tkzDrawSegment[dashed](C,E)
            \tkzDrawSegment[dashed](A,D)

            \tkzMarkRightAngle(A,D,O)
            \tkzMarkRightAngle(C,E,O)

            \tkzMarkAngle[arrows=stealth-](O,A,B)
            \tkzLabelAngle[pos=0.05cm](O,A,B){$\theta_2$}

            \tkzMarkAngle[arrows=stealth-](E,O,A)
            \tkzLabelAngle[pos=0.05cm](E,O,A){$\theta_1$}

            \tkzLabelSegment(O,A){$L_1$}
            \tkzLabelSegment(A,B){$L_2$}

            \tkzDrawSegment[line width=1pt, vector style](O, B)

            \tkzDrawPoints(O, A, B, C, E, D)
            \tkzLabelPoints(O, A, B, C, E, D)
        \end{tikzpicture}
        \centering
        \caption{Robot arm geometric interpretation}
    \end{figure}
\begin{proof}
    Let the origin $O$ be the attachment point of the lower arm 
    and $A$ be the attachment point of the upper arm. Also,
    let $C$ be the end effector position and $OABC$ be a parallelogram.
    One can easily find the coordinates of the end effector by noticing that
    \[
        \overrightarrow{OB} 
        = \overrightarrow{OA} + \overrightarrow{OC}
        = \overrightarrow{OD} + \overrightarrow{DA} + 
        \overrightarrow{OE} + \overrightarrow{EC} 
        = \overrightarrow{i}(OE - OD) + \overrightarrow{j}(DA + EC) 

    \] 
    where $D, E$ are the projections of $A$ and $C$ over the x-axis.
    Since the angles around a parallelogram sum to $2\pi$, it's straightforward
    to show that
    \[
        \angle{COA} = \pi - \theta_2
        \hspace{3em}
        \angle{COE} = \theta_1 + \theta_2 - \pi
        \hspace{3em}
        \angle{DOA} = \pi - \theta_1
    \] 
    Hence,
    \[
        \Delta OAD:
        \begin{cases}
            DA = OA\sin(\angle DOA) = L_1 \sin(\theta_1) \\
            OD = OA\cos(\angle DOA) = -L_1 \cos(\theta_1)
        \end{cases}
    \]
    \[
        \Delta OCE:
        \begin{cases}
            EC = OC\sin(\angle COE) = -L_2 \sin(\theta_1 + \theta_2) \\
            OE = OC\cos(\angle COE) = -L_2 \cos(\theta_1 + \theta_2)
        \end{cases}
    \] 
    Finally, the equations that define the 'forward kinematics' of
    the arm will be given by
    \[
        \begin{cases}
            x_1 = OE - OD = L_1 \cos(\theta_1) - L_2 \cos(\theta_1 + \theta_2) \\
            y_1 = DA + EC = L_1\sin(\theta_1) - L_2 \sin(\theta_1 + \theta_2)
        \end{cases}
    \] 
\end{proof}
