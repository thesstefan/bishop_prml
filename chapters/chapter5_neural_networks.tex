\chapter{Neural Networks}

\section*{Exercise 5.1 $\star \star$}
Consider a two-layer network function of the form $\eqref{eq:5.7}$ 
in which the hidden-unit nonlinear activation functions $h(\cdot)$ 
are given by logistic sigmoid functions of the form
\begin{equation}\label{eq:5.191}\tag{5.191}
    \sigma(a) = \frac{1}{1 + \exp(-a)}.
\end{equation}
Show that there exists an equivalent network, which computes 
exactly the same function, but with hidden activation functions
given by $\tanh(a)$ where the $\tanh$ function is defined by
(5.59). Hint: first find the relation between $\sigma(a)$ 
and $\tanh(a)$, and then show that the parameters of the two
networks differ by linear transformations.

\vspace{1em}

\begin{proof}
    The considered two-layer network has the form
    \begin{equation}\label{eq:5.7}\tag{5.7}
        y_k(\mathbf{x}, \mathbf{w}) 
        = \sigma\bigg(\sum_{j=1}^{M} w_{kj}^{(2)} 
        h\bigg(\sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)}\bigg) + w_{k0}^{(2)}\bigg)
    \end{equation}
    Now, we've proved in Exercise 3.1 that 
    \[
        \sigma(x) = \frac{1}{2} \tanh \frac{x}{2} + \frac{1}{2}
    \] 
    Therefore, we can rewrite $y_k$ as
    \begin{align*}
        y_k(\mathbf{x}, \mathbf{w})
        &= \sigma\bigg(\frac{1}{2}\sum_{j=1}^{M} w_{kj}^{(2)} 
            \tanh\bigg(\frac{1}{2}\sum_{i=1}^{D} w_{ji}^{(1)}x_i + \frac{1}{2} w_{j0}^{(1)}\bigg)
            + \frac{1}{2} \sum_{j=1}^{M} w_{kj}^{(2)} + w_{k0}^{(2)}\bigg) \\
        &= \sigma\bigg(\sum_{j=1}^{M} \omega_{kj}^{(2)} 
        h\bigg(\sum_{i=1}^{D} \omega_{ji}^{(1)}x_i + \omega_{j0}^{(1)}\bigg) + \omega_{k0}^{(2)}\bigg)
    \end{align*}
    where 
    \newline
    \begin{minipage}[b]{0.25\textwidth}
    \[
       \omega_{ji}^{(1)} = \frac{1}{2} w_{ji}^{(1)}
    \]
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
    \[
        \omega_{j0}^{(1)} = \frac{1}{2} w_{j0}^{(1)}
    \] 
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
    \[
        \omega_{kj}^{(2)} = \frac{1}{2} w_{kj}^{(2)}
    \]
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
        \[
            \omega_{k0}^{(2)} = \frac{1}{2}\sum_{j = 1}^M w_{kj}^{(2)} + w_{k0}^{(2)}
        \] 
    \end{minipage}
    Both new parameter sets can be obtained as linear transformations of the old ones,
    so there exists an equivalent two-layer network using $\tanh$ hidden activation functions,
    but different parameters.
\end{proof}

\section*{Exercise 5.2 $\star$}
Show that maximizing the likelihood function under the conditional 
distribution \eqref{eq:5.16} for a multioutput network is equivalent to minimizing
the sum-of-squares error function (5.11).

\vspace{1em}

\begin{proof}
    The likelihood function is given by 
    \[
        p(\mathbf{T} | \mathbf{X}, \mathbf{w}, \beta)
        = \prod_{n = 1}^N p(\mathbf{t}_n | \mathbf{x}_n, \mathbf{w}, \beta)
    \] 
    The target variables are assumed to be distributed normally
    \begin{equation}\label{eq:5.16}\tag{5.16}
        p(\mathbf{t}_n | \mathbf{x}_n, \mathbf{w}, \beta) 
        = \mathcal{N}(\mathbf{t}_n | \mathbf{y}(\mathbf{x}_n, \mathbf{w}), \beta^{-1}\mathbf{I})
    \end{equation}
    and since
    \[
        \ln \mathcal{N}(\mathbf{t}_n | y(\mathbf{x}_n, \mathbf{w}), \beta^{-1}\mathbf{I})
        = -\frac{N}{2} \ln \beta - \frac{NK}{2} \ln(2\pi) - \frac{\beta}{2} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2
    \] 
    the negative log-likelihood is given by
    \[
        -\ln p(\mathbf{t} | \mathbf{X}, \mathbf{w}, \beta)
        = \frac{\beta}{2}\sum_{n=1}^{N} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2 + \text{const}
    \] 
    where we grouped the terms that don't depend on $\mathbf{w}$ under the constant term.
    Maximization of the likelihood function is equivalent to minimizing the negative 
    log-likelihood.
    Therefore, one can easily find that this is equivalent to minimizing the error function
    \begin{equation}\label{eq:5.11}\tag{5.11}
        E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2
    \end{equation}
\end{proof}

\section*{Exercise 5.3 $\star \star$}
Consider a regression problem involving multiple target variables in which
it is assumed that the distribution of the targets, conditioned on the input
vector $\mathbf{x}$, is a Gaussian of the form
\begin{equation}\label{eq:5.192}\tag{5.192}
    p(\mathbf{t} | \mathbf{x}, \mathbf{w}) 
    = \mathcal{N}(\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{w}), \mathbf{\Sigma})
\end{equation}
where $\mathbf{y(x, w)}$ is the output of a neural network with input vector
$\mathbf{x}$ and weight vector $\mathbf{w}$, and $ \mathbf{\Sigma}$ is the
covariance of the assumed Gaussian noise on the targets. Given a set
of independent observations of $\mathbf{x}$ and $\mathbf{t}$, write down
the error function that must be minimized in order to find the maximum
likelihood solution for $\mathbf{w}$, if we assume that $ \mathbf{\Sigma}$
is fixed and known. Now assume that $ \mathbf{\Sigma}$ is also to be determined
from the data, and write down an expression for the maximum likelihood
solution for $ \mathbf{\Sigma}$. Note that the optimizations of
$\mathbf{w}$ and $\mathbf{\Sigma}$ are now coupled, in contrast to the
case of independent target variables discussed in Section 5.2.

\vspace{1em}

\begin{proof}
    The negative log-likelihood is given by
    \begin{align*}
    -\ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
    &= -\sum_{i=1}^{N} \ln \mathcal{N}(\mathbf{t}_n | \mathbf{y}(\mathbf{x}_n, \mathbf{w}), \mathbf{\Sigma}) \\
    &= \frac{NK}{2} \ln(2\pi) + \frac{N}{2} \ln|\mathbf{\Sigma}| 
    + \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
    \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \end{align*}
    Maximizing the likelihood is equivalent to minimizing the negative
    log-likelihood. Therefore, the error function that must be minimized
    to obtain maximum likelihood is given by
    \[
        E(\mathbf{w}, \mathbf{\Sigma}) 
        = \frac{N}{2} \ln|\mathbf{\Sigma}|
        + \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
    In the case when $\mathbf{\Sigma}$ is known, we can simply treat
    the determinant term as a constant, so minimizing
    the error function
    \[
        E(\mathbf{w}) = 
        \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
    would yield the maximum likelihood solution $\mathbf{w}_\text{ML}$.
    If $ \mathbf{\Sigma}$ is unknown, we can't do that and the
    determination of $\mathbf{w}_{\text{ML}}$ would use $\mathbf{\Sigma}$,
    so that's why this time the optimizations of $\mathbf{w}$
    and $\mathbf{\Sigma}$ are coupled. The MLE for
    the covariance matrix is obtained by taking the
    derivative of the negative log-likelihood wrt. $\mathbf{\Sigma}^{-1}$, equalizing
    it to 0 and then solving for $\mathbf{\Sigma}$. Taking the
    derivative of the negative log-likelihood yields
    \begin{align*}
        \pdv{\mathbf{\Sigma}^{-1}} \ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
        &= \frac{N}{2} \pdv{\mathbf{\Sigma}^{-1}} \ln |\mathbf{\Sigma}| +
        \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big) \\
        &= -\frac{N}{2} \pdv{\mathbf{\Sigma}^{-1}} \ln |\mathbf{\Sigma}^{-1}| +
        \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \Trace\big\{\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)\big\} \\
        &= -\frac{N}{2} \mathbf{\Sigma} 
        + \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \Trace\big\{\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big) \mathbf{\Sigma}^{-1}\big\}\\
        &= -\frac{N}{2} \mathbf{\Sigma} + \frac{1}{2} \sum_{n=1}^{N} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \end{align*}
    where we've used the cyclic property of the trace operator and the fact that
    \[
        \pdv{\mathbf{A}} \ln |A| = A^{-T}
    \]
    Now, equalizing
    the derivative with 0 and solving for $\mathbf{\Sigma}$ gives
    the MLE for the covariance matrix:
    \[
        \mathbf{\Sigma}_\text{ML}
        = \frac{1}{N} \sum_{n=1}^{N} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
\end{proof}

\section*{Exercise 5.4 $\star \star$}
Consider a binary classification problem where the target values
are $t \in \{0, 1\}$, with a network output $y(\mathbf{x}, \mathbf{w})$
that represents $p(t = 1 | \mathbf{x})$, and suppose that
there is a probability $\epsiilon$ that the class label
on a trainining data point has been incorrectly set.
Assuming independent and identically distributed data,
write down the error function corresponding to the negative
log likelihood. Verify that the error function $\eqref{5.21}$ 
is obtained when $\epsilon = 0$. Note that this error function
makes the model robust to incorrectly labelled data, in contrast
to the usual error function.

\vspace{1em}

\begin{proof}
    
\end{proof}
