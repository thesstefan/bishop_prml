\chapter{Neural Networks}

\section*{Exercise 5.1 $\star \star$}
Consider a two-layer network function of the form $\eqref{eq:5.7}$ 
in which the hidden-unit nonlinear activation functions $h(\cdot)$ 
are given by logistic sigmoid functions of the form
\begin{equation}\label{eq:5.191}\tag{5.191}
    \sigma(a) = \frac{1}{1 + \exp(-a)}.
\end{equation}
Show that there exists an equivalent network, which computes 
exactly the same function, but with hidden activation functions
given by $\tanh(a)$ where the $\tanh$ function is defined by
(5.59). Hint: first find the relation between $\sigma(a)$ 
and $\tanh(a)$, and then show that the parameters of the two
networks differ by linear transformations.

\vspace{1em}

\begin{proof}
    The considered two-layer network has the form
    \begin{equation}\label{eq:5.7}\tag{5.7}
        y_k(\mathbf{x}, \mathbf{w}) 
        = \sigma\bigg(\sum_{j=1}^{M} w_{kj}^{(2)} 
        h\bigg(\sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)}\bigg) + w_{k0}^{(2)}\bigg)
    \end{equation}
    Now, we've proved in Exercise 3.1 that 
    \[
        \sigma(x) = \frac{1}{2} \tanh \frac{x}{2} + \frac{1}{2}
    \] 
    Therefore, we can rewrite $y_k$ as
    \begin{align*}
        y_k(\mathbf{x}, \mathbf{w})
        &= \sigma\bigg(\frac{1}{2}\sum_{j=1}^{M} w_{kj}^{(2)} 
            \tanh\bigg(\frac{1}{2}\sum_{i=1}^{D} w_{ji}^{(1)}x_i + \frac{1}{2} w_{j0}^{(1)}\bigg)
            + \frac{1}{2} \sum_{j=1}^{M} w_{kj}^{(2)} + w_{k0}^{(2)}\bigg) \\
        &= \sigma\bigg(\sum_{j=1}^{M} \omega_{kj}^{(2)} 
        h\bigg(\sum_{i=1}^{D} \omega_{ji}^{(1)}x_i + \omega_{j0}^{(1)}\bigg) + \omega_{k0}^{(2)}\bigg)
    \end{align*}
    where 
    \newline
    \begin{minipage}[b]{0.25\textwidth}
    \[
       \omega_{ji}^{(1)} = \frac{1}{2} w_{ji}^{(1)}
    \]
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
    \[
        \omega_{j0}^{(1)} = \frac{1}{2} w_{j0}^{(1)}
    \] 
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
    \[
        \omega_{kj}^{(2)} = \frac{1}{2} w_{kj}^{(2)}
    \]
    \end{minipage}
    \begin{minipage}[b]{0.25\textwidth}
        \[
            \omega_{k0}^{(2)} = \frac{1}{2}\sum_{j = 1}^M w_{kj}^{(2)} + w_{k0}^{(2)}
        \] 
    \end{minipage}
    Both new parameter sets can be obtained as linear transformations of the old ones,
    so there exists an equivalent two-layer network using $\tanh$ hidden activation functions,
    but different parameters.
\end{proof}

\section*{Exercise 5.2 $\star$}
Show that maximizing the likelihood function under the conditional 
distribution \eqref{eq:5.16} for a multioutput network is equivalent to minimizing
the sum-of-squares error function (5.11).

\vspace{1em}

\begin{proof}
    The likelihood function is given by 
    \[
        p(\mathbf{T} | \mathbf{X}, \mathbf{w}, \beta)
        = \prod_{n = 1}^N p(\mathbf{t}_n | \mathbf{x}_n, \mathbf{w}, \beta)
    \] 
    The target variables are assumed to be distributed normally
    \begin{equation}\label{eq:5.16}\tag{5.16}
        p(\mathbf{t}_n | \mathbf{x}_n, \mathbf{w}, \beta) 
        = \mathcal{N}(\mathbf{t}_n | \mathbf{y}(\mathbf{x}_n, \mathbf{w}), \beta^{-1}\mathbf{I})
    \end{equation}
    and since
    \[
        \ln \mathcal{N}(\mathbf{t}_n | y(\mathbf{x}_n, \mathbf{w}), \beta^{-1}\mathbf{I})
        = -\frac{N}{2} \ln \beta - \frac{NK}{2} \ln(2\pi) - \frac{\beta}{2} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2
    \] 
    the negative log-likelihood is given by
    \[
        -\ln p(\mathbf{t} | \mathbf{X}, \mathbf{w}, \beta)
        = \frac{\beta}{2}\sum_{n=1}^{N} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2 + \text{const}
    \] 
    where we grouped the terms that don't depend on $\mathbf{w}$ under the constant term.
    Maximization of the likelihood function is equivalent to minimizing the negative 
    log-likelihood.
    Therefore, one can easily find that this is equivalent to minimizing the error function
    \begin{equation}\label{eq:5.11}\tag{5.11}
        E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} 
        ||\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n||^2
    \end{equation}
\end{proof}

\section*{Exercise 5.3 $\star \star$}
Consider a regression problem involving multiple target variables in which
it is assumed that the distribution of the targets, conditioned on the input
vector $\mathbf{x}$, is a Gaussian of the form
\begin{equation}\label{eq:5.192}\tag{5.192}
    p(\mathbf{t} | \mathbf{x}, \mathbf{w}) 
    = \mathcal{N}(\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{w}), \mathbf{\Sigma})
\end{equation}
where $\mathbf{y(x, w)}$ is the output of a neural network with input vector
$\mathbf{x}$ and weight vector $\mathbf{w}$, and $ \mathbf{\Sigma}$ is the
covariance of the assumed Gaussian noise on the targets. Given a set
of independent observations of $\mathbf{x}$ and $\mathbf{t}$, write down
the error function that must be minimized in order to find the maximum
likelihood solution for $\mathbf{w}$, if we assume that $ \mathbf{\Sigma}$
is fixed and known. Now assume that $ \mathbf{\Sigma}$ is also to be determined
from the data, and write down an expression for the maximum likelihood
solution for $ \mathbf{\Sigma}$. Note that the optimizations of
$\mathbf{w}$ and $\mathbf{\Sigma}$ are now coupled, in contrast to the
case of independent target variables discussed in Section 5.2.

\vspace{1em}

\begin{proof}
    The negative log-likelihood is given by
    \begin{align*}
    -\ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
    &= -\sum_{i=1}^{N} \ln \mathcal{N}(\mathbf{t}_n | \mathbf{y}(\mathbf{x}_n, \mathbf{w}), \mathbf{\Sigma}) \\
    &= \frac{NK}{2} \ln(2\pi) + \frac{N}{2} \ln|\mathbf{\Sigma}| 
    + \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
    \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \end{align*}
    Maximizing the likelihood is equivalent to minimizing the negative
    log-likelihood. Therefore, the error function that must be minimized
    to obtain maximum likelihood is given by
    \[
        E(\mathbf{w}, \mathbf{\Sigma}) 
        = \frac{N}{2} \ln|\mathbf{\Sigma}|
        + \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
    In the case when $\mathbf{\Sigma}$ is known, we can simply treat
    the determinant term as a constant, so minimizing
    the error function
    \[
        E(\mathbf{w}) = 
        \frac{1}{2}  \sum_{n=1}^{N} \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
    would yield the maximum likelihood solution $\mathbf{w}_\text{ML}$.
    If $ \mathbf{\Sigma}$ is unknown, we can't do that and the
    determination of $\mathbf{w}_{\text{ML}}$ would use $\mathbf{\Sigma}$,
    so that's why this time the optimizations of $\mathbf{w}$
    and $\mathbf{\Sigma}$ are coupled. The MLE for
    the covariance matrix is obtained by taking the
    derivative of the negative log-likelihood wrt. $\mathbf{\Sigma}^{-1}$, equalizing
    it to 0 and then solving for $\mathbf{\Sigma}$. Taking the
    derivative of the negative log-likelihood yields
    \begin{align*}
        \pdv{\mathbf{\Sigma}^{-1}} \ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
        &= \frac{N}{2} \pdv{\mathbf{\Sigma}^{-1}} \ln |\mathbf{\Sigma}| +
        \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big) \\
        &= -\frac{N}{2} \pdv{\mathbf{\Sigma}^{-1}} \ln |\mathbf{\Sigma}^{-1}| +
        \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \Trace\big\{\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \mathbf{\Sigma}^{-1}\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)\big\} \\
        &= -\frac{N}{2} \mathbf{\Sigma} 
        + \frac{1}{2}  \sum_{n=1}^{N} \pdv{\mathbf{\Sigma}^{-1}} 
        \Trace\big\{\big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big) \mathbf{\Sigma}^{-1}\big\}\\
        &= -\frac{N}{2} \mathbf{\Sigma} + \frac{1}{2} \sum_{n=1}^{N} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \end{align*}
    where we've used the cyclic property of the trace operator and the fact that
    \[
        \pdv{\mathbf{A}} \ln |A| = A^{-T}
    \]
    Now, equalizing
    the derivative with 0 and solving for $\mathbf{\Sigma}$ gives
    the MLE for the covariance matrix:
    \[
        \mathbf{\Sigma}_\text{ML}
        = \frac{1}{N} \sum_{n=1}^{N} 
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)^T
        \big(\mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n\big)
    \] 
\end{proof}

\section*{Exercise 5.4 $\star \star$}
Consider a binary classification problem where the target values
are $t \in \{0, 1\}$, with a network output $y(\mathbf{x}, \mathbf{w})$
that represents $p(t = 1 | \mathbf{x})$, and suppose that
there is a probability $\epsiilon$ that the class label
on a trainining data point has been incorrectly set.
Assuming independent and identically distributed data,
write down the error function corresponding to the negative
log likelihood. Verify that the error function $\eqref{eq:5.21}$
is obtained when $\epsilon = 0$. Note that this error function
makes the model robust to incorrectly labelled data, in contrast
to the usual error function.

\vspace{1em}

\begin{proof}
    We're going to model the problem similarly with what we've done in
    Section 4.2.2, but this time taking into account the mislabelled training data
    probability. As a result, let $r \in \{0, 1\}$ the real target values, considering
    mislabelling. Therefore, we can find the label probabilities by weighting in the error
    chance:
    \begin{align*}
        p(r = 1 | \mathbf{x}, \mathbf{w}) 
        &= (1 - \epsilon)p(t = 1 | \mathbf{x}, \mathbf{w}) 
        + \epsilon p(t = 0 | \mathbf{x}, \mathbf{w})
        = (1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) 
        + \epsilon \big(1 - y(\mathbf{x}_n, \mathbf{w})\big)
    \end{align*}
    \vspace{-2em}
    \begin{align*}
        p(r = 0 | \mathbf{x}, \mathbf{w}) 
        &= (1 - \epsilon)p(t = 0 | \mathbf{x}, \mathbf{w}) 
        + \epsilon p(t = 1 | \mathbf{x}, \mathbf{w})
        = (1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big) 
        + \epsilon y(\mathbf{x}_n, \mathbf{w})
    \end{align*}
    We can combine both of these into
    \begin{align*}
        p(r | \mathbf{x}, \mathbf{w}) 
        &= p(r = 1 | \mathbf{x}, \mathbf{w})^r p(r = 0 | \mathbf{x}, \mathbf{w})^{1 - r} \\
        &= \big[(1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) + \epsilon \big(1 - y(\mathbf{x}_n, \mathbf{w})\big)\big]^r
        \big[(1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big) 
        + \epsilon y(\mathbf{x}_n, \mathbf{w})\big]^{1 - r}
    \end{align*}
    Therefore, the negative log-likelihood is given by
    \begin{align*}
        - \ln p(\mathbf{r} | \mathbf{\mathbf{X}}, \mathbf{w})
        = - \ln \prod_{i=1}^N p(r_n | \mathbf{x}_n, \mathbf{w})
        = - \sum_{i=1}^{N} \{r_n \ln p(r_n = 1 | \mathbf{x}_n, \mathbf{w}) 
            + (1 - r_n) \ln p(r_n = 0 | \mathbf{x}, \mathbf{w})\} 
    \end{align*}
    As a result, this is equivalent to minimizing the error function
    \[
        E(\mathbf{w}) = -\sum_{i=1}^{N} 
        \big[r_n \ln\big\{(1 - \epsilon)y(\mathbf{x}_n, \mathbf{w}) 
            + \epsilon\big(1 - y(\mathbf{x}_n, \mathbf{w})\big)\big\}
            + (1 - r_n) \ln\big\{(1 - \epsilon)\big(1 - y(\mathbf{x}_n, \mathbf{w})\big)
            + \epsilon y(\mathbf{x}_n, \mathbf{w})\big]
    \] 
    which for $\epsilon = 0$ is equivalent to $\eqref{eq:5.21}$.
\end{proof}

\section*{Exercise 5.5 $\star$}
Show that maximizing likelihood for a multiclass neural network
model in which the network outputs have the interpretation
$y_k(\mathbf{x}, \mathbf{w}) = p(t_k = 1 | \mathbf{x})$ is equivalent to minimization
of the cross-entropy function \eqref{eq:5.24}.

\vspace{1em}

\begin{proof}
    Let's consider the binary target variables $t_k \in \{0, 1\}$ have a 1-of-$K$ 
    coding scheme indicating the class. If we assume the class labels are independent, 
    given the input vector, the conditional distribution of the targets is
    \[
        p(t_k | \mathbf{x}) = \prod_{k=1}^K p(t_k = 1 | \mathbf{x})^{t_k}
    \] 
    As a result, the corresponding negative log likelihood is given by
    \[
        -\ln p(\mathbf{T} | \mathbf{X}, \mathbf{w})
        = -\ln \prod_{n=1}^N \prod_{k=1}^K p(t_{nk} = 1 | \mathbf{x}_n)^{t_{nk}}
        = -\ln \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln p(t_{nk} = 1 | \mathbf{x}_n) 
    \]
    Therefore, maximizing the likelihood of the model is equivalent to minimization
    of the cross entropy function
    \begin{equation}\label{eq:5.24}\tag{5.24}
        E(\mathbf{w}) 
        = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln p(t_{nk} = 1 | \mathbf{x}_n) 
    \end{equation}
\end{proof}

\section*{Exercise 5.6 $\star$}
Show the derivative of the error function (5.21) with respect
to the activation $a_k$ for output units having a softmax activation
function satisfies \eqref{eq:5.18}.

\vspace{1em}

\begin{proof}
    The general result for the derivative of the softmax function with respect to the 
    activation $a_k$ was proved in Exercise 4.17 and is given by $\eqref{eq:4.106}$.
    Therefore, we have that
    \[
        \pdv{y_k}{a_k} = y_k(1 - y_k)
    \] 
    Taking the derivative of 
    \begin{equation}\label{eq:5.21}\tag{5.21}
        E(\mathbf{w}) = -\sum_{n=1}^{N} \{t_n \ln y_n + (1 - t_n) \ln(1 - y_n)\}
    \end{equation}
    with respect to $a_k$ yields
    \begin{align*}
        \pdv{a_k} E(\mathbf{w}) &= -t_k \pdv{a_k} \ln y_k - (1 - t_k) \pdv{a_k} \ln(1 - y_k) 
        = -t_k(1 - y_k) + y_k(1 - t_k)y_k 
        = y_k - t_k
    \end{align*}
    As a result,
    \begin{equation}\label{eq:5.18}\tag{5.18}
        \pdv{E}{a_k} = y_k - t_k
    \end{equation}
\end{proof}

\section*{Exercise 5.7 $\star$}
Show the derivative of the error function $\eqref{eq:5.21}$ with respect
to the activation $a_k$ for an output unit having a logistic sigmoid activation
function satisfies $\eqref{eq:5.18}$.

\vspace{1em}

\begin{proof}
    We've seen in Exercise 4.12 that
    \begin{equation}\tag{4.88}
        \pdv{a} \sigma(a) = \sigma(a)(1 - \sigma(a)
    \end{equation}
    Since the output unit has a logistic sigmoid activation function, then
    \[
        y_k = \sigma(a_k)
    \] 
    Therefore, using $\eqref{eq:4.88}$ gives
     \[
         \pdv{y_k}{a_k} = \sigma(a_k)\big(1 - \sigma(a_k)\big) = y_k(1 - y_k)
    \] 
    Analogously to Exercise 5.6, one can quickly reach that $\eqref{eq:5.18}$ holds.
\end{proof}
