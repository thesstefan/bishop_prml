\chapter{Kernel Methods}

\section*{Exercise 6.1 $\star \star$}
Consider the dual formulation of the least squares linear
regression problem given in Section 6.1. Show that the
solution for the components $a_n$ of the vector
$\mathbf{a}$ can be expressed as a linear combination of the
elements of the vector $\bm{\phi}(\mathbf{x}_n)$. Denoting 
these coefficients by the vector $\mathbf{w}$, show
that the dual of the dual formulation is given by
the original representation in terms of the parameter vector
$\mathbf{w}$.

\vspace{1em}

\begin{proof}
    By rewriting (6.4), one has that
    \begin{align*}
        a_n 
        &= -\frac{1}{\lambda} \{\mathbf{w}^T \bm{\phi}(\mathbf{x}_n) - t_n\} \\
        &= -\frac{1}{\lambda} \bigg\{\sum_{i=1}^{M} w_i \phi_i(\mathbf{x}_n) 
            - \frac{t_n}{\sum_{i=1}^{M} \phi_i(\mathbf{x}_n)} \sum_{i=1}^{M} \phi_i(\mathbf{x}_n)
        \bigg\} \\
        &= \sum_{i=1}^{M} \bigg(
            \frac{t_n}{\lambda \sum_{i=1}^{M} \phi_i(\mathbf{x}_n)} - \frac{w_i}{\lambda}\bigg)
            \phi_i(\mathbf{x}_n) \\
        &= \sum_{i=1}^{M} \Omega_{ni} \phi_i(\mathbf{x}_n) \\
        &= \bm{\Omega}_n^T \bm{\phi}(\mathbf{x}_n)
    \end{align*}
    where
    \[
        \Omega_{ni} = 
            \frac{t_n}{\lambda \sum_{i=1}^{M} \phi_i(\mathbf{x}_n)} - \frac{w_i}{\lambda}
    \] 
    Therefore, $a_n$ can be written as a linear combination of the elements
    of $\bm{\phi}(\mathbf{x}_n)$ and
    \[
        \mathbf{a} = \text{diag}(\bm{\Omega}\bm{\Phi})
    \] 
\end{proof}

\section*{Exercise 6.3 $\star$}
The nearest-neighbour classifier (Section 2.5.2) assigns
a new input vector $\mathbf{x}$ to the same class as that of the nearest input
vector $\mathbf{x}_n$ from the training set, where in the simple case,
the distance is defined by the Euclidean metric $\norm{\mathbf{x} - \mathbf{x}_n}^2$.
By expressing this rule in terms of scalar product and then making use of kernel
substitution, formulate the nearest-neighbour classifier for a general nonlinear kernel.

\vspace{1em}

\begin{proof}
    Since we're dealing with inner products over $\mathbb{R}$,
    the Euclidian metric can be rewritten as
    \begin{align*}
        \norm{\mathbf{x} - \mathbf{x}_n}^2
        = \langle \mathbf{x} - \mathbf{x}_n, \mathbf{x} - \mathbf{x}_n\rangle
        = \langle\mathbf{x}, \mathbf{x}\rangle - 2\langle \mathbf{x}, \mathbf{x}_n \rangle
        + \langle \mathbf{x}_n \mathbf{x}_n \rangle
    \end{align*}
    Similarly to what happens in Section 6.2, using kernel
    substitution above to replace $\langle \mathbf{x}, \mathbf{x}' \rangle$ with
    a nonlinear kernel $\kappa(\mathbf{x}, \mathbf{x}')$ yields the
    nearest-neighbour classifier for a general nonlinear kernel:
    \[
        k(\mathbf{x}, \mathbf{x}') 
        = \kappa(\mathbf{x}, \mathbf{x}) - 2\kappa(\mathbf{x}, \mathbf{x}_n)
        + \kappa(\mathbf{x}_n, \mathbf{x}_n)
    \] 
\end{proof}

\section*{Exercise 6.4 $\star$}
In Appendix C, we give an example of a matrix that has positive elements
but that ahas a negative eigenvalue and hence that is not positive
definite. Find an example of the converse property, namely a $2 \times 2$ 
matrix with positive eigenvalues that has at least one negative element.

\vspace{1em}

\begin{proof}
    Consider the matrix 
    \[
        A = 
        \begin{bmatrix}
            2 & 1 \\
            -1 & 2
        \end{bmatrix}
    \] 
    $A$ contains one negative element and the eigenvalues of $A$ 
    are $\lambda_1 = 1$ and $\lambda_2 = 3$, which proves that
    a matrix can be positive definite and have negative elements.
\end{proof}

\section*{Exercise 6.5 $\star$}
Verify the results \eqref{eq:6.13} and \eqref{eq:6.14} for constructing valid kernels.

\vspace{1em}

\begin{proof}
    Since $k_1$ is a valid kernel, let $\bm{\alpha}$ be
    a feature mapping such that
    \[
        k_1(\mathbf{x}, \mathbf{x}') 
        = \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle
    \] 
    Using the fact that an inner product on a real vector space is a 
    positive-definite symmetric bilinear form, we have that
    \[
        ck_1(\mathbf{x}, \mathbf{x}')
        = c \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle
        = \langle \sqrt{c} \bm{\alpha}(\mathbf{x}), \sqrt{c} \bm{\alpha}(\mathbf{x}') \rangle
        = \langle \bm{\beta}(\mathbf{x}), \bm{\beta}(\mathbf{x}') \rangle
    \] 
    where $c > 0$ is a constant and $\bm{\beta}(\mathbf{x}) = \sqrt{c}\bm{\alpha}(\mathbf{x})$.
    Therefore, the new kernel 
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = ck_1(\mathbf{x}, \mathbf{x}')
        \tag{6.13}\label{eq:6.13}
    \end{equation*}
    is valid. Analogously, since $f(\cdot)$ is a real-valued function, 
    \[
        f(\mathbf{x}) k_1(\mathbf{x}, \mathbf{x}') f(\mathbf{x}')
        = f(\mathbf{x}) \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle
        f(\mathbf{x}')
        = \langle f(\mathbf{x}) \bm{\alpha}(\mathbf{x}), f(\mathbf{x}') \bm{\alpha}(\mathbf{x}')
        \rangle
        = \langle \bm{\gamma}(\mathbf{x}), \bm{\gamma}(\mathbf{x}') \rangle
    \] 
    where $\bm{\gamma}(\mathbf{x}) = f(\mathbf{x}) \bm{\alpha}(\mathbf{x})$.
    As a result, the kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = f(\mathbf{x}) k_1(\mathbf{x}, \mathbf{x}') f(\mathbf{x}')
        \tag{6.14}\label{eq:6.14}
    \end{equation*}
    will also be valid.
\end{proof}

\section*{Exercise 6.6 $\star$}
Verify the results \eqref{eq:6.15} and \eqref{eq:6.16} for constructing valid kernels.

\vspace{1em}

\begin{proof}
    Let $q(\cdot)$ be a polynomial with nonnegative coefficients. Since in the polynomial
    kernels are summed and multiplied by nonnegative constants or other kernels, combining
    \eqref{eq:6.13}, \eqref{eq:6.17} and \eqref{eq:6.18} proves that
    the kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = q\big(k_1(\mathbf{x}, \mathbf{x}')\big)
        \tag{6.15}\label{eq:6.15}
    \end{equation*}
    is valid. Now, the exponential function is defined as
    \[
        \exp(x) \coloneqq \sum_{i=0}^{\infty} \frac{x^i}{i!} 
    \],
    so
    \[
        \exp\big(k_1(\mathbf{x}, \mathbf{x}')\big)
        = \sum_{i=0}^{\infty} \frac{k_1(\mathbf{x}, \mathbf{x}')^i}{i!}
    \] 
    Note that the exponential of a kernel is an infinite sequence of kernel sums
    and products (with itself or nonnegative constants), so by using
    \eqref{eq:6.13}, \eqref{eq:6.17}, \eqref{eq:6.18} again, one has that
    the new kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = \exp\big(k_1(\mathbf{x}, \mathbf{x}')\big)
        \tag{6.16}\label{eq:6.16}
    \end{equation*}
    is valid.
\end{proof}

\section*{Exercise 6.7 $\star$}
Verify the results \eqref{eq:6.17} and \eqref{eq:6.18} for constructing valid kernels.

\vspace{1em}

\begin{proof}
    Let $K_1$ and $K_2$ be the Gram matrices corresponding to the 
    kernels $k_1$ and $k_2$. Therefore, they are positive semidefinite matrices, so
    for any $\mathbf{a} \in \mathbb{R}^n$, one has that
    \[
        \mathbf{a}^T\mathbf{H}\mathbf{a}
        = \mathbf{a}^T\big(\mathbf{H}_1 + \mathbf{H}_2\big)\mathbf{a}
        = \mathbf{a}^T \mathbf{H}_1 \mathbf{a} + \mathbf{a}^T \mathbf{H}_2 \mathbf{a} > 0
    \] 
    Since $\mathbf{H} = \mathbf{H}_1 + \mathbf{H}_2$ is positive semidefinite
    and corresponds to the Gram matrix of the 
    kernel $k(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x'}) + 
    k_2(\mathbf{x}, \mathbf{x}')$, one has that the kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') + k_2(\mathbf{x}, \mathbf{x}')
        \tag{6.17}\label{eq:6.17}
    \end{equation*}
    is valid. Now, let $\bm{\alpha}, \bm{\beta}$ be feature mappings such that
    \[
        k_1(\mathbf{x}, \mathbf{x}') 
        = \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle 
    \] 
    \[
        k_2(\mathbf{x}, \mathbf{x}') 
        = \langle \bm{\beta}(\mathbf{x}), \bm{\beta}(\mathbf{x}') \rangle 
    \]
    As a result,
    \begin{align*}
        k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')
        &= \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle 
        \langle \bm{\beta}(\mathbf{x}), \bm{\beta}(\mathbf{x}') \rangle \\
        &= \bm{\alpha}(\mathbf{x})^T \bm{\alpha}(\mathbf{x}')
        \bm{\beta}^T(\mathbf{x}) \bm{\beta}(\mathbf{x}') \\
        &= \bigg[\sum_{i=1}^{N} \alpha_i(\mathbf{x}) \alpha_i(\mathbf{x}')\bigg]
        \bigg[\sum_{j=1}^{M} \beta_i(\mathbf{x}) \beta_i(\mathbf{x}')\bigg] \\
        &= \sum_{i=1}^{N} \sum_{j=1}^{M} \alpha_i(\mathbf{x}) \beta_j(\mathbf{x})
        \alpha_i(\mathbf{x}') \beta_j(\mathbf{x}') \tag{*}\\
        &= \sum_{i=1}^{N} \sum_{j=1}^{M}  A_{ij}(\mathbf{x}) A_{ij}(\mathbf{x}') \\
        &= \langle \mathbf{A}(\mathbf{x}), \mathbf{A}(\mathbf{x'}) \rangle_\mathbf{F}
    \end{align*}
    where $\mathbf{A}$ is a matrix with
    \[
        A_{ij}(\mathbf{x}) = \alpha_i(\mathbf{x}) \beta_j(\mathbf{x})
    \] 
    and $\langle \cdot, \cdot \rangle_\mathbf{F}$ is the Frobenius inner product.
    Since the product kernel can be rewritten as a valid inner product in the 
    feature space defined by the feature mapping $\mathbf{A}(\mathbf{x})$,
    the new kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}')
        = k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')
        \tag{6.18}\label{eq:6.18}
    \end{equation*}
    is valid. Note that we can continue differently from (*), so
    \begin{align*}
        k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')
        = \sum_{k=1}^{K} \phi_k (\mathbf{x}) \phi_k(\mathbf{x}')
        = \langle \bm{\phi}(\mathbf{x}), \bm{\phi}(\mathbf{x}') \rangle
    \end{align*}
    where $K = NM$ and
    \[
        \phi_k(\mathbf{x}) 
        = \alpha_{((k - 1) \varoslash N) + 1}(\mathbf{x})
        \beta_{((k - 1) \odot N) + 1}(\mathbf{x})
    \] 
    where $\varoslash$ and $\odot$ denote integer division and remainder,
    respectively.
\end{proof}

\section*{Exercise 6.8 $\star$}
Verify the results \eqref{eq:6.19} and \eqref{eq:6.20} for constructing valid kernels.

\vspace{1em}

\begin{proof}
    Let $\bm{\psi}$ be a feature mapping such that
    \[
        k_3(\mathbf{x}, \mathbf{x}') 
        = \langle \bm{\psi}(\mathbf{x}), \bm{\psi}(\mathbf{x}') \rangle
    \] 
    Then,
    \begin{align*}
        k_3\big(\bm{\phi}(\mathbf{x}), \bm{\phi}(\mathbf{x}')\big)
        &= \langle \bm{\psi}\big(\bm{\phi}(\mathbf{x})\big),
            \bm{\psi}\big(\bm{\phi}(\mathbf{x}')\big)\rangle \\
        &= \langle \big(\bm{\psi} \circ \bm{\phi}\big)(\mathbf{x}),
            \big(\bm{\psi} \circ \bm{\phi}\big)(\mathbf{x}')\rangle \\
        &= \langle \bm{\gamma}(\mathbf{x}), \bm{\gamma}(\mathbf{x}')\rangle
    \end{align*}
    where $\bm{\phi}$ is a function from $\mathbf{x}$ to $\mathbb{R}^M$ and
    $\bm{\gamma} = \bm{\psi} \circ \bm{\phi}$. Therefore, the kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = k_3\big(\bm{\phi}(\mathbf{x}),
        \bm{\phi}(\mathbf{x}')\big)
        \tag{6.19}\label{eq:6.19} 
    \end{equation*}
    is valid. For the second part, since $\mathbf{A}$ is a 
    symmetric, positive semidefinite matrix, one can use the Cholesky
    decomposition to obtain a matrix $\mathbf{L}$ such that
    \[
        \mathbf{A} = \mathbf{L} \mathbf{L}^T
    \] 
    As a result, one can show that
    \begin{align*}
        \mathbf{x}^T \mathbf{A} \mathbf{x}
        = \mathbf{x}^T \mathbf{L} \mathbf{L}^T \mathbf{x}
        = \big(\mathbf{L}^T \mathbf{x}\big)^T \big(\mathbf{L}^T \mathbf{x}\big)
        = \langle \bm{\zeta}(\mathbf{x}), \bm{\zeta}(\mathbf{x}') \rangle
    \end{align*}
    where $\bm{\zeta}(\mathbf{x}) = \mathbf{L}^T \mathbf{x}$.
    Hence, the kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^T \mathbf{A} \mathbf{x}
        \tag{6.20}\label{eq:6.20}
    \end{equation*}
    is valid.
\end{proof} 

\section*{Exercise 6.9 $\star$}
Verify the results \eqref{eq:6.21} and \eqref{eq:6.22} for constructing valid kernels.

\vspace{1em}

\begin{proof}
    Let $\bm{\phi}_a$ and $\bm{\phi}_b$ be feature mappings so that
    \[
        k_a(\mathbf{x}, \mathbf{x}') 
        = \langle\bm{\phi}_a(\mathbf{x}), \bm{\phi}_a(\mathbf{x}')\rangle
    \] 
    \[
        k_b(\mathbf{x}, \mathbf{x}') 
        = \langle\bm{\phi}_b(\mathbf{x}), \bm{\phi}_b(\mathbf{x}')\rangle
    \] 
    Therefore, since the inner product becomes a bilinear form on $\mathbb{R}$,
    \begin{align*}
        k_a(\mathbf{x}_a, \mathbf{x'}_a) + k_b(\mathbf{x}_b, \mathbf{x}_b')
        &= \langle\bm{\phi}_a(\mathbf{x}_a), \bm{\phi}_a(\mathbf{x}_a')\rangle
        + \langle\bm{\phi}_b(\mathbf{x}_b), \bm{\phi}_b(\mathbf{x}_b')\rangle \\
        &= \langle \big(\bm{\phi}_a(\mathbf{x}_a), \bm{\phi}_a(\mathbf{x}_a')\big),
        \big(\bm{\phi}_b(\mathbf{x}_b), \bm{\phi}_b(\mathbf{x}_b')\big) \rangle \\
        &= \langle \bm{\phi}(\mathbf{x}), \bm{\phi}(\mathbf{x'}) \rangle
    \end{align*}
    where
    \[
        \bm{\phi}(\mathbf{x}) = 
        \begin{bmatrix}
            \bm{\phi}_a(\mathbf{x}_a) \\
            \bm{\phi}_b(\mathbf{x}_b)
        \end{bmatrix}
    \] 
    Hence, the kernel 
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = 
        k_a(\mathbf{x}_a, \mathbf{x}_a') +
        k_b(\mathbf{x}_b, \mathbf{x}_b')
        \tag{6.21}\label{eq:6.21}
    \end{equation*}
    is valid. The product identity is obtained similarly to
    what we do in Exercise 6.7. One has that
    \begin{align*}
        k_a(\mathbf{x}_a, \mathbf{x}_a')k_b(\mathbf{x}_b, \mathbf{x}_b')
        &= \langle\bm{\phi}_a(\mathbf{x}_a), \bm{\phi}_a(\mathbf{x}_a')\rangle
        \langle\bm{\phi}_b(\mathbf{x}_b), \bm{\phi}_b(\mathbf{x}_b')\rangle \\
        &= \bigg[\sum_{i=1}^{N_a} \phi_{ai}(\mathbf{x}_a) \phi_{ai}(\mathbf{x}_a')\bigg]
        \bigg[\sum_{j=1}^{N_b} \phi_{bj}(\mathbf{x}_b) \phi_{bj}(\mathbf{x}_b')\bigg] \\
        &= \sum_{i=1}^{N_a} \sum_{j=1}^{N_b}
        \phi_{ai}(\mathbf{x}_a) \phi_{bj}(\mathbf{x}_b)
        \phi_{ai}(\mathbf{x}_a') \phi_{bj}(\mathbf{x}_b') \\
        &= \sum_{i=1}^{N_a} \sum_{j=1}^{N_b} A_{ij}(\mathbf{x}) A_{ij}(\mathbf{x}') \\
        &= \langle \mathbf{A}(\mathbf{x}), \mathbf{A}(\mathbf{x}') \rangle_{\bm{F}}
    \end{align*}
    where $\langle \cdot, \cdot \rangle_{\bm{F}}$ is the Frobenius inner product,
    $\phi_{ai}(\mathbf{x})$ is the $i$-th element of $\bm{\phi}_a(\mathbf{x})$ and
    \[
        A_{ij}(\mathbf{x}) = \phi_{ai}(\mathbf{x}_a) \phi_{bj}(\mathbf{x}_b)
    \] 
    Therefore, the new kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = 
        k_a(\mathbf{x}_a, \mathbf{x}_a') k_b(\mathbf{x}_b, \mathbf{x}_b')
        \tag{6.22}\label{eq:6.22}
    \end{equation*}
    will also be valid.
\end{proof}

\section*{Exercise 6.10 $\star$}
Show that an excellent choice of kernel for learning a function
$f(\mathbf{x})$ is given by $k(\mathbf{x}, \mathbf{x}') = f(\mathbf{x})f(\mathbf{x}')$
by showing that a linear learning machine-based on this kernel will always find a solution
proportional to $f(\mathbf{x})$.

\vspace{1em}

\begin{proof}
    By substituting the kernel and (6.8) into (6.9), one has that
    \[
        y(\mathbf{x}) 
        = \mathbf{k}(\mathbf{x})^T(\mathbf{K} + \lambda \mathbf{I}_N)^{-1} \mathbf{t}
        = \mathbf{k}(\mathbf{x})^T \mathbf{a}
        = \sum_{n=1}^{N} k(\mathbf{x}, \mathbf{x}_n) a_n
        = f(\mathbf{x}) \bigg[\sum_{n=1}^{N} f(\mathbf{x}_n) a_n\bigg]
    \] 
    which shows that the prediction function will always be proportional
    to $f(\mathbf{x})$.
\end{proof}

\section*{Exercise 6.11 $\star$}
By making use of the expansion \eqref{eq:6.25}, and then expanding
the middle factor as a power series, show that the Gaussian kernel
(6.23) can be expressed as the inner product
of an infinite-dimensional feature vector.

\vspace{1em}

\begin{proof}
    We've seen in Section 6.2 that the Gaussian kernel can be expanded
    as
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') 
        = \exp{-\frac{\norm{\mathbf{x}}^2}{2\sigma^2}}
        \exp{\frac{\langle \mathbf{x}, \mathbf{x}' \rangle}{\sigma^2}}
        \exp{-\frac{\norm{\mathbf{x}'}^2}{2\sigma^2}}
        \tag{6.25}\label{eq:6.25}
    \end{equation*}
    In Exercise 6.7 we proved that if $\bm{\alpha}, \bm{\beta}$ are feature maps,
    there exists a feature map $\bm{\psi}$ such that
    \[
        \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle
        \langle \bm{\beta}(\mathbf{x}), \bm{\beta}(\mathbf{x}') \rangle
        = \langle \bm{\bm{\psi}}(\mathbf{x}), \bm{\psi}(\mathbf{x}') \rangle
    \] 
    Therefore, one can prove using induction that there exists a feature map $\bm{\zeta}$
    such that for $n \in \mathbb{N}$,
    \[
        \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}')\rangle^n
        = \langle \bm{\zeta}(\mathbf{x}), \bm{\zeta}(\mathbf{x}') \rangle
    \] 
    Now, using the definition of the exponential function for the middle term gives
    \begin{align*}
        \exp{\frac{\langle \mathbf{x}, \mathbf{x}' \rangle}{\sigma^2}}
        = \sum_{i=0}^{\infty} \frac{1}{i! \sigma^{2i}} \langle \mathbf{x}, \mathbf{x}' \rangle^i
        = \sum_{i=0}^{\infty} \frac{1}{i! \sigma^{2i}}
        \langle \bm{\Psi}_i(\mathbf{x}), \bm{\Psi}_i(\mathbf{x}') \rangle
        &= \sum_{i=0}^{\infty} 
        \bigg\langle \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}),
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}') \bigg\rangle
    \end{align*}
    where $\bm{\Psi}_i$ are feature maps such that
    \[
        \langle\mathbf{x}, \mathbf{x}'\rangle^i 
        = \langle \bm{\Psi}_i(\mathbf{x}), \bm{\Psi}_i(\mathbf{x}')\rangle 
    \] 
    Substituting this result back into \eqref{eq:6.25} yields
    \begin{align*}
        k(\mathbf{x}, \mathbf{x}')
        &= \exp{-\frac{\norm{\mathbf{x}}^2}{2\sigma^2}}
        \exp{-\frac{\norm{\mathbf{x}'}^2}{2\sigma^2}}
        \sum_{i=0}^{\infty} \bigg\langle \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}),
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}') \bigg\rangle \\
        &= \sum_{i=0}^{\infty} 
        \exp{-\frac{\norm{\mathbf{x}}^2}{2\sigma^2}}
        \exp{-\frac{\norm{\mathbf{x}'}^2}{2\sigma^2}}
        \bigg\langle \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}),
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}') \bigg\rangle \\
        &= \sum_{i=0}^{\infty} \bigg\langle
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x})
        \exp{-\frac{\norm{\mathbf{x}}^2}{2\sigma^2}},
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}')
        \exp{-\frac{\norm{\mathbf{x}'}^2}{2\sigma^2}} \bigg\rangle \\ 
        &= \sum_{i=0}^{\infty} \phi_i(\mathbf{x}) \phi_i(\mathbf{x}') \\
        &= \langle \bm{\phi}(\mathbf{x}), \bm{\phi}(\mathbf{x}') \rangle
    \end{align*}
    where $\bm{\phi}(\mathbf{x})$ is a feature vector of infinite dimensionality with
    \[
        \phi_i(\mathbf{x}) = 
        \bigg\langle
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x})
        \exp{-\frac{\norm{\mathbf{x}}^2}{2\sigma^2}}\bigg\rangle
    \] 
\end{proof}

\section*{Exercise 6.12 $\star \star$}
Consider the space fo all possible subsets $A$ of a given fixed set $D$.
Show that the kernel function \eqref{eq:6.27} corresponds to an inner product in a feature
space of dimensionality $2^{|D|}$ defined by the mapping $\bm{\phi}(A)$ where $A$ 
is a subset of $D$ and the element $\phi_U(\mathbf{A})$, indexed by the subset
$U$, is given by
\begin{equation*}
    \phi_U(A) = \begin{cases}
        1, & \text{ if }  U \subseteq A \\
        0, & \text{ otherwise }
    \end{cases}
    \tag{6.95}\label{eq:6.95}
\end{equation*}
Here $U \subseteq A$ denotes that $U$ is either a subset of $A$ or is equal to $A$. 

\vspace{1em}

\begin{proof}
    Using simple combinatorics, one can easily show that the number
    of subsets of a given fixed set $D$ is given by $2^{|D|}$.
    Therefore, $\bm{\phi}(A)$ will be of dimensionality $2^{|D|}$. Since
    the element $\phi_U(A)$ is 1 if $U \subseteq A$ and 0 otherwise,
    the result of the inner product $\langle \bm{\phi}(A_1), \bm{\phi}(A_2) \rangle$
    will give the number of subsets of $D$ contained by both $A_1$ and $A_2$. 
    However, since $A_1, A_2 \subseteq D$ this can also be expressed by counting
    the number of subsets of $A_1 \cap A_2$. This is done by the kernel
    \begin{equation*}
        k(A_1, A_2) = 2^{|A_1 \cap A_2|}
        \tag{6.27}\label{eq:6.27}
    \end{equation*}
    Hence, the kernel can be written as an inner product in the space defined
    by the mapping $\bm{\phi}(A)$ since
     \[
        k(A_1, A_2) = 2^{|A_1 \cap A_2|} = \langle \bm{\phi}(A_1), \bm{\phi}(A_2) \rangle
    \] 
\end{proof}

\section*{Exercise 6.13 $\star$ TODO}
Show that the Fisher kernel, defined by \eqref{eq:6.33}, remains
invariant if we make a nonlinear transformation of the parameter
vector $\bm{\theta} \to \bm{\psi}(\bm{\theta})$, where the function
$\bm{\psi}(\cdot)$ is invertible and differentiable.

\vspace{1em}

\begin{proof}
    The Fisher kernel is defined by
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') 
        = \mathbf{g}(\bm{\theta}, \mathbf{x})^T \mathbf{F}^{-1} \mathbf{g}(\bm{\theta}, \mathbf{x}')
        \tag{6.33}\label{eq:6.33}
    \end{equation*}
    where 
    \begin{equation*}
        \mathbf{g}(\bm{\theta}, \mathbf{x}) 
        = \nabla_{\bm{\theta}} \ln p(\mathbf{x} | \bm{\theta})
        \tag{6.32}\label{eq:6.32}
    \end{equation*}
    is the Fisher \emph{score} and $\mathbf{F}$ is the Fisher \emph{information matrix}, given
    by
    \begin{equation*}
        \mathbf{F} = \mathbb{E}_\mathbf{x}\big[
        \mathbf{g}(\bm{\theta}, \mathbf{x})\mathbf{g}(\bm{\theta}, \mathbf{x})^T\big]
        \tag{6.34}\label{eq:6.34}
    \end{equation*}
\end{proof}

\section*{Exercise 6.14 $\star$}
Write down the form of the Fisher kernel, defined by \eqref{eq:6.33},
for the case of a distribution 
$p(\mathbf{x} | \bm{\mu}) = \mathcal{N}(\mathbf{x} | \bm{\mu}, \mathbf{S})$ 
that is Gaussian with mean $\bm{\mu}$ and fixed covariance $\mathbf{S}$.

\vspace{1em}

\begin{proof}
    We start by evaluating the Fisher \emph{score} using \eqref{eq:6.32}:
    \begin{align*}
        \mathbf{g}(\bm{\mu}, \mathbf{x})
        &= \nabla_{\bm{\mu}} \ln p(\mathbf{x} | \bm{\mu}) \\
        &= \nabla_{\bm{\mu}} \ln \mathcal{N}(\mathbf{x} | \bm{\mu}, \mathbf{S}) \\
        &= \nabla_{\bm{\mu}} \ln \bigg[\frac{1}{(2\pi)^{k/2} |\mathbf{S}|^{1/2}}
        \exp{-\frac{1}{2}(\mathbf{x} - \bm{\mu})^T \mathbf{S}^{-1}(\mathbf{x} - \bm{\mu})}\bigg] \\
        &= \nabla_{\bm{\mu}} \bigg[-\frac{1}{2}(\mathbf{x} - \bm{\mu}) 
            \mathbf{S}^{-1} (\mathbf{x} - \bm{\mu})\bigg] \\
        &= \mathbf{S}^{-1}(\mathbf{x} - \bm{\mu})
    \end{align*}
    Now, the \emph{information matrix} will be given by \eqref{eq:6.34}:
    \begin{align*}
        \mathbf{F} 
        = \mathbb{E}_{\mathbf{x}}\big[\mathbf{g}(\bm{\mu}, \mathbf{x}) 
        \mathbf{g}(\bm{\mu}, \mathbf{x})^T\big]
        = \mathbb{E}_{\mathbf{x}}\big[\mathbf{S}^{-1}
        (\mathbf{x} - \bm{\mu})(\mathbf{x} - \bm{\mu})^T \mathbf{S}^{-1}\big]
        = \mathbf{S}^{-1} \mathbb{E}_{\mathbf{x}}\big[
        (\mathbf{x} - \bm{\mu})(\mathbf{x} - \bm{\mu})^T\big] \mathbf{S}^{-1}
    \end{align*}
    Since the expectation corresponds to the covariance matrix, we have that
    \[
        \mathbf{F} = \mathbf{S}^{-1}
    \] 
    Finally, the Fisher kernel can be obtained by substituting the obtained values
    into \eqref{eq:6.33}:
    \[
        k(\mathbf{x}, \mathbf{x}')
        = \mathbf{g}(\bm{\mu}, \mathbf{x})^T \mathbf{F}^{-1} \mathbf{g}(\bm{\mu}, \mathbf{x}')
        = (\mathbf{x} - \bm{\mu})^T \mathbf{S}^{-1} \mathbf{S} \mathbf{S}^{-1} (\mathbf{x} - \bm{\mu})
        = (\mathbf{x} - \bm{\mu})^T \mathbf{S}^{-1} (\mathbf{x} - \bm{\mu})
    \] 
    which turns out to be the Mahalanobis distance.
\end{proof}

\section*{Exercise 6.15 $\star$}
By considering the determinant of a $2 \times 2$ Gram matrix,
show that a positive definite kernel function $k(x, x')$ satisfies
the Cauchy-Schwartz inequality
\begin{equation*}
    k(x_1, x_2)^2 \leq k(x_1, x_1)k(x_2, x_2)
    \tag{6.96}\label{eq:6.96}
\end{equation*}

\vspace{1em}

\begin{proof}
    Consider the $2 \times 2$ Gram matrix corresponding to the kernel $k$:
    \[
        \mathbf{K} = 
        \begin{bmatrix}
            k(x_1, x_1) & k(x_1, x_2) \\
            k(x_2, x_1) & k(x_2, x_2)
        \end{bmatrix}
    \] 
    Since the kernel function is symmetric, the determinant of $K$ is given by
    \[
        |\mathbf{K}| = k(x_1, x_1) k(x_2, x_2) - k(x_1, x_2)^2
    \] 
    Now, since the Gram matrix $\mathbf{K}$ is positive definite, its eigenvalues
    are positive. Since the determinant of a matrix is given by the product
    of its eigenvalues, then the determinant of $\mathbf{K}$ must then be positive.
    Hence,
    \begin{equation*}
        k(x_1, x_2)^2 \leq k(x_1, x_1)k(x_2, x_2)
        \tag{6.96}
    \end{equation*}
\end{proof}
