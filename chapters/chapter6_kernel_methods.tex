\chapter{Kernel Methods}

\section*{Exercise 6.1 $\star \star$}
Consider the dual formulation of the least squares linear
regression problem given in Section 6.1. Show that the
solution for the components $a_n$ of the vector
$\mathbf{a}$ can be expressed as a linear combination of the
elements of the vector $\bm{\phi}(\mathbf{x}_n)$. Denoting 
these coefficients by the vector $\mathbf{w}$, show
that the dual of the dual formulation is given by
the original representation in terms of the parameter vector
$\mathbf{w}$.

\vspace{1em}

\begin{proof}
    By rewriting (6.4), one has that
    \begin{align*}
        a_n 
        &= -\frac{1}{\lambda} \{\mathbf{w}^T \bm{\phi}(\mathbf{x}_n) - t_n\} \\
        &= -\frac{1}{\lambda} \bigg\{\sum_{i=1}^{M} w_i \phi_i(\mathbf{x}_n) 
            - \frac{t_n}{\sum_{i=1}^{M} \phi_i(\mathbf{x}_n)} \sum_{i=1}^{M} \phi_i(\mathbf{x}_n)
        \bigg\} \\
        &= \sum_{i=1}^{M} \bigg(
            \frac{t_n}{\lambda \sum_{i=1}^{M} \phi_i(\mathbf{x}_n)} - \frac{w_i}{\lambda}\bigg)
            \phi_i(\mathbf{x}_n) \\
        &= \sum_{i=1}^{M} \Omega_{ni} \phi_i(\mathbf{x}_n) \\
        &= \bm{\Omega}_n^T \bm{\phi}(\mathbf{x}_n)
    \end{align*}
    where
    \[
        \Omega_{ni} = 
            \frac{t_n}{\lambda \sum_{i=1}^{M} \phi_i(\mathbf{x}_n)} - \frac{w_i}{\lambda}
    \] 
    Therefore, $a_n$ can be written as a linear combination of the elements
    of $\bm{\phi}(\mathbf{x}_n)$ and
    \[
        \mathbf{a} = \text{diag}(\bm{\Omega}\bm{\Phi})
    \] 
\end{proof}

\section*{Exercise 6.3 $\star$}
The nearest-neighbour classifier (Section 2.5.2) assigns
a new input vector $\mathbf{x}$ to the same class as that of the nearest input
vector $\mathbf{x}_n$ from the training set, where in the simple case,
the distance is defined by the Euclidean metric $\norm{\mathbf{x} - \mathbf{x}_n}^2$.
By expressing this rule in terms of scalar product and then making use of kernel
substitution, formulate the nearest-neighbour classifier for a general nonlinear kernel.

\vspace{1em}

\begin{proof}
    Since we're dealing with inner products over $\mathbb{R}$,
    the Euclidian metric can be rewritten as
    \begin{align*}
        \norm{\mathbf{x} - \mathbf{x}_n}^2
        = \langle \mathbf{x} - \mathbf{x}_n, \mathbf{x} - \mathbf{x}_n\rangle
        = \langle\mathbf{x}, \mathbf{x}\rangle - 2\langle \mathbf{x}, \mathbf{x}_n \rangle
        + \langle \mathbf{x}_n \mathbf{x}_n \rangle
    \end{align*}
    Similarly to what happens in Section 6.2, using kernel
    substitution above to replace $\langle \mathbf{x}, \mathbf{x}' \rangle$ with
    a nonlinear kernel $\kappa(\mathbf{x}, \mathbf{x}')$ yields the
    nearest-neighbour classifier for a general nonlinear kernel:
    \[
        k(\mathbf{x}, \mathbf{x}') 
        = \kappa(\mathbf{x}, \mathbf{x}) - 2\kappa(\mathbf{x}, \mathbf{x}_n)
        + \kappa(\mathbf{x}_n, \mathbf{x}_n)
    \] 
\end{proof}

\section*{Exercise 6.4 $\star$}
In Appendix C, we give an example of a matrix that has positive elements
but that ahas a negative eigenvalue and hence that is not positive
definite. Find an example of the converse property, namely a $2 \times 2$ 
matrix with positive eigenvalues that has at least one negative element.

\vspace{1em}

\begin{proof}
    Consider the matrix 
    \[
        A = 
        \begin{bmatrix}
            2 & 1 \\
            -1 & 2
        \end{bmatrix}
    \] 
    $A$ contains one negative element and the eigenvalues of $A$ 
    are $\lambda_1 = 1$ and $\lambda_2 = 3$, which proves that
    a matrix can be positive definite and have negative elements.
\end{proof}

\section*{Exercise 6.5 $\star$}
Verify the results \eqref{eq:6.13} and \eqref{eq:6.14} for constructing valid kernels.

\vspace{1em}

\begin{proof}
    Since $k_1$ is a valid kernel, let $\bm{\alpha}$ be
    a feature mapping such that
    \[
        k_1(\mathbf{x}, \mathbf{x}') 
        = \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle
    \] 
    Using the fact that an inner product on a real vector space is a 
    positive-definite symmetric bilinear form, we have that
    \[
        ck_1(\mathbf{x}, \mathbf{x}')
        = c \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle
        = \langle \sqrt{c} \bm{\alpha}(\mathbf{x}), \sqrt{c} \bm{\alpha}(\mathbf{x}') \rangle
        = \langle \bm{\beta}(\mathbf{x}), \bm{\beta}(\mathbf{x}') \rangle
    \] 
    where $c > 0$ is a constant and $\bm{\beta}(\mathbf{x}) = \sqrt{c}\bm{\alpha}(\mathbf{x})$.
    Therefore, the new kernel 
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = ck_1(\mathbf{x}, \mathbf{x}')
        \tag{6.13}\label{eq:6.13}
    \end{equation*}
    is valid. Analogously, since $f(\cdot)$ is a real-valued function, 
    \[
        f(\mathbf{x}) k_1(\mathbf{x}, \mathbf{x}') f(\mathbf{x}')
        = f(\mathbf{x}) \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle
        f(\mathbf{x}')
        = \langle f(\mathbf{x}) \bm{\alpha}(\mathbf{x}), f(\mathbf{x}') \bm{\alpha}(\mathbf{x}')
        \rangle
        = \langle \bm{\gamma}(\mathbf{x}), \bm{\gamma}(\mathbf{x}') \rangle
    \] 
    where $\bm{\gamma}(\mathbf{x}) = f(\mathbf{x}) \bm{\alpha}(\mathbf{x})$.
    As a result, the kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = f(\mathbf{x}) k_1(\mathbf{x}, \mathbf{x}') f(\mathbf{x}')
        \tag{6.14}\label{eq:6.14}
    \end{equation*}
    will also be valid.
\end{proof}

\section*{Exercise 6.6 $\star$}
Verify the results \eqref{eq:6.15} and \eqref{eq:6.16} for constructing valid kernels.

\vspace{1em}

\begin{proof}
    Let $q(\cdot)$ be a polynomial with nonnegative coefficients. Since in the polynomial
    kernels are summed and multiplied by nonnegative constants or other kernels, combining
    \eqref{eq:6.13}, \eqref{eq:6.17} and \eqref{eq:6.18} proves that
    the kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = q\big(k_1(\mathbf{x}, \mathbf{x}')\big)
        \tag{6.15}\label{eq:6.15}
    \end{equation*}
    is valid. Now, the exponential function is defined as
    \[
        \exp(x) \coloneqq \sum_{i=0}^{\infty} \frac{x^i}{i!} 
    \],
    so
    \[
        \exp\big(k_1(\mathbf{x}, \mathbf{x}')\big)
        = \sum_{i=0}^{\infty} \frac{k_1(\mathbf{x}, \mathbf{x}')^i}{i!}
    \] 
    Note that the exponential of a kernel is an infinite sequence of kernel sums
    and products (with itself or nonnegative constants), so by using
    \eqref{eq:6.13}, \eqref{eq:6.17}, \eqref{eq:6.18} again, one has that
    the new kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = \exp\big(k_1(\mathbf{x}, \mathbf{x}')\big)
        \tag{6.16}\label{eq:6.16}
    \end{equation*}
    is valid.
\end{proof}

\section*{Exercise 6.7 $\star$}
Verify the results \eqref{eq:6.17} and \eqref{eq:6.18} for constructing valid kernels.

\vspace{1em}

\begin{proof}
    Let $K_1$ and $K_2$ be the Gram matrices corresponding to the 
    kernels $k_1$ and $k_2$. Therefore, they are positive semidefinite matrices, so
    for any $\mathbf{a} \in \mathbb{R}^n$, one has that
    \[
        \mathbf{a}^T\mathbf{H}\mathbf{a}
        = \mathbf{a}^T\big(\mathbf{H}_1 + \mathbf{H}_2\big)\mathbf{a}
        = \mathbf{a}^T \mathbf{H}_1 \mathbf{a} + \mathbf{a}^T \mathbf{H}_2 \mathbf{a} > 0
    \] 
    Since $\mathbf{H} = \mathbf{H}_1 + \mathbf{H}_2$ is positive semidefinite
    and corresponds to the Gram matrix of the 
    kernel $k(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x'}) + 
    k_2(\mathbf{x}, \mathbf{x}')$, one has that the kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') + k_2(\mathbf{x}, \mathbf{x}')
        \tag{6.17}\label{eq:6.17}
    \end{equation*}
    is valid. Now, let $\bm{\alpha}, \bm{\beta}$ be feature mappings such that
    \[
        k_1(\mathbf{x}, \mathbf{x}') 
        = \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle 
    \] 
    \[
        k_2(\mathbf{x}, \mathbf{x}') 
        = \langle \bm{\beta}(\mathbf{x}), \bm{\beta}(\mathbf{x}') \rangle 
    \]
    As a result,
    \begin{align*}
        k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')
        &= \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle 
        \langle \bm{\beta}(\mathbf{x}), \bm{\beta}(\mathbf{x}') \rangle \\
        &= \bm{\alpha}(\mathbf{x})^T \bm{\alpha}(\mathbf{x}')
        \bm{\beta}^T(\mathbf{x}) \bm{\beta}(\mathbf{x}') \\
        &= \bigg[\sum_{i=1}^{N} \alpha_i(\mathbf{x}) \alpha_i(\mathbf{x}')\bigg]
        \bigg[\sum_{j=1}^{M} \beta_i(\mathbf{x}) \beta_i(\mathbf{x}')\bigg] \\
        &= \sum_{i=1}^{N} \sum_{j=1}^{M} \alpha_i(\mathbf{x}) \beta_j(\mathbf{x})
        \alpha_i(\mathbf{x}') \beta_j(\mathbf{x}') \tag{*}\\
        &= \sum_{i=1}^{N} \sum_{j=1}^{M}  A_{ij}(\mathbf{x}) A_{ij}(\mathbf{x}') \\
        &= \langle \mathbf{A}(\mathbf{x}), \mathbf{A}(\mathbf{x'}) \rangle_\mathbf{F}
    \end{align*}
    where $\mathbf{A}$ is a matrix with
    \[
        A_{ij}(\mathbf{x}) = \alpha_i(\mathbf{x}) \beta_j(\mathbf{x})
    \] 
    and $\langle \cdot, \cdot \rangle_\mathbf{F}$ is the Frobenius inner product.
    Since the product kernel can be rewritten as a valid inner product in the 
    feature space defined by the feature mapping $\mathbf{A}(\mathbf{x})$,
    the new kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}')
        = k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')
        \tag{6.18}\label{eq:6.18}
    \end{equation*}
    is valid. Note that we can continue differently from (*), so
    \begin{align*}
        k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')
        = \sum_{k=1}^{K} \phi_k (\mathbf{x}) \phi_k(\mathbf{x}')
        = \langle \bm{\phi}(\mathbf{x}), \bm{\phi}(\mathbf{x}') \rangle
    \end{align*}
    where $K = NM$ and
    \[
        \phi_k(\mathbf{x}) 
        = \alpha_{((k - 1) \varoslash N) + 1}(\mathbf{x})
        \beta_{((k - 1) \odot N) + 1}(\mathbf{x})
    \] 
    where $\varoslash$ and $\odot$ denote integer division and remainder,
    respectively.
\end{proof}

\section*{Exercise 6.8 $\star$}
Verify the results \eqref{eq:6.19} and \eqref{eq:6.20} for constructing valid kernels.

\vspace{1em}

\begin{proof}
    Let $\bm{\psi}$ be a feature mapping such that
    \[
        k_3(\mathbf{x}, \mathbf{x}') 
        = \langle \bm{\psi}(\mathbf{x}), \bm{\psi}(\mathbf{x}') \rangle
    \] 
    Then,
    \begin{align*}
        k_3\big(\bm{\phi}(\mathbf{x}), \bm{\phi}(\mathbf{x}')\big)
        &= \langle \bm{\psi}\big(\bm{\phi}(\mathbf{x})\big),
            \bm{\psi}\big(\bm{\phi}(\mathbf{x}')\big)\rangle \\
        &= \langle \big(\bm{\psi} \circ \bm{\phi}\big)(\mathbf{x}),
            \big(\bm{\psi} \circ \bm{\phi}\big)(\mathbf{x}')\rangle \\
        &= \langle \bm{\gamma}(\mathbf{x}), \bm{\gamma}(\mathbf{x}')\rangle
    \end{align*}
    where $\bm{\phi}$ is a function from $\mathbf{x}$ to $\mathbb{R}^M$ and
    $\bm{\gamma} = \bm{\psi} \circ \bm{\phi}$. Therefore, the kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = k_3\big(\bm{\phi}(\mathbf{x}),
        \bm{\phi}(\mathbf{x}')\big)
        \tag{6.19}\label{eq:6.19} 
    \end{equation*}
    is valid. For the second part, since $\mathbf{A}$ is a 
    symmetric, positive semidefinite matrix, one can use the Cholesky
    decomposition to obtain a matrix $\mathbf{L}$ such that
    \[
        \mathbf{A} = \mathbf{L} \mathbf{L}^T
    \] 
    As a result, one can show that
    \begin{align*}
        \mathbf{x}^T \mathbf{A} \mathbf{x}
        = \mathbf{x}^T \mathbf{L} \mathbf{L}^T \mathbf{x}
        = \big(\mathbf{L}^T \mathbf{x}\big)^T \big(\mathbf{L}^T \mathbf{x}\big)
        = \langle \bm{\zeta}(\mathbf{x}), \bm{\zeta}(\mathbf{x}') \rangle
    \end{align*}
    where $\bm{\zeta}(\mathbf{x}) = \mathbf{L}^T \mathbf{x}$.
    Hence, the kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^T \mathbf{A} \mathbf{x}
        \tag{6.20}\label{eq:6.20}
    \end{equation*}
    is valid.
\end{proof} 

\section*{Exercise 6.9 $\star$}
Verify the results \eqref{eq:6.21} and \eqref{eq:6.22} for constructing valid kernels.

\vspace{1em}

\begin{proof}
    Let $\bm{\phi}_a$ and $\bm{\phi}_b$ be feature mappings so that
    \[
        k_a(\mathbf{x}, \mathbf{x}') 
        = \langle\bm{\phi}_a(\mathbf{x}), \bm{\phi}_a(\mathbf{x}')\rangle
    \] 
    \[
        k_b(\mathbf{x}, \mathbf{x}') 
        = \langle\bm{\phi}_b(\mathbf{x}), \bm{\phi}_b(\mathbf{x}')\rangle
    \] 
    Therefore, since the inner product becomes a bilinear form on $\mathbb{R}$,
    \begin{align*}
        k_a(\mathbf{x}_a, \mathbf{x'}_a) + k_b(\mathbf{x}_b, \mathbf{x}_b')
        &= \langle\bm{\phi}_a(\mathbf{x}_a), \bm{\phi}_a(\mathbf{x}_a')\rangle
        + \langle\bm{\phi}_b(\mathbf{x}_b), \bm{\phi}_b(\mathbf{x}_b')\rangle \\
        &= \langle \big(\bm{\phi}_a(\mathbf{x}_a), \bm{\phi}_a(\mathbf{x}_a')\big),
        \big(\bm{\phi}_b(\mathbf{x}_b), \bm{\phi}_b(\mathbf{x}_b')\big) \rangle \\
        &= \langle \bm{\phi}(\mathbf{x}), \bm{\phi}(\mathbf{x'}) \rangle
    \end{align*}
    where
    \[
        \bm{\phi}(\mathbf{x}) = 
        \begin{bmatrix}
            \bm{\phi}_a(\mathbf{x}_a) \\
            \bm{\phi}_b(\mathbf{x}_b)
        \end{bmatrix}
    \] 
    Hence, the kernel 
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = 
        k_a(\mathbf{x}_a, \mathbf{x}_a') +
        k_b(\mathbf{x}_b, \mathbf{x}_b')
        \tag{6.21}\label{eq:6.21}
    \end{equation*}
    is valid. The product identity is obtained similarly to
    what we do in Exercise 6.7. One has that
    \begin{align*}
        k_a(\mathbf{x}_a, \mathbf{x}_a')k_b(\mathbf{x}_b, \mathbf{x}_b')
        &= \langle\bm{\phi}_a(\mathbf{x}_a), \bm{\phi}_a(\mathbf{x}_a')\rangle
        \langle\bm{\phi}_b(\mathbf{x}_b), \bm{\phi}_b(\mathbf{x}_b')\rangle \\
        &= \bigg[\sum_{i=1}^{N_a} \phi_{ai}(\mathbf{x}_a) \phi_{ai}(\mathbf{x}_a')\bigg]
        \bigg[\sum_{j=1}^{N_b} \phi_{bj}(\mathbf{x}_b) \phi_{bj}(\mathbf{x}_b')\bigg] \\
        &= \sum_{i=1}^{N_a} \sum_{j=1}^{N_b}
        \phi_{ai}(\mathbf{x}_a) \phi_{bj}(\mathbf{x}_b)
        \phi_{ai}(\mathbf{x}_a') \phi_{bj}(\mathbf{x}_b') \\
        &= \sum_{i=1}^{N_a} \sum_{j=1}^{N_b} A_{ij}(\mathbf{x}) A_{ij}(\mathbf{x}') \\
        &= \langle \mathbf{A}(\mathbf{x}), \mathbf{A}(\mathbf{x}') \rangle_{\bm{F}}
    \end{align*}
    where $\langle \cdot, \cdot \rangle_{\bm{F}}$ is the Frobenius inner product,
    $\phi_{ai}(\mathbf{x})$ is the $i$-th element of $\bm{\phi}_a(\mathbf{x})$ and
    \[
        A_{ij}(\mathbf{x}) = \phi_{ai}(\mathbf{x}_a) \phi_{bj}(\mathbf{x}_b)
    \] 
    Therefore, the new kernel
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') = 
        k_a(\mathbf{x}_a, \mathbf{x}_a') k_b(\mathbf{x}_b, \mathbf{x}_b')
        \tag{6.22}\label{eq:6.22}
    \end{equation*}
    will also be valid.
\end{proof}

\section*{Exercise 6.10 $\star$}
Show that an excellent choice of kernel for learning a function
$f(\mathbf{x})$ is given by $k(\mathbf{x}, \mathbf{x}') = f(\mathbf{x})f(\mathbf{x}')$
by showing that a linear learning machine-based on this kernel will always find a solution
proportional to $f(\mathbf{x})$.

\vspace{1em}

\begin{proof}
    By substituting the kernel and (6.8) into (6.9), one has that
    \[
        y(\mathbf{x}) 
        = \mathbf{k}(\mathbf{x})^T(\mathbf{K} + \lambda \mathbf{I}_N)^{-1} \mathbf{t}
        = \mathbf{k}(\mathbf{x})^T \mathbf{a}
        = \sum_{n=1}^{N} k(\mathbf{x}, \mathbf{x}_n) a_n
        = f(\mathbf{x}) \bigg[\sum_{n=1}^{N} f(\mathbf{x}_n) a_n\bigg]
    \] 
    which shows that the prediction function will always be proportional
    to $f(\mathbf{x})$.
\end{proof}

\section*{Exercise 6.11 $\star$}
By making use of the expansion \eqref{eq:6.25}, and then expanding
the middle factor as a power series, show that the Gaussian kernel
(6.23) can be expressed as the inner product
of an infinite-dimensional feature vector.

\vspace{1em}

\begin{proof}
    We've seen in Section 6.2 that the Gaussian kernel can be expanded
    as
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') 
        = \exp{-\frac{\norm{\mathbf{x}}^2}{2\sigma^2}}
        \exp{\frac{\langle \mathbf{x}, \mathbf{x}' \rangle}{\sigma^2}}
        \exp{-\frac{\norm{\mathbf{x}'}^2}{2\sigma^2}}
        \tag{6.25}\label{eq:6.25}
    \end{equation*}
    In Exercise 6.7 we proved that if $\bm{\alpha}, \bm{\beta}$ are feature maps,
    there exists a feature map $\bm{\psi}$ such that
    \[
        \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}') \rangle
        \langle \bm{\beta}(\mathbf{x}), \bm{\beta}(\mathbf{x}') \rangle
        = \langle \bm{\bm{\psi}}(\mathbf{x}), \bm{\psi}(\mathbf{x}') \rangle
    \] 
    Therefore, one can prove using induction that there exists a feature map $\bm{\zeta}$
    such that for $n \in \mathbb{N}$,
    \[
        \langle \bm{\alpha}(\mathbf{x}), \bm{\alpha}(\mathbf{x}')\rangle^n
        = \langle \bm{\zeta}(\mathbf{x}), \bm{\zeta}(\mathbf{x}') \rangle
    \] 
    Now, using the definition of the exponential function for the middle term gives
    \begin{align*}
        \exp{\frac{\langle \mathbf{x}, \mathbf{x}' \rangle}{\sigma^2}}
        = \sum_{i=0}^{\infty} \frac{1}{i! \sigma^{2i}} \langle \mathbf{x}, \mathbf{x}' \rangle^i
        = \sum_{i=0}^{\infty} \frac{1}{i! \sigma^{2i}}
        \langle \bm{\Psi}_i(\mathbf{x}), \bm{\Psi}_i(\mathbf{x}') \rangle
        &= \sum_{i=0}^{\infty} 
        \bigg\langle \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}),
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}') \bigg\rangle
    \end{align*}
    where $\bm{\Psi}_i$ are feature maps such that
    \[
        \langle\mathbf{x}, \mathbf{x}'\rangle^i 
        = \langle \bm{\Psi}_i(\mathbf{x}), \bm{\Psi}_i(\mathbf{x}')\rangle 
    \] 
    Substituting this result back into \eqref{eq:6.25} yields
    \begin{align*}
        k(\mathbf{x}, \mathbf{x}')
        &= \exp{-\frac{\norm{\mathbf{x}}^2}{2\sigma^2}}
        \exp{-\frac{\norm{\mathbf{x}'}^2}{2\sigma^2}}
        \sum_{i=0}^{\infty} \bigg\langle \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}),
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}') \bigg\rangle \\
        &= \sum_{i=0}^{\infty} 
        \exp{-\frac{\norm{\mathbf{x}}^2}{2\sigma^2}}
        \exp{-\frac{\norm{\mathbf{x}'}^2}{2\sigma^2}}
        \bigg\langle \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}),
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}') \bigg\rangle \\
        &= \sum_{i=0}^{\infty} \bigg\langle
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x})
        \exp{-\frac{\norm{\mathbf{x}}^2}{2\sigma^2}},
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x}')
        \exp{-\frac{\norm{\mathbf{x}'}^2}{2\sigma^2}} \bigg\rangle \\ 
        &= \sum_{i=0}^{\infty} \phi_i(\mathbf{x}) \phi_i(\mathbf{x}') \\
        &= \langle \bm{\phi}(\mathbf{x}), \bm{\phi}(\mathbf{x}') \rangle
    \end{align*}
    where $\bm{\phi}(\mathbf{x})$ is a feature vector of infinite dimensionality with
    \[
        \phi_i(\mathbf{x}) = 
        \bigg\langle
        \frac{1}{\sigma}\sqrt{\frac{1}{i!}} \bm{\Psi}_i(\mathbf{x})
        \exp{-\frac{\norm{\mathbf{x}}^2}{2\sigma^2}}\bigg\rangle
    \] 
\end{proof}

\section*{Exercise 6.12 $\star \star$}
Consider the space fo all possible subsets $A$ of a given fixed set $D$.
Show that the kernel function \eqref{eq:6.27} corresponds to an inner product in a feature
space of dimensionality $2^{|D|}$ defined by the mapping $\bm{\phi}(A)$ where $A$ 
is a subset of $D$ and the element $\phi_U(\mathbf{A})$, indexed by the subset
$U$, is given by
\begin{equation*}
    \phi_U(A) = \begin{cases}
        1, & \text{ if }  U \subseteq A \\
        0, & \text{ otherwise }
    \end{cases}
    \tag{6.95}\label{eq:6.95}
\end{equation*}
Here $U \subseteq A$ denotes that $U$ is either a subset of $A$ or is equal to $A$. 

\vspace{1em}

\begin{proof}
    Using simple combinatorics, one can easily show that the number
    of subsets of a given fixed set $D$ is given by $2^{|D|}$.
    Therefore, $\bm{\phi}(A)$ will be of dimensionality $2^{|D|}$. Since
    the element $\phi_U(A)$ is 1 if $U \subseteq A$ and 0 otherwise,
    the result of the inner product $\langle \bm{\phi}(A_1), \bm{\phi}(A_2) \rangle$
    will give the number of subsets of $D$ contained by both $A_1$ and $A_2$. 
    However, since $A_1, A_2 \subseteq D$ this can also be expressed by counting
    the number of subsets of $A_1 \cap A_2$. This is done by the kernel
    \begin{equation*}
        k(A_1, A_2) = 2^{|A_1 \cap A_2|}
        \tag{6.27}\label{eq:6.27}
    \end{equation*}
    Hence, the kernel can be written as an inner product in the space defined
    by the mapping $\bm{\phi}(A)$ since
     \[
        k(A_1, A_2) = 2^{|A_1 \cap A_2|} = \langle \bm{\phi}(A_1), \bm{\phi}(A_2) \rangle
    \] 
\end{proof}

\section*{Exercise 6.13 $\star$ TODO}
Show that the Fisher kernel, defined by \eqref{eq:6.33}, remains
invariant if we make a nonlinear transformation of the parameter
vector $\bm{\theta} \to \bm{\psi}(\bm{\theta})$, where the function
$\bm{\psi}(\cdot)$ is invertible and differentiable.

\vspace{1em}

\begin{proof}
    The Fisher kernel is defined by
    \begin{equation*}
        k(\mathbf{x}, \mathbf{x}') 
        = \mathbf{g}(\bm{\theta}, \mathbf{x})^T \mathbf{F}^{-1} \mathbf{g}(\bm{\theta}, \mathbf{x}')
        \tag{6.33}\label{eq:6.33}
    \end{equation*}
    where 
    \begin{equation*}
        \mathbf{g}(\bm{\theta}, \mathbf{x}) 
        = \nabla_{\bm{\theta}} \ln p(\mathbf{x} | \bm{\theta})
        \tag{6.32}\label{eq:6.32}
    \end{equation*}
    is the Fisher \emph{score} and $\mathbf{F}$ is the Fisher \emph{information matrix}, given
    by
    \begin{equation*}
        \mathbf{F} = \mathbb{E}_\mathbf{x}\big[
        \mathbf{g}(\bm{\theta}, \mathbf{x})\mathbf{g}(\bm{\theta}, \mathbf{x})^T\big]
        \tag{6.34}\label{eq:6.34}
    \end{equation*}
\end{proof}

\section*{Exercise 6.14 $\star$}
Write down the form of the Fisher kernel, defined by \eqref{eq:6.33},
for the case of a distribution 
$p(\mathbf{x} | \bm{\mu}) = \mathcal{N}(\mathbf{x} | \bm{\mu}, \mathbf{S})$ 
that is Gaussian with mean $\bm{\mu}$ and fixed covariance $\mathbf{S}$.

\vspace{1em}

\begin{proof}
    We start by evaluating the Fisher \emph{score} using \eqref{eq:6.32}:
    \begin{align*}
        \mathbf{g}(\bm{\mu}, \mathbf{x})
        &= \nabla_{\bm{\mu}} \ln p(\mathbf{x} | \bm{\mu}) \\
        &= \nabla_{\bm{\mu}} \ln \mathcal{N}(\mathbf{x} | \bm{\mu}, \mathbf{S}) \\
        &= \nabla_{\bm{\mu}} \ln \bigg[\frac{1}{(2\pi)^{k/2} |\mathbf{S}|^{1/2}}
        \exp{-\frac{1}{2}(\mathbf{x} - \bm{\mu})^T \mathbf{S}^{-1}(\mathbf{x} - \bm{\mu})}\bigg] \\
        &= \nabla_{\bm{\mu}} \bigg[-\frac{1}{2}(\mathbf{x} - \bm{\mu}) 
            \mathbf{S}^{-1} (\mathbf{x} - \bm{\mu})\bigg] \\
        &= \mathbf{S}^{-1}(\mathbf{x} - \bm{\mu})
    \end{align*}
    Now, the \emph{information matrix} will be given by \eqref{eq:6.34}:
    \begin{align*}
        \mathbf{F} 
        = \mathbb{E}_{\mathbf{x}}\big[\mathbf{g}(\bm{\mu}, \mathbf{x}) 
        \mathbf{g}(\bm{\mu}, \mathbf{x})^T\big]
        = \mathbb{E}_{\mathbf{x}}\big[\mathbf{S}^{-1}
        (\mathbf{x} - \bm{\mu})(\mathbf{x} - \bm{\mu})^T \mathbf{S}^{-1}\big]
        = \mathbf{S}^{-1} \mathbb{E}_{\mathbf{x}}\big[
        (\mathbf{x} - \bm{\mu})(\mathbf{x} - \bm{\mu})^T\big] \mathbf{S}^{-1}
    \end{align*}
    Since the expectation corresponds to the covariance matrix, we have that
    \[
        \mathbf{F} = \mathbf{S}^{-1}
    \] 
    Finally, the Fisher kernel can be obtained by substituting the obtained values
    into \eqref{eq:6.33}:
    \[
        k(\mathbf{x}, \mathbf{x}')
        = \mathbf{g}(\bm{\mu}, \mathbf{x})^T \mathbf{F}^{-1} \mathbf{g}(\bm{\mu}, \mathbf{x}')
        = (\mathbf{x} - \bm{\mu})^T \mathbf{S}^{-1} \mathbf{S} \mathbf{S}^{-1} (\mathbf{x} - \bm{\mu})
        = (\mathbf{x} - \bm{\mu})^T \mathbf{S}^{-1} (\mathbf{x} - \bm{\mu})
    \] 
    which turns out to be the Mahalanobis distance.
\end{proof}

\section*{Exercise 6.15 $\star$}
By considering the determinant of a $2 \times 2$ Gram matrix,
show that a positive definite kernel function $k(x, x')$ satisfies
the Cauchy-Schwartz inequality
\begin{equation*}
    k(x_1, x_2)^2 \leq k(x_1, x_1)k(x_2, x_2)
    \tag{6.96}\label{eq:6.96}
\end{equation*}

\vspace{1em}

\begin{proof}
    Consider the $2 \times 2$ Gram matrix corresponding to the kernel $k$:
    \[
        \mathbf{K} = 
        \begin{bmatrix}
            k(x_1, x_1) & k(x_1, x_2) \\
            k(x_2, x_1) & k(x_2, x_2)
        \end{bmatrix}
    \] 
    Since the kernel function is symmetric, the determinant of $K$ is given by
    \[
        |\mathbf{K}| = k(x_1, x_1) k(x_2, x_2) - k(x_1, x_2)^2
    \] 
    Now, since the Gram matrix $\mathbf{K}$ is positive definite, its eigenvalues
    are positive. Since the determinant of a matrix is given by the product
    of its eigenvalues, then the determinant of $\mathbf{K}$ must then be positive.
    Hence,
    \begin{equation*}
        k(x_1, x_2)^2 \leq k(x_1, x_1)k(x_2, x_2)
        \tag{6.96}
    \end{equation*}
\end{proof}

\section*{Exercise 6.16 $\star \star$}
Consider a parametric model governed by the parameter vector
$\mathbf{w}$ together with a data set of input values $\mathbf{x}_1
\ldots, \mathbf{x}_N$ and a nonlinear feature mapping $\bm{\phi}(\mathbf{x})$.
Suppose that the dependence of the error function on $\mathbf{w}$ takes the form
\begin{equation}
    J(\mathbf{w}) 
    = f\big(\mathbf{w}^T\bm{\phi}(\mathbf{x}_1), \ldots, \mathbf{w}^T\bm{\phi}(\mathbf{x}_N)\big)
    + g(\mathbf{w}^T \mathbf{w})
    \tag{6.97}\label{eq:6.97}
\end{equation}
where $g(\cdot)$ is a monotonically increasing function. By writing $\mathbf{w}$ in 
the form
\begin{equation}
    \mathbf{w} = \sum_{n=1}^{N}  \alpha_n \bm{\phi}(\mathbf{x}_n) + \mathbf{w}_\perp
    \tag{6.98}\label{eq:6.98}
\end{equation}
show that the value of $\mathbf{w}$ that minimizes $J(\mathbf{w})$ takes the form of a
linear combination of the basis functions $\bm{\phi}(\mathbf{x}_n)$ for $n=1,\ldots,N$.

\vspace{1em}

\begin{proof}
    Taking the derivative of \eqref{eq:6.97} with respect to $\mathbf{w}$ yields:
    \begin{align*}
        \dv{\mathbf{w}} J(\mathbf{w})
        &= \pdv{\mathbf{w}} 
        f\big(\mathbf{w}^T\bm{\phi}(\mathbf{x}_1), \ldots, \mathbf{w}^T\bm{\phi}(\mathbf{x}_N)\big)
        + \pdv{\mathbf{w}} g(\mathbf{w}^T \mathbf{w}) \\
        &= \sum_{i=1}^{N} \pdv{f}{\mathbf{w}^T \bm{\phi}(\mathbf{x}_n)} 
        \pdv{\mathbf{w}^T \bm{\phi}(\mathbf{x}_n)}{\mathbf{w}}
        + \pdv{g}{\mathbf{w}^T \mathbf{w}} \pdv{\mathbf{w}^T \mathbf{w}}{\mathbf{w}} \\
        &= \sum_{i=1}^{N} \pdv{f}{\mathbf{w}^T \bm{\phi}(\mathbf{x}_n)} 
        \bm{\phi}(\mathbf{x}_n)^T 
        + \pdv{g}{\mathbf{w}^T \mathbf{w}} 2\mathbf{w}^T
    \end{align*}
    Since the error function is convex, it is minimized when the derivative
    is 0, so when
    \[
        \mathbf{w}
        = -\frac{1}{2}\bigg(\pdv{g}{\mathbf{w}^T \mathbf{w}}\bigg)^{-1}
        \sum_{i=1}^{N} \pdv{f}{\mathbf{w}^T \bm{\phi}(\mathbf{x}_n)} 
        \bm{\phi}(\mathbf{x}_n)
        = \sum_{i=1}^{N} \alpha_n \bm{\phi}(\mathbf{x}_n)
    \] 
    with
    \[
        \alpha_n = 
        -\frac{1}{2}\bigg(\pdv{g}{\mathbf{w}^T \mathbf{w}}\bigg)^{-1}
        \pdv{f}{\mathbf{w}^T \bm{\phi}(\mathbf{x}_n)} 
    \] 
    which is equivalent to \eqref{eq:6.98} for $\mathbf{w}_\perp = \mathbf{0}$.
\end{proof}

\section*{Exercise 6.18 $\star$}
Consider a Nadaraya-Watson model with one input variable $x$ and one target variable
$t$ having Gaussian components with isotropic covariances,
so that the covariance matrix is given by  $\sigma^2\mathbf{I}$ where $\mathbf{I}$ is
the unit matrix. Write down experssions for the conditional density $p(t | x)$ and
for the conditional mean $\mathbb{E}[t | x]$ and variance var[$t | x$], in terms of the 
kernel function $k(x, x_n)$.

\vspace{1em}

\begin{proof}
To simplify the notation, let
    \[
        \mathbf{z} = \begin{pmatrix}
            x \\
            t
        \end{pmatrix}
    \] 
Since the model has Gaussian components with isotropic covariances,
    \[
        f(x, t) = f(z) = \mathcal{N}(\mathbf{z} | \mathbf{0}, \sigma^2 \mathbf{I})
    \] 
and therefore,
    \[
        f(x - x_n, t - t_n) 
        = f(\mathbf{z} - \mathbf{z}_n) 
        = \mathcal{N}(\mathbf{z} - \mathbf{z}_n | \mathbf{0}, \sigma^2 \mathbf{I})
        = \mathcal{N}(\mathbf{z} | \mathbf{z}_n, \sigma^2 \mathbf{I})
    \] 
To aid computation, we note that
\begin{align*}
    f(\mathbf{z} - \mathbf{z}_n) 
    &= \mathcal{N}(\mathbf{z} | \mathbf{z}_n, \sigma^2 \mathbf{I}) \\
    &= \frac{1}{2 \pi \sigma^2} \exp{-\frac{1}{2\sigma^2} ||\mathbf{z} - \mathbf{z}_n||^2} \\
    &= \frac{1}{2 \pi \sigma^2} \exp{-\frac{(x - x_n)^2 + (t - t_n)^2}{2\sigma^2}} \\
    &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp{-\frac{(x - x_n)^2}{2\sigma^2}}
    \frac{1}{\sqrt{2\pi\sigma^2}} \exp{-\frac{(t - t_n)^2}{2\sigma^2}} \\
    &= \mathcal{N}(x | x_n, \sigma^2) \mathcal{N}(t | t_n, \sigma^2)
\end{align*}
and
\[
    \int f(\mathbf{z} - \mathbf{z}_n) \diff t
    = \int \mathcal{N}(x | x_n, \sigma^2) \mathcal{N}(t | t_n, \sigma^2) \diff t
    = \mathcal{N}(x | x_n, \sigma^2) \int \mathcal{N}(t | t_n, \sigma^2) \diff t
    = \mathcal{N}(x | x_n, \sigma^2)
\] 
By the Nadaraya-Watson model, from (6.42) we have that
\[
    p(x, t) = \frac{1}{N} \sum_{n=1}^{N} f(x - x_n, t - t_n)
    = \frac{1}{N} \sum_{n=1}^{N} f(\mathbf{z} - \mathbf{z}_n) 
    = \frac{1}{N} \sum_{n=1}^{N} \mathcal{N}(x | x_n, \sigma^2) \mathcal{N}(t | t_n, \sigma^2)
\] 
The conditional probability is now given by
\begin{align*}
    p(t | x) = \frac{p(t, x)}{\displaystyle \int_{}^{} p(t, x) \diff t}
    = \frac{\displaystyle \sum_{n=1}^{N} f(\mathbf{z} - \mathbf{z}_n)}
    {\displaystyle \sum_{m=1}^{N} \int f(\mathbf{z} - \mathbf{z}_m) \diff t}
    &= \sum_{n=1}^{N} \frac{\mathcal{N}(x | x_n, \sigma^2) \mathcal{N}(t | t_n, \sigma^2)}
    {\displaystyle \sum_{m=1}^{N} \mathcal{N}(x | x_m, \sigma^2)} \\
    &= \sum_{n=1}^{N} k(x, x_n) \mathcal{N}(t | t_n, \sigma^2)
\end{align*}
where 
\[
    k(x, x_n) = \frac{\mathcal{N}(x | x_n, \sigma^2)}
    {\displaystyle \sum_{m=1}^{N} \mathcal{N}(x | x_m, \sigma^2)}
\] 
is the kernel function corresponding to the model.
The conditional expectation is obtained as
\[
    \mathbb{E}[t | x] 
    = \int t p(t | x) \diff t
    = \sum_{n=1}^{N} \int_{}^{} t k(x, x_n) \mathcal{N}(t | t_n, \sigma^2) \diff t
    = \sum_{n=1}^{N} k(x, x_n) t_n
\] 
The variance is then given by
\begin{align*}
    \text{var}[t | x]
    &= \int_{}^{} (t - \mathbb{E}[t | x])^2 p(t | x) \diff t \\
    &= \int_{}^{} t^2 p(t | x) \diff t
    - 2\mathbb{E}[t | x] \int t p(t | x) \diff t + \mathbb{E}[t | x]^2 \int_{}^{} p(t | x) \diff t \\
    &= \sum_{n=1}^{N} k(x, x_n) \int_{}^{} t^2 \mathcal{N}(t | t_n, \sigma^2) \diff t
    - \mathbb{E}[t | x]^2 \\
    &= \sum_{n=1}^{N} k(x, x_n) \big(t_n^2 + \sigma^2\big) 
    - \bigg[\sum_{n=1}^{N} k(x, x_n)\mathcal{N}(t | t_n, \sigma^2)\bigg]^2
\end{align*}
\end{proof}

\section*{Exercise 6.20 $\star \star$}
Verify the results \eqref{eq:6.66} and \eqref{eq:6.67}.

\vspace{1em}

\begin{proof}
    By considering the same setup as the one described in Section 6.4.2,
    e.g. splitting the covariance matrix $\mathbf{C}_{N + 1}$ into (6.65),
    one has that
    \[
        p(\mathbf{t}_N) = \mathcal{N}(\mathbf{t}_N | 0, \mathbf{C}_N)
    \] 
    \[
        p(t_{N + 1}) = \mathcal{N}(t_{N + 1} | 0, c)
    \] 
    \[
        \text{cov}[\mathbf{t}_N, t_{N + 1}] = \mathbf{k}
    \] 
    where $\mathbf{k}$ is a $N$-dimensional vector with
    \[
        k_i = k(\mathbf{x}_i, \mathbf{x}_{N + 1})
    \] 
    and
    \[
        c = k(\mathbf{x}_{N + 1}, x_{N + 1}) + \beta^{-1}
    \]
    Simply matching our variables with the general formulas
    (2.81), (2.82) for the mean and covariance of the conditional distribution
    $p(t_{N + 1} | \mathbf{t}_N)$ gives the desired results
    \begin{equation}
        m(\mathbf{x}_{N + 1}) = \mathbb{E}\big[t_{N + 1} | \mathbf{t}_N, \mathbf{x}_{N + 1}\big]
        = \mathbf{k}^T \mathbf{C}_N^{-1}\mathbf{t}_N
        \tag{6.66}\label{eq:6.66}
    \end{equation}
    \begin{equation}
        \sigma^2(\mathbf{x}_{N + 1}) 
        = \text{var}\big[t_{N + 1} | \mathbf{t}_N, \mathbf{x}_{N + 1}\big]
        = c - \mathbf{k}^T\mathbf{C}_N^{-1}\mathbf{k}
        \tag{6.67}\label{eq:6.67}
    \end{equation}
\end{proof}

\section*{Exercise 6.21 $\star \star$}
Consider a Gaussian process regression model in which
the kernel function is defined in terms of a fixed set of nonlinear basis
functions. Show that the predictive distribution is identical to the result \eqref{eq:3.58}
obtained in Section 3.3.2 for the prediction distributions, and so it is only necessary
to show that the conditional mean and variance are the same. For the mean, make
use of the matrix identity (C.6), and for the variance, make use of the
matrix identity (C.7).

\vspace{1em}

\begin{proof}
    Let the kernel function of our Gaussian process regression model be defined
    by 
    \[
        k(\mathbf{x}_n, \mathbf{x}_m) 
        = \frac{1}{\alpha} \bm{\phi}(\mathbf{x}_n)^T \bm{\phi}(\mathbf{x}_m)
    \] 
    where $\bm{\phi}$ are nonlinear basis functions and $\alpha$ represents
    the precision of $p(\mathbf{w)}$. As stated above, our goal
    is to show that the mean $m(\mathbf{x}_{N + 1})$ and covariance 
    $\sigma^2(\mathbf{x}_{N + 1})$ are equivalent to the ones obtained in Section 3.3.2
    for the predictive distribution given by \eqref{eq:3.58}. Notice that both $\mathbf{k}$ 
    and $\mathbf{C}_N$ can be rewritten as forms depending on the basis functions. Since
    \[
        k_i = k(\mathbf{x}_i, \mathbf{x}_{N + 1})
        = \frac{1}{\alpha} \bm{\phi}(\mathbf{x}_i)^T \bm{\phi}(\mathbf{x}_{N + 1})
    \]
    then $\mathbf{k} = \alpha^{-1} \bm{\Phi}\bm{\phi}(\mathbf{x}_{N + 1})$, where
    $\bm{\Phi}$ is the design matrix. Now, the elements of 
    $\mathbf{C}_N$ can be written as
    \begin{align*}
        {\mathbf{C}_N}_{nm} 
        = \mathbf{C}_N(\mathbf{x}_N, \mathbf{x}_M)
        = k(\mathbf{x}_N, \mathbf{x}_M) + \beta^{-1} I_{nm}
        = K_{nm} + \beta^{-1} I_{nm}
        = \frac{1}{\alpha} \big(\bm{\Phi}\bm{\Phi}^T\big)_{nm} + \frac{1}{\beta} I_{nm}
    \end{align*}
    where $\mathbf{K}$ is the Gram matrix given by (6.54), so 
    \[
        \mathbf{C}_N = \frac{1}{\alpha} \bm{\Phi}\bm{\Phi}^T + \frac{1}{\beta} \mathbf{I}
    \] 
    The mean of the predictive distribution in the Gaussian process model is given by 
    \eqref{eq:6.66} and can be rewritten as
    \begin{align*}
        m(\mathbf{x}_{N + 1}) 
        &= \mathbf{k}^T \mathbf{C}_N^{-1} \mathbf{t} \\
        &= \frac{1}{\alpha}\bm{\phi}^T(\mathbf{x}_{N + 1})^T \bm{\Phi}
        \bigg(\frac{1}{\alpha} \bm{\Phi}\bm{\Phi}^T 
            + \frac{1}{\beta} \mathbf{I}\bigg)^{-1}\mathbf{t} \\
        &= \bm{\phi}(\mathbf{x}_{N + 1})^T \bm{\Phi}^T
        \bigg(\bm{\Phi}\bm{\Phi}^T + \frac{\alpha}{\beta} \mathbf{I}\bigg)^{-1}\mathbf{t} \\
        &= \beta \bm{\phi}(\mathbf{x}_{N + 1})^T \bm{\Phi}^T
        \big(\beta \bm{\Phi}\bm{\Phi}^T + \alpha \mathbf{I}\big)^{-1}\mathbf{t} \\
        &= \beta \bm{\phi}(\mathbf{x}_{N + 1})^T \bm{\Phi}^T
        \big(\beta \bm{\Phi}^T\bm{\Phi} + \alpha \mathbf{I}\big)^{-1}\mathbf{t} \\
        &= \beta\bm{\phi}(\mathbf{x}_{N + 1})^T \bm{\Phi}^T \mathbf{S}_N \mathbf{t}
    \end{align*}
    where $\mathbf{S}_N$ is given by \eqref{eq:3.54} and we've used (C.6) to obtain that
    \[
        \bm{\Phi}^T \big(\beta \bm{\Phi}\bm{\Phi}^T + \alpha \mathbf{I}\big)^{-1}
        = \bm{\Phi}^T \big(\beta \bm{\Phi}^T\bm{\Phi} + \alpha \mathbf{I}\big)^{-1}
    \] 
    Starting from \eqref{eq:6.67}, the variance can be obtained as
    \begin{align*}
        \sigma^2(\mathbf{x}_{N + 1})
        &= c - \mathbf{k}^T \mathbf{C}_N^{-1} \mathbf{k} \\
        &= \frac{1}{\beta} + k(\mathbf{x}_{N + 1}, \mathbf{x}_{N + 1}) 
        - \frac{\beta}{\alpha} \bm{\phi}(\mathbf{x}_{N + 1})^T
        \bm{\Phi}^T \mathbf{S}_N \bm{\Phi} \bm{\phi}(\mathbf{x}_{N + 1}) \\
        &= \frac{1}{\beta} 
        + \frac{1}{\alpha}\bm{\phi}(\mathbf{x}_{N + 1})^T \bm{\phi}(\mathbf{x}_{N + 1})
        - \frac{\beta}{\alpha} \bm{\phi}(\mathbf{x}_{N + 1})^T
        \bm{\Phi}^T \mathbf{S}_N \bm{\Phi} \bm{\phi}(\mathbf{x}_{N + 1}) \\
        &= \frac{1}{\beta} + \bm{\phi}(\mathbf{x}_{N + 1})^T \bigg[
            \frac{1}{\alpha} \mathbf{I} - \frac{\beta}{\alpha} \bm{\Phi}^T \mathbf{S}_N \bm{\Phi}
        \bigg] \bm{\phi}(\mathbf{x}_{N + 1}) \\
        &= \frac{1}{\beta} + \bm{\phi}(\mathbf{x}_{N + 1})^T \bigg[
            \frac{1}{\alpha} \mathbf{I} - \frac{\beta}{\alpha} \bm{\Phi}^T 
            \big(\alpha \mathbf{I} + \beta \bm{\Phi}^T\bm{\Phi}\big)^{-1} \bm{\Phi}
        \bigg] \bm{\phi}(\mathbf{x}_{N + 1}) \\
        &= \frac{1}{\beta} + \bm{\phi}(\mathbf{x}_{N + 1})^T \bigg[
            \frac{1}{\alpha} \mathbf{I} - \frac{\beta}{\alpha} \bm{\Phi}^T 
            \big(\alpha \mathbf{I} + \beta \bm{\Phi}^T\bm{\Phi}\big)^{-1} \bm{\Phi}
        \bigg] \bm{\phi}(\mathbf{x}_{N + 1}) \\
        &= \frac{1}{\beta} + \bm{\phi}(\mathbf{x}_{N + 1})^T \bigg[
            \frac{1}{\alpha} \mathbf{I} - \frac{1}{\alpha^2} \bm{\Phi}^T 
            \bigg(\frac{1}{\beta} \mathbf{I} + \frac{1}{\alpha} 
            \bm{\Phi}^T\bm{\Phi}\bigg)^{-1} \bm{\Phi}
        \bigg] \bm{\phi}(\mathbf{x}_{N + 1}) 
    \end{align*}
    Using the Woodbury identity (C.7) combined with \eqref{eq:3.54}, one could show that
    \begin{align*}
       \mathbf{S}_N
       &= \big(\alpha \mathbf{I} + \beta \bm{\Phi}^T \bm{\Phi}\big)^{-1} \\
       &= \frac{1}{\alpha} \mathbf{I} - \frac{1}{\alpha^2} \bm{\Phi}^T
       \bigg(\frac{1}{\beta} \mathbf{I} + \frac{1}{\alpha}\bm{\Phi}^T \bm{\Phi}\bigg)^{-1} 
       \bm{\Phi} 
    \end{align*}
    Hence, the variance is the same as in the result \eqref{eq:3.58}:
    \[
        \sigma^2(\mathbf{x}_{N + 1}) 
        = \frac{1}{\beta} + \bm{\phi}(\mathbf{x}_{N + 1})^T \mathbf{S}_N 
        \bm{\phi}(\mathbf{x}_{N + 1})
    \] 
    Therefore, since both our predictive distributions are Gaussian and
    have the same mean and variance, they are identical.
\end{proof}

\section*{Exercise 6.24 $\star$}
Show that a diagonal matrix $\mathbf{W}$ whose elements satisfy
$0 < W_{ii} < 1$ is positive definite. Show that the sum of two
positive definite matrices is itself positive definite.

\vspace{1em}

\begin{proof}
    Since $\mathbf{W}$ is a $N \cross N$ diagonal matrix with $0 < W_{ii} < 1$,
    \[
        \mathbf{a}^T\mathbf{W}\mathbf{a} = \sum_{i=1}^{N} a_i^2 W_{ii} > 0, 
        \forall \mathbf{a} \in \mathbb{R}^N
    \] 
    Therefore, $\mathbf{W}$ is a positive definite matrix. Let 
    $\mathbf{C}_1, \mathbf{C}_2$ be $N \cross N$ positive definite matrices and 
    $\mathbf{A} = \mathbf{C}_1 + \mathbf{C}_2$.
    Then,
    \[
        \mathbf{a}^T\mathbf{A}\mathbf{a} 
        = \mathbf{a}^T\big(\mathbf{C}_1 + \mathbf{C}_2\big)\mathbf{a}
        = \mathbf{a}^T\mathbf{C}_1\mathbf{a} + \mathbf{a}^T\mathbf{C}_2\mathbf{a} > 0,
        \forall \mathbf{a} \in \mathbb{R}^N
    \] 
    As a result, we showed that the sum of two positive definite matrices is also
    a positive definite matrix.
\end{proof}
