\chapter{Linear Models for Classification}

\section*{Exercise 4.1 $\star \star$}
Given a set of data points $\{\mathbf{x}_n\}$, we can define the
$\emph{convex hull}$ to be the data set of all points 
$\mathbf{x}$ given by
\begin{equation}\label{eq:4.156}\tag{4.156}
    \mathbf{x} = \sum_{n} \alpha_n \mathbf{x}_n
\end{equation}
where $\alpha_n \geq 0$ and $\sum_{n} \alpha_n = 1$. Consider
a second set of points $\{\mathbf{y}_n\}$ together with
their corresponding convex hull. By definition,
the two set of points will be linearly separable
if there exists a vector $\widehat{\mathbf{w}}$ and
a scalar $w_0$ such that $\widehat{\mathbf{w}}^T\mathbf{x}_n + w_0 > 0$
for all $\mathbf{x}_n$, and $\widehat{\mathbf{w}}^T\mathbf{y}_n + w_0 < 0$
for all $\mathbf{y}_n$. Show that if their convex hulls intersect,
the two sets of points cannot be linearly separable, and conversely
that if they are linearly separable, their covex hulls do not intersect.

\vspace{1em}

\begin{proof}
    The vertices of the convex hulls are the data points $\{\mathbf{x}_n\}$ and $\{\mathbf{y}_n$\}. 
    Therefore, the edges of the hulls will be represented by some segments beteween the data points.
    As a result, any point situated on the boundary of the hull can be written as a convex
    combination of the end-points of the segment it's contained by. Also,
    one can easily see that if two hulls intersect, they intersect in at least one 
    point that is contained by the boundaries of both hulls. 

    \vspace{1em}
    \textbf{1st Hypothesis}: If the hulls intersect, the two sets of 
    points are not linearly separable
    \vspace{1em}

    Assume that the two hulls intersect in the point $\mathbf{z}$ situated
    on both hulls boundaries. From what we've seen above, 
    the point $\mathbf{z}$ can be expressed as a convex
    combination between two data points of each set of data points. Therefore,
    there exist 
    $\mathbf{x}_A, \mathbf{x}_B$ from $\{\mathbf{x}_n\}, \mathbf{y}_A, \mathbf{y}_B$
    from $\{\mathbf{y}_n\}$ and $\lambda_\mathbf{x}, \lambda_\mathbf{y} \in [0, 1]$ such that
    we can express $\mathbf{z}$ as
     \[
         \lambda_\mathbf{x} \mathbf{x}_A + (1 - \lambda_\mathbf{x}) \mathbf{x}_B = 
         \lambda_\mathbf{y} \mathbf{y}_A + (1 - \lambda_\mathbf{y}) \mathbf{y}_B
    \] 
    Suppose that the sets \{$\mathbf{x}_n$\} and $\{\mathbf{y}_n\}$ are linearly 
    separable. Then there exists a discriminant function
    \[
        \theta(\mathbf{a}) = \widehat{\mathbf{w}}^T \mathbf{a} + w_0
    \]
    such that $\theta(\mathbf{x}_n) > 0$ for all $\mathbf{x}_n$ and $\theta(\mathbf{y}_n) < 0$
    for all $\mathbf{y}_n$. From the linearity of the discriminant function, 
    and rewritting $\theta(\mathbf{z})$ using the convex combinations forms,
    we have that
    \[
        \lambda_\mathbf{x} \theta(\mathbf{x}_A) + (1 - \lambda_\mathbf{x}) \theta(\mathbf{x}_B)
        = \lambda_\mathbf{y} \theta(\mathbf{y}_A) + (1 - \lambda_\mathbf{y}) \theta(\mathbf{y}_B)
    \] 
    Since $\theta(\mathbf{x}_A), \theta(\mathbf{x}_B) > 0$ and 
    $\theta(\mathbf{y}_A), \theta(\mathbf{y}_B) < 0$, this
    expression is obviously false, since the left-hand side of the equality
    is positive and the right-hand one is negative. Therefore, our supposition
    that the data sets are linearly separable is false and our main hypothesis is true.

    \vspace{1em}
    \textbf{2nd Hypothesis}: If the two sets of points are linearly separable,
    then the hulls don't intersect. 
    \vspace{0.25em}

    This hypothesis is the counterpositive of the 1st hypothesis. Therefore,
    it's valid too.
\end{proof}

\section*{Exercise 4.2 $\start \star$}
Consider the minimization of a sum-of-squares error
function ($\ref{eq:4.15}$), and suppose that all of the 
target vectors in the training set satisfy a linear constraint
\begin{equation}\label{eq:4.157}\tag{4.157}
    \mathbf{a}^T\mathbf{t}_n + b = 0
\end{equation}
where $\mathbf{t}_n$ corresponds to the $n^\text{th}$ row of the matrix $\mathbf{T}$
in $(\ref{eq:4.15})$. Show that as a consequence of this constraint,
the elements of the model prediction $\mathbf{y}(\mathbf{x})$ given
by the least-squares solution ($\ref{eq:4.17}$) also satisfy
this constraint, so that
\begin{equation}\label{eq:4.158}\tag{4.158}
    \mathbf{a}^T\mathbf{y}(\mathbf{x}) + b = 0
\end{equation}
To do so, assume that one of the basis functions $\phi_0(\mathbf{x}) = 1$,
so that the corresponding parameter $w_0$ plays the role of a bias.

\section*{Exercise 4.4 $\star$}
Show that maximization of the class separation criterion given by
($\ref{eq:3.24}$) with respect to $\mathbf{w}$, using a Lagrange multiplier
to enforce the constraint $\mathbf{w}^T\mathbf{w} = 1$, leads to the
result that $\mathbf{w} \propto (\mathbf{m}_2 - \mathbf{m}_1)$.

\vspace{1em}

\begin{proof}
    Our goal is to maximize
    \begin{equation}\label{eq:3.24}\tag{3.24}
        m_2 - m_1 = \mathbf{w}^T(\mathbf{m}_2 - \mathbf{m}_1)
    \end{equation}
    with the constraint that $\mathbf{w}^T\mathbf{w} = 1$.
    The corresponding Lagrangian is given by
    \[
        \mathcal{L}(\mathbf{w}, \lambda) 
        = \mathbf{w}^T(\mathbf{m}_2 - \mathbf{m}_1) + \lambda(\mathbf{w}^T\mathbf{w} - 1)
    \] 
    By taking the gradient of this with respect to $\mathbf{w}$ and $\lambda$,
    we have that
    \[
        \nabla_{\mathbf{w}, \lambda} \mathcal{L}(\mathbf{w}, \lambda) 
        = \begin{pmatrix}
            \mathbf{m}_2 - \mathbf{m}_1 + 2\lambda\mathbf{w} \\
            \mathbf{w}^T\mathbf{w} - 1
        \end{pmatrix}
    \] 
    Setting to 0 the derivative with respect to $\mathbf{w}$ gives the initial result, that is
    \[
        \mathbf{w} = \frac{\mathbf{m}_1 - \mathbf{m}_2}{2\lambda} 
    \] 
    By replacing into the $\lambda$ derivative and setting it to 0, we'd obtain that
     \[
         \lambda = \frac{1}{4} ||\mathbf{m}_1 - \mathbf{m}_2||^2
    \] 
    which gives
    \[
        \mathbf{w} = \frac{2(\mathbf{m}_1 - \mathbf{m}_2)}{||\mathbf{m}_1 - \mathbf{m}_2||^2}
        \propto (\mathbf{m}_2 - \mathbf{m}_1)
    \] 
\end{proof}

\section*{Exercise 4.5 $\star$}
By making use of (4.20), ($\ref{eq:4.23}$), and (4.24), show that the Fischer 
criterion ($\ref{eq:4.25}$) can be written in the form ($\ref{eq:4.26}$).

\vspace{1em}

\begin{proof}
    The Fisher criterion is defined to be the reatio of the between-class
    variance to the within-class variance and is given by
    \begin{equation}\label{eq:4.25}\tag{4.25}
        J(\mathbf{w}) = \frac{(m_2 - m_1)^2}{s_1^2 + s_2^2}
    \end{equation}
    where 
    \begin{equation}\label{eq:4.23}\tag{4.23}
        m_k = \mathbf{w}^T\mathbf{m}_k
    \end{equation}
    and
    \begin{equation}\label{eq:4.24}\tag{4.24}
        s_k^2 = \sum_{n \in \mathcal{C}_k} (y_n - m_k)^2
    \end{equation}
    By substituting $(\ref{eq:4.23})$ into the numerator of the Fischer expression,
    \begin{align*}
        (m_2 - m_1)^2 
        = (\mathbf{w}^T\mathbf{m}_2 - \mathbf{w}^T\mathbf{m}_1)^2
        = \mathbf{w}^T(\mathbf{m}_2 - \mathbf{m}_1)\mathbf{w}^T(\mathbf{m}_2 - \mathbf{m}_1)
        &= \mathbf{w}^T(\mathbf{m}_2 - \mathbf{m}_1)(\mathbf{m}_2 - \mathbf{m}_1)^T\mathbf{w} \\
        &= \mathbf{w}^T\mathbf{S}_B\mathbf{w}
    \end{align*}
    where $\mathbf{S}_B$ is the $\emph{between-class}$ covariance matrix and is given by
    \begin{equation}\label{eq:4.27}\tag{4.27}
        \mathbf{S}_B = (\mathbf{m}_2 - \mathbf{m}_1)(\mathbf{m}_2 - \mathbf{m}_1)^T
    \end{equation}
    Similarily, we use the fact that the projection of the D-dimensional input vector
    $\mathbf{w}$ to one dimension is given by
    \begin{equation}\label{eq:4.20}\tag{4.20}
        y = \mathbf{w}^T\mathbf{x}
    \end{equation}
    along with $(\ref{eq:4.23})$ and $(\ref{eq:4.24})$ to rewrite the denominator as 
    \begin{align*}
        s_1^2 + s_2^2
        &= \sum_{n \in \mathcal{C}_1} (y_n - m_1)^2 + \sum_{n \in \mathcal{C}_2} (y_n - m_2)^2 \\
        &= \sum_{n \in \mathcal{C}_1} (\mathbf{w}^T\mathbf{x}_n - \mathbf{w}^T\mathbf{m}_1)^2 
            + \sum_{n \in \mathcal{C}_2} (\mathbf{w}^T\mathbf{x}_n - \mathbf{w}^T\mathbf{m}_2)^2 \\
        &= \sum_{n \in \mathcal{C}_1} \mathbf{w}^T(\mathbf{x}_n - \mathbf{m}_1)
            (\mathbf{x}_n - \mathbf{m}_1)^T\mathbf{w}
            + \sum_{n \in \mathcal{C}_2} \mathbf{w}^T(\mathbf{x}_n - \mathbf{m}_2)
            (\mathbf{x}_n - \mathbf{m}_2)^T\mathbf{w} \\
        &= \mathbf{w}^T \bigg[\sum_{n \in \mathcal{C}_1} 
            (\mathbf{x}_n - \mathbf{m}_1)(\mathbf{x}_n - \mathbf{m}_1)^T
            + \sum_{n \in \mathcal{C}_2}  
            (\mathbf{x}_n - \mathbf{m}_2)(\mathbf{x}_n - \mathbf{m}_2)^T
            \bigg]\mathbf{w} \\
        &= \mathbf{w}^T\mathbf{S}_W\mathbf{w}
    \end{align*}
    where $\mathbf{S}_W$ is the $\emph{within-class}$ covariance matrix and is given by
    \begin{equation}\label{eq:4.28}\tag{4.28}
        \mathbf{S}_W = 
           \sum_{n \in \mathcal{C}_1} 
            (\mathbf{x}_n - \mathbf{m}_1)(\mathbf{x}_n - \mathbf{m}_1)^T
            + \sum_{n \in \mathcal{C}_2}  
            (\mathbf{x}_n - \mathbf{m}_2)(\mathbf{x}_n - \mathbf{m}_2)^T
    \end{equation}
    Finally, by substituting the new expressions into ($\ref{eq:4.25}$), we can rewrite
    the Fischer criterion as
    \begin{equation}\label{eq:4.26}\tag{4.26}
        J(\mathbf{w}) = \frac{\mathbf{w}^T\mathbf{S}_B\mathbf{w}}{\mathbf{w}^T\mathbf{S}_W\mathbf{w}}
    \end{equation}
\end{proof}

\section*{Exercise 4.7 $\star$}
Show that the logistic sigmoid function $(\ref{eq:4.59})$ satisfies the property
$\sigma(-a) = 1 - \sigma(a)$ and that its inverse is given by 
$\sigma^{-1}(y) = \ln\bigg(\displaystyle \frac{y}{1 - y}\bigg)$. 

\vspace{1em}

\begin{proof}
    The sigmoid function is given by
    \begin{equation}\label{eq:4.59}\tag{4.59}
        \sigma(a) = \frac{1}{1 + e^{-a}}
    \end{equation}
    The symmetry property is easily satisfied, as
    \begin{equation}\label{eq:4.60}\tag{4.60}
        \sigma(-a) 
        = \frac{1}{1 + e^a} 
        = \frac{1 + e^a + 1 + e^{-a}}{(1 + e^{-a})(1 + e^a)}
        - \frac{1 + e^a}{(1 + e^{-a})(1 + e^a)}
        = 1 - \frac{1}{1 + e^{-a}} 
        = 1 - \sigma(a)
    \end{equation}\label{eq:4.60}
    The sigmoid function is bijective, so inversable. Therefore, let $\sigma(x) = y$.
    Then,
    \[
        y = \frac{1}{1 + e^{-x}} 
        \iff (1 + e^{-x})y = 1 
        \iff e^{-x} &= \frac{1 - y}{y} 
        \iff x = \ln\bigg(\frac{y}{1-y}\bigg)
    \] 
    so the inverse of the sigmoid function is given by
    \[
        \sigma^{-1}(y) = \ln\bigg(\frac{y}{1 - y}\bigg)
    \] 
\end{proof}

\section*{Exercise 4.8 $\star$}
Using ($\ref{eq:4.57}$) and $(\ref{eq:4.58})$, derive the result $(\ref{eq:4.65})$ 
for the posterior class probability in the two-class generative model with Gaussian
densities, and verify the results $(\ref{eq:4.66})$ and $(\ref{eq:4.67})$ for the
parameters $\mathbf{w}$ and $w_0$.

\vspace{1em}

\begin{proof}
    It is known that the posterior probability for class $\mathcal{C}_1$ can
    be written as
    \begin{equation}\label{eq:4.57}\tag{4.57}
        p(\mathcal{C}_1 | \mathbf{x}) = \sigma(a)
    \end{equation}
    where we have defined 
    \begin{equation}\label{eq:4.58}\tag{4.58}
        a = \ln \frac{p(\mathbf{x} | \mathcal{C}_1)p(\mathcal{C}_1)}
            {p(\mathbf{x} | \mathcal{C}_2)p(\mathcal{C}_2)}
    \end{equation}
    and $\sigma$ is the logistic sigmoid function defined by $(\ref{eq:4.59})$.
    We start by expanding $a$ and rewritting it as
    \[
        a = \ln p(\mathbf{x} | \mathcal{C}_1) - \ln p(\mathbf{x} | \mathcal{C}_2)
            + \ln \frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}
    \] 
    Since the class-conditional densities are Gaussian, i.e. the density
    for a class $\mathcal{C}_k$ is given by
    \begin{equation}\label{eq:4.64}\tag{4.64}
        p(\mathbf{x} | \mathcal{C}_k) = \mathcal{N}(\mathbf{x} | \bm{\mu}_k, \mathbf{\Sigma})
        = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\mathbf{\Sigma}|^{1/2}}
        \exp \bigg\{-\frac{1}{2} (\mathbf{x} - \bm{\mu}_k)^T\mathbf{\Sigma}^{-1}
        (\mathbf{x} - \bm{\mu}_k)\bigg\}
    \end{equation}
    one can easily obtain that
    \begin{align*}
        a 
        &= \ln p(\mathbf{x} | \mathcal{C}_1) - \ln p(\mathbf{x} | \mathcal{C}_2) 
        + \ln \frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\\
        &= \bm{\mu}_1^T\mathbf{\Sigma}^{-1}\mathbf{x}
        - \bm{\mu}_2^T\mathbf{\Sigma}^{-1}\mathbf{x} 
        - \frac{1}{2} \bm{\mu}_1^T\mathbf{\Sigma}^{-1}\bm{\mu}_1
        + \frac{1}{2} \bm{\mu}_2^T\mathbf{\Sigma}^{-1}\bm{\mu}_2
        + \ln \frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\\
        &= (\bm{\mu}_1 - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1} \mathbf{x}
        - \frac{1}{2} \bm{\mu}_1^T\mathbf{\Sigma}^{-1}\bm{\mu}_1
        + \frac{1}{2} \bm{\mu}_2^T\mathbf{\Sigma}^{-1}\bm{\mu}_2
        + \ln \frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\\
        &= \mathbf{w}^T\mathbf{x} + w_0
    \end{align*}
    where we have defined 
    \begin{equation}\label{eq:4.66}\tag{4.66}
        \mathbf{w} = \mathbf{\Sigma}^{-1}(\bm{\mu}_1 - \bm{\mu}_2)
    \end{equation}
    \vspace{-1em}
    \begin{equation}\label{eq:4.67}\tag{4.67}
        w_0 = 
        - \frac{1}{2} \bm{\mu}_1^T\mathbf{\Sigma}^{-1}\bm{\mu}_1
        + \frac{1}{2} \bm{\mu}_2^T\mathbf{\Sigma}^{-1}\bm{\mu}_2
        + \ln \frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}
    \end{equation}
    Therefore, the posterior probability for class $\mathcal{C}_1$ 
    is given by
    \begin{equation}\label{eq:4.65}\tag{4.65}
        p(\mathcal{C}_1 | \mathbf{x}) = \sigma(\mathbf{w}^T\mathbf{x} + w_0)
    \end{equation}
\end{proof}

\section*{Exercise 4.9 $\star$}
Consider a generative classification model for $K$ classes
defined by prior class probabilities $p(\mathcal{C}_k) = \pi_k$
and general class-conditional densities  $p(\bm{\phi} | \mathcal{C}_k)$
where $\mathbf{\phi}$ is the input feature vector. Suppose
we are given a training set $\{\bm{\phi}_n, \mathbf{t}_n\}$ where
$n = 1,\ldots, N$, and $\mathbf{t}_n$ is a binary target vector
of length $K$ that use the 1-of-$K$ coding scheme, so
that it has components $t_{nj} = \mathbf{I}_{jk}$ if pattern
$n$ is from class $\mathcal{C}_k$. Assuming that the data points
are drawn independently from this model, show that the 
maximum-likelihood solution for the prior probabilites is given
by 
\begin{equation}\label{eq:4.159}\tag{4.159}
    \pi_k = \frac{N_k}{N}
\end{equation}
where $N_k$ is the number of data points assigned to class 
$\mathcal{C}_k$.

\vspace{1em}

\begin{proof}
    Let $\mathbf{T}$ be the $N \times K$ matrix with the rows $\mathbf{t}_n^T$ and
    $\mathbf{\Phi}$ the $N \times M$ matrix with the rows $\bm{\phi}_n^T$.
    Also, let's define the column vector $\bm{\pi} = (\pi_1, \pi_2, \ldots, \pi_K)^T$.
    We have that
    \[
        p(\bm{\phi}_n, \mathcal{C}_k)
        = p(\mathcal{C}_k)p(\bm{\phi}_n | \mathcal{C}_k)
        = \pi_k p(\bm{\phi}_n | \mathcal{C}_k)
    \] 
    so the likelihood function is given by
    \[
        p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        = \prod_{n = 1}^N p(\mathbf{t}_n | \mathbf{\Phi}, \bm{\pi})
        = \prod_{n = 1}^N \prod_{j = 1}^K
        \bigg[\pi_j p(\bm{\phi}_n | \mathcal{C}_j)\bigg]^{t_{nj}}
    \] 
    The log likelihood is then easily derived as
    \[
        \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        = \sum_{n=1}^{N} \sum_{j=1}^{K} \bigg(t_{nj} \ln \pi_j
        + t_{nj} \ln p(\bm{\phi}_n | \mathcal{C}_j)\bigg)
    \] 
    We aim to minimize this with respect to $\pi_k$, while
    still maintaining the constraint $\sum_{k=1}^{N} \pi_k = 1$
    Therefore, by only keeping the terms depending on $\pi_k$, we obtain the Lagrangian
    \[
        \mathcal{L}(\bm{\pi}, \lambda)
        = \sum_{n=1}^{N} \sum_{j=1}^{K} t_{nj} \ln \pi_j
        + \lambda \bigg(\sum_{j=1}^{N} \pi_j - 1\bigg)
    \]
    with the gradient
    \[
        \nabla_{\pi_k, \lambda} \mathcal{L}(\bm{\pi}, \lambda)
        = \begin{pmatrix}
            \displaystyle \frac{1}{\pi_k} \sum_{n=1}^{N} t_{nk} + \lambda \\
            \\
            \displaystyle \sum_{k=1}^{N} \pi_k - 1
        \end{pmatrix}
    \] 
    By setting this gradient to $0$, from the first relation we have that
    \[
        \pi_k\lambda = -\sum_{n=1}^{N} t_{nk} = -N_k
    \] 
    Summing this over $k$, one can see that
     \[
         \lambda = -N
    \] 
    After substituting this into the derivative and then setting it to 0, 
    we obtain the maximum-likelihood solution for $\pi_k$, that is
    \[
        \pi_k = \frac{N_k}{N}
    \] 
\end{proof}

\section*{Exercise 4.10 $\star \star$}
Consider the classification model of Exercise 4.9 and now suppose
that the class-conditional densities are given by Gaussian 
distributions with a shared covariance matrix, so that
\begin{equation}\label{eq:4.160}\tag{4.160}
    p(\bm{\phi} | \mathcal{C}_k) = \mathcal{N}(\bm{\phi} | \bm{\mu}_k, \mathbf{\Sigma})
\end{equation}
Show that the maximum likelihood solution for the mean of the
Gaussian distribution for class $\mathcal{C}_k$ is given by
\begin{equation}\label{eq:4.161}\tag{4.161}
    \bm{\mu}_k = \frac{1}{N_k} \sum_{n=1}^{N} t_{nk} \bm{\phi}_n
\end{equation}
which represents the mean of those feature vectors assigned
to class $\mathcal{C}_k$. Similarly, show that the maximum likelihood
solution for the shared covariance matrix is given by
\begin{equation}\label{eq:4.162}\tag{4.162}
    \mathbf{\Sigma} = \sum_{k=1}^{K} \frac{N_k}{N}\mathbf{S}_k
\end{equation}
where 
\begin{equation}\label{eq:4.163}\tag{4.163}
\mathbf{S}_k = \frac{1}{N_k} \sum_{n=1}^{N} 
t_{nk}(\bm{\phi}_n - \bm{\mu_k})(\bm{\phi}_{k} - \bm{\mu}_k)^T
\end{equation}
Thus $\mathbf{\Sigma}$ is given by a weighted average of the covariances
of the data associated with each class, in which the weighting coefficients
are given by the prior probabilities of the classes.

\vspace{1em}

\begin{proof}
    Using the same notations as in the last exercise, we remember that the
    log likelihood is given by
    \[
        \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        = \sum_{n=1}^{N} \sum_{j=1}^{K} \bigg(t_{nj} \ln \pi_j
        + t_{nj} \ln p(\bm{\phi}_n | \mathcal{C}_j)\bigg)
    \] 
    By keeping only the parts depending on $\bm{\mu}_k$,
    \[
        \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        = -\frac{1}{2}\sum_{n=1}^{N} \sum_{j=1}^{K} 
        t_{nj} (\bm{\phi}_n - \bm{\mu}_k)^T\mathbf{\Sigma}^{-1}
            (\bm{\phi}_n - \bm{\mu}_k) + \text{const}
    \] 
    For a symmetric matrix $\mathbf{W}$, one could show that
    \[
        \pdv{s}(\mathbf{x} - \mathbf{s})^T\mathbf{W}(\mathbf{x} - \mathbf{s}) 
        = -2 \mathbf{W}(\mathbf{x} - \mathbf{s})
    \] 
    Therefore, the derivative with respect to $\mathbf{\mu}_k$ of the 
    log-likelihood is given by
    \[
        \pdv{\bm{\mu}_k}
        \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        = \sum_{n=1}^{N} t_{nk} \mathbf{\Sigma}^{-1}(\bm{\phi}_n - \bm{\mu}_k)
    \] 
    Since $\sum_{n=1}^{N} t_{nk} = N_k$, 
    by setting the derivative to 0 and rearranging the terms, we have that
    the solution for maximum likelihood is
    \[
        \bm{\mu}_k = \frac{1}{N_k} \sum_{n=1}^{N} t_{nk} \bm{\phi}_n
        \tag{4.161}
    \] 
    Similarly, we do the same for the shared covariance matrix.
    By keeping only the terms depending on $\mathbf{\Sigma}$, the
    log likelihood is given by
    \begin{align*}
        \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        &= -\frac{N}{2} \ln|\mathbf{\Sigma}| - \frac{1}{2}\sum_{n=1}^{N} \sum_{k=1}^{K} 
            t_{nk} (\bm{\phi}_n - \bm{\mu}_k)^T\mathbf{\Sigma}^{-1}
            (\bm{\phi}_n - \bm{\mu}_k) + \text{const} \\
        &= -\frac{N}{2} \ln|\mathbf{\Sigma}| - \frac{1}{2}\sum_{n=1}^{N} \sum_{k=1}^{K} 
            t_{nk} \big(\bm{\phi}_n^T\mathbf{\Sigma}^{-1}\bm{\phi}_n 
            - 2\bm{\phi}_n^T\mathbf{\Sigma}^{-1}\bm{\mu}_k 
            + \bm{\mu}_k^T\mathbf{\Sigma}^{-1}\bm{\mu}_k\big)
    \end{align*}
    By using (C.28) and 
    \[
    \pdv{\mathbf{X}} \mathbf{a}^T\mathbf{X}^{-1}\mathbf{b}
    = -\mathbf{X}^{-T} \mathbf{ab}^T \mathbf{X}^{-T}
    \] 
    we take the derivative of the log likelihood with respect to $\mathbf{\Sigma}$ 
    and obtain:
    \begin{align*}
        \pdv{\mathbf{\Sigma}} \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        &= -\frac{N}{2} \mathbf{\Sigma}^{-1}
        + \frac{1}{2} \sum_{n=1}^{N} \sum_{j=k}^{K} t_{nk} \mathbf{\Sigma}^{-1}
        \big(\bm{\phi}_n\bm{\phi}_n^T - 2\bm{\phi}_n\bm{\mu}_k^T + \bm{\mu}_k\bm{\mu}_k^T\big) 
        \mathbf{\Sigma}^{-1} \\
        &= -\frac{N}{2} \mathbf{\Sigma}^{-1} 
        + \frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \mathbf{\Sigma}^{-1}
        (\bm{\phi}_n - \bm{\mu}_k)(\bm{\phi}_n - \bm{\mu}_k)^T \mathbf{\Sigma}^{-1} \\
        &= -\frac{N}{2} \mathbf{\Sigma}^{-1} 
        + \frac{1}{2} \mathbf{\Sigma}^{-1} 
        \bigg(\sum_{k=1}^{K} N_k\mathbf{S}_k\bigg) \mathbf{\Sigma}^{-1}
    \end{align*}
    where $\mathbf{S}_k$ is defined by $(\ref{eq:4.163})$. Therefore, by setting
    this derivative to 0 and rearranging the terms,
    we obtain the maximum-likelihood solution for the
    shared covariance matrix
    \[
        \mathbf{\Sigma} = \sum_{k=1}^{K} \frac{N_k}{N} \mathbf{S}_k \tag{4.162}
    \] 
\end{proof}

\section*{Exercise 4.11 $\star \star$}
Consider a classification problem with $K$ classses for which the feature
vector $\mathbf{\phi}$ has $M$ components each of which can take $L$ discrete
states. Let the values of the components be represented by a 1-of-$L$ binary coding scheme.
Further suppose that, conditioned on the class $\mathcal{C}_k$, the $M$ components
of $\bm{\phi}$ are independent, so that the class-conditional density factorizes
with respect to the feature vector components. Show that the quantities given by
$(\ref{eq:4.63})$, which appear in the argument to the softmax function
describing the posterior class probabilties, are linear functions of the components
of $\bm{\phi}$. Note that this represents an example of the naive Bayes model
which is discussed in Section 8.2.2.

\vspace{1em}

\begin{proof}
    We've seen in Section 4.2 that the posterior probabilities
    can be written as $\emph{normalized}$ $\emph{exponentials}$:
    \begin{equation}\label{eq:4.62}\tag{4.62}
        p(\mathcal{C}_k | \bm{\phi}) 
        = \frac{\exp(a_k)}{\sum_{j} \exp(a_j)}
    \end{equation}
    where
    \begin{equation}\label{eq:4.63}\tag{4.63}
        a_k = \ln p(\bm{\phi} | \mathcal{C}_k)p(\mathcal{C}_k)
    \end{equation}
    Considering the setup of our classification problem,
    our class-conditional distribution will be of the form
    \[
        p(\bm{\phi} | \mathcal{C}_k) 
        = \prod_{i = 1}^M \prod_{j = 1}^L
            \bm{\mu}_{kij}^{\bm{\phi}_{ij}}
    \] 
    where $\bm{\mu}_k$ is given by $(\ref{eq:4.161})$. 
    Therefore,
    by replacing into $(\ref{eq:4.63})$, the arguments
    of the softmax function are given by
    \[
        a_k = \ln p(\mathcal{C}_k) 
        + \sum_{i=1}^{M} \sum_{j=1}^{L} \bm{\phi}_{ij} \ln \bm{\mu}_{kij} 
    \] 
    and are obviously linear functions of the components
    of $\bm{\phi}$.
\end{proof}

\section*{Exercise 4.12 $\star$}
Verify the relation ($\ref{eq:4.88}$) for the derivative of the logistic sigmoid
function defined by ($\ref{eq:4.59}$).

\vspace{1em}

\begin{proof}
    By taking the derivative of $(\ref{eq:4.59})$, we have that: 
    \[
        \pdv{a} \sigma(a) 
        = \pdv{a}\bigg(\frac{1}{1 + e^{-a}}\bigg)
        = \frac{e^{-a}}{(1 + e^{-a})^2}
        = \frac{1 + e^{-a}}{(1 + e^{-a})^2} - \frac{1}{(1 + e^{-a})^2} 
        = \frac{1}{1 + e^{-a}} + \bigg(\frac{1}{1 + e^{-a}}\bigg)^2
    \] 
    We recognize the expression of the logistic sigmoid function, so
    \begin{equation}\label{eq:4.88}\tag{4.88}
        \pdv{a} \sigma(a) = \sigma(a) + \sigma(a)^2 = \sigma(a) \big(1 + \sigma(a)\big)
    \end{equation}
\end{proof}

\section*{Exercise 4.13 $\star$}
By making use of the result ($\ref{eq:4.88}$) for the derivative of the logistic
sigmoid, show that the derivative of the error function ($\ref{eq:4.90}$) for 
the logistic regression model is given by (\ref{eq:4.91}).

\vspace{1em}

\begin{proof}
    The error function for the logistic regression is given by
    \begin{equation}\label{eq:4.90}\tag{4.90}
        E(\mathbf{w}) 
        = -\ln p(\mathbf{t} | \mathbf{w})
        = -\sum_{n = 1}^{N} \{t_n \ln y_n + (1 - t_n) \ln(1 - y_n)\} 
    \end{equation}
    where $y_n = \sigma(a_n)$ and $a_n = \mathbf{w}^T\bm{\phi}_n$.
    Taking the derivative of the error function with respect to $\mathbf{w}$
    gives
    \begin{align*}
        \nabla_{\mathbf{w}} E(\mathbf{w})
        &= \nabla_{\mathbf{w}} \bigg(-\sum_{n=1}^{N} \big\{
            t_n \ln \sigma(\mathbf{w}^T\bm{\phi}_n) 
                + (1 - t_n) \ln \sigma(1 - \mathbf{w}^T\bm{\phi}_n)
            \big\}\bigg) \\
        &= -\sum_{n=1}^{N} \bigg[t_n \nabla_{\mathbf{w}} \ln \sigma(\mathbf{w}^T\bm{\phi}_n)
        + (1 - t_n) \nabla_{\mathbf{w}} \ln \{1 - \sigma(\mathbf{w}^T\bm{\phi}_n)\}\bigg]
    \end{align*}
    Using ($\ref{eq:4.88}$), we can compute the first gradient term :
    \begin{align*}
        \nabla_{\mathbf{w}} \ln \sigma(\mathbf{w}^T\bm{\phi}_n)
        = \frac{1}{\sigma(\mathbf{w}^T\bm{\phi}_n)} 
        \bigg(\nabla_{\mathbf{w}} \sigma(\mathbf{w}^T\bm{\phi}_n)\bigg)
        &= \frac{\sigma(\mathbf{w}^T\bm{\phi}_n)\big(1 + \sigma(\mathbf{w}^T\bm{\phi}_n)\big)}
        {\sigma(\mathbf{w}^T\bm{\phi}_n)} \bigg(\nabla_{\mathbf{w}} \mathbf{w}^T\bm{\phi}_n\bigg) \\
        &= \big(1 + \sigma(\mathbf{w}^T\bm{\phi}_n)\big) \bm{\phi}_n
        = (1 + y_n)\bm{\phi}_n
    \end{align*}
    Analogously, but this time using the fact that $1 - \sigma(a) = \sigma(-a)$,
    the second gradient term is given by
    \begin{align*}
        \nabla_{\mathbf{w}} \ln\{1 - \sigma(\mathbf{w}^T\bm{\phi}_n)\}
        = \nabla_{\mathbf{w}} \ln \sigma(-\mathbf{w}^T\bm{\phi}_n)
        = -\big(1 + \sigma(-\mathbf{w}^T\bm{\phi}_n)\big) \bm{\phi}_n
        = -\sigma(\mathbf{w}^T\bm{\phi}_n)\bm{\phi}_n 
        = -y_n \bm{\phi}_n
    \end{align*}
    By replacing back into the gradient of the error function,
    we obtain
    \begin{equation}\label{eq:4.91}\tag{4.91}
        \nabla_{\mathbf{w}} E(\mathbf{w})
        = -\sum_{n=1}^{N} \big\{t_n (1 + y_n) \bm{\phi}_n - (1 - t_n) y_n \bm{\phi}_n\big\}
        &= \sum_{n=1}^{N} (y_n - t_n) \bm{\phi}_n \tag{4.91}
    \end{equation}
\end{proof}

