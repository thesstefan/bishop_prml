\chapter{Linear Models for Classification}

\section*{Exercise 4.1 $\star \star$}
Given a set of data points $\{\mathbf{x}_n\}$, we can define the
$\emph{convex hull}$ to be the data set of all points 
$\mathbf{x}$ given by
\begin{equation}\label{eq:4.156}\tag{4.156}
    \mathbf{x} = \sum_{n} \alpha_n \mathbf{x}_n
\end{equation}
where $\alpha_n \geq 0$ and $\sum_{n} \alpha_n = 1$. Consider
a second set of points $\{\mathbf{y}_n\}$ together with
their corresponding convex hull. By definition,
the two set of points will be linearly separable
if there exists a vector $\widehat{\mathbf{w}}$ and
a scalar $w_0$ such that $\widehat{\mathbf{w}}^T\mathbf{x}_n + w_0 > 0$
for all $\mathbf{x}_n$, and $\widehat{\mathbf{w}}^T\mathbf{y}_n + w_0 < 0$
for all $\mathbf{y}_n$. Show that if their convex hulls intersect,
the two sets of points cannot be linearly separable, and conversely
that if they are linearly separable, their covex hulls do not intersect.

\vspace{1em}

\begin{proof}
    The vertices of the convex hulls are the data points $\{\mathbf{x}_n\}$ and $\{\mathbf{y}_n$\}. 
    Therefore, the edges of the hulls will be represented by some segments beteween the data points.
    As a result, any point situated on the boundary of the hull can be written as a convex
    combination of the end-points of the segment it's contained by. Also,
    one can easily see that if two hulls intersect, they intersect in at least one 
    point that is contained by the boundaries of both hulls. 

    \vspace{1em}
    \textbf{1st Hypothesis}: If the hulls intersect, the two sets of 
    points are not linearly separable
    \vspace{1em}

    Assume that the two hulls intersect in the point $\mathbf{z}$ situated
    on both hulls boundaries. From what we've seen above, 
    the point $\mathbf{z}$ can be expressed as a convex
    combination between two data points of each set of data points. Therefore,
    there exist 
    $\mathbf{x}_A, \mathbf{x}_B$ from $\{\mathbf{x}_n\}, \mathbf{y}_A, \mathbf{y}_B$
    from $\{\mathbf{y}_n\}$ and $\lambda_\mathbf{x}, \lambda_\mathbf{y} \in [0, 1]$ such that
    we can express $\mathbf{z}$ as
     \[
         \lambda_\mathbf{x} \mathbf{x}_A + (1 - \lambda_\mathbf{x}) \mathbf{x}_B = 
         \lambda_\mathbf{y} \mathbf{y}_A + (1 - \lambda_\mathbf{y}) \mathbf{y}_B
    \] 
    Suppose that the sets \{$\mathbf{x}_n$\} and $\{\mathbf{y}_n\}$ are linearly 
    separable. Then there exists a discriminant function
    \[
        \theta(\mathbf{a}) = \widehat{\mathbf{w}}^T \mathbf{a} + w_0
    \]
    such that $\theta(\mathbf{x}_n) > 0$ for all $\mathbf{x}_n$ and $\theta(\mathbf{y}_n) < 0$
    for all $\mathbf{y}_n$. From the linearity of the discriminant function, 
    and rewritting $\theta(\mathbf{z})$ using the convex combinations forms,
    we have that
    \[
        \lambda_\mathbf{x} \theta(\mathbf{x}_A) + (1 - \lambda_\mathbf{x}) \theta(\mathbf{x}_B)
        = \lambda_\mathbf{y} \theta(\mathbf{y}_A) + (1 - \lambda_\mathbf{y}) \theta(\mathbf{y}_B)
    \] 
    Since $\theta(\mathbf{x}_A), \theta(\mathbf{x}_B) > 0$ and 
    $\theta(\mathbf{y}_A), \theta(\mathbf{y}_B) < 0$, this
    expression is obviously false, since the left-hand side of the equality
    is positive and the right-hand one is negative. Therefore, our supposition
    that the data sets are linearly separable is false and our main hypothesis is true.

    \vspace{1em}
    \textbf{2nd Hypothesis}: If the two sets of points are linearly separable,
    then the hulls don't intersect. 
    \vspace{0.25em}

    This hypothesis is the counterpositive of the 1st hypothesis. Therefore,
    it's valid too.
\end{proof}

\section*{Exercise 4.2 $\start \star$}
Consider the minimization of a sum-of-squares error
function ($\ref{eq:4.15}$), and suppose that all of the 
target vectors in the training set satisfy a linear constraint
\begin{equation}\label{eq:4.157}\tag{4.157}
    \mathbf{a}^T\mathbf{t}_n + b = 0
\end{equation}
where $\mathbf{t}_n$ corresponds to the $n^\text{th}$ row of the matrix $\mathbf{T}$
in $(\ref{eq:4.15})$. Show that as a consequence of this constraint,
the elements of the model prediction $\mathbf{y}(\mathbf{x})$ given
by the least-squares solution ($\ref{eq:4.17}$) also satisfy
this constraint, so that
\begin{equation}\label{eq:4.158}\tag{4.158}
    \mathbf{a}^T\mathbf{y}(\mathbf{x}) + b = 0
\end{equation}
To do so, assume that one of the basis functions $\phi_0(\mathbf{x}) = 1$,
so that the corresponding parameter $w_0$ plays the role of a bias.

\section*{Exercise 4.4 $\star$}
Show that maximization of the class separation criterion given by
($\ref{eq:3.24}$) with respect to $\mathbf{w}$, using a Lagrange multiplier
to enforce the constraint $\mathbf{w}^T\mathbf{w} = 1$, leads to the
result that $\mathbf{w} \propto (\mathbf{m}_2 - \mathbf{m}_1)$.

\vspace{1em}

\begin{proof}
    Our goal is to maximize
    \begin{equation}\label{eq:3.24}\tag{3.24}
        m_2 - m_1 = \mathbf{w}^T(\mathbf{m}_2 - \mathbf{m}_1)
    \end{equation}
    with the constraint that $\mathbf{w}^T\mathbf{w} = 1$.
    The corresponding Lagrangian is given by
    \[
        \mathcal{L}(\mathbf{w}, \lambda) 
        = \mathbf{w}^T(\mathbf{m}_2 - \mathbf{m}_1) + \lambda(\mathbf{w}^T\mathbf{w} - 1)
    \] 
    By taking the gradient of this with respect to $\mathbf{w}$ and $\lambda$,
    we have that
    \[
        \nabla_{\mathbf{w}, \lambda} \mathcal{L}(\mathbf{w}, \lambda) 
        = \begin{pmatrix}
            \mathbf{m}_2 - \mathbf{m}_1 + 2\lambda\mathbf{w} \\
            \mathbf{w}^T\mathbf{w} - 1
        \end{pmatrix}
    \] 
    Setting to 0 the derivative with respect to $\mathbf{w}$ gives the initial result, that is
    \[
        \mathbf{w} = \frac{\mathbf{m}_1 - \mathbf{m}_2}{2\lambda} 
    \] 
    By replacing into the $\lambda$ derivative and setting it to 0, we'd obtain that
     \[
         \lambda = \frac{1}{4} ||\mathbf{m}_1 - \mathbf{m}_2||^2
    \] 
    which gives
    \[
        \mathbf{w} = \frac{2(\mathbf{m}_1 - \mathbf{m}_2)}{||\mathbf{m}_1 - \mathbf{m}_2||^2}
        \propto (\mathbf{m}_2 - \mathbf{m}_1)
    \] 
\end{proof}

\section*{Exercise 4.5 $\star$}
By making use of (4.20), ($\ref{eq:4.23}$), and (4.24), show that the Fischer 
criterion ($\ref{eq:4.25}$) can be written in the form ($\ref{eq:4.26}$).

\vspace{1em}

\begin{proof}
    The Fisher criterion is defined to be the ratio of the between-class
    variance to the within-class variance and is given by
    \begin{equation}\label{eq:4.25}\tag{4.25}
        J(\mathbf{w}) = \frac{(m_2 - m_1)^2}{s_1^2 + s_2^2}
    \end{equation}
    where 
    \begin{equation}\label{eq:4.23}\tag{4.23}
        m_k = \mathbf{w}^T\mathbf{m}_k
    \end{equation}
    and
    \begin{equation}\label{eq:4.24}\tag{4.24}
        s_k^2 = \sum_{n \in \mathcal{C}_k} (y_n - m_k)^2
    \end{equation}
    By substituting $(\ref{eq:4.23})$ into the numerator of the Fischer expression,
    \begin{align*}
        (m_2 - m_1)^2 
        = (\mathbf{w}^T\mathbf{m}_2 - \mathbf{w}^T\mathbf{m}_1)^2
        = \mathbf{w}^T(\mathbf{m}_2 - \mathbf{m}_1)\mathbf{w}^T(\mathbf{m}_2 - \mathbf{m}_1)
        &= \mathbf{w}^T(\mathbf{m}_2 - \mathbf{m}_1)(\mathbf{m}_2 - \mathbf{m}_1)^T\mathbf{w} \\
        &= \mathbf{w}^T\mathbf{S}_B\mathbf{w}
    \end{align*}
    where $\mathbf{S}_B$ is the $\emph{between-class}$ covariance matrix and is given by
    \begin{equation}\label{eq:4.27}\tag{4.27}
        \mathbf{S}_B = (\mathbf{m}_2 - \mathbf{m}_1)(\mathbf{m}_2 - \mathbf{m}_1)^T
    \end{equation}
    Similarily, we use the fact that the projection of the D-dimensional input vector
    $\mathbf{w}$ to one dimension is given by
    \begin{equation}\label{eq:4.20}\tag{4.20}
        y = \mathbf{w}^T\mathbf{x}
    \end{equation}
    along with $(\ref{eq:4.23})$ and $(\ref{eq:4.24})$ to rewrite the denominator as 
    \begin{align*}
        s_1^2 + s_2^2
        &= \sum_{n \in \mathcal{C}_1} (y_n - m_1)^2 + \sum_{n \in \mathcal{C}_2} (y_n - m_2)^2 \\
        &= \sum_{n \in \mathcal{C}_1} (\mathbf{w}^T\mathbf{x}_n - \mathbf{w}^T\mathbf{m}_1)^2 
            + \sum_{n \in \mathcal{C}_2} (\mathbf{w}^T\mathbf{x}_n - \mathbf{w}^T\mathbf{m}_2)^2 \\
        &= \sum_{n \in \mathcal{C}_1} \mathbf{w}^T(\mathbf{x}_n - \mathbf{m}_1)
            (\mathbf{x}_n - \mathbf{m}_1)^T\mathbf{w}
            + \sum_{n \in \mathcal{C}_2} \mathbf{w}^T(\mathbf{x}_n - \mathbf{m}_2)
            (\mathbf{x}_n - \mathbf{m}_2)^T\mathbf{w} \\
        &= \mathbf{w}^T \bigg[\sum_{n \in \mathcal{C}_1} 
            (\mathbf{x}_n - \mathbf{m}_1)(\mathbf{x}_n - \mathbf{m}_1)^T
            + \sum_{n \in \mathcal{C}_2}  
            (\mathbf{x}_n - \mathbf{m}_2)(\mathbf{x}_n - \mathbf{m}_2)^T
            \bigg]\mathbf{w} \\
        &= \mathbf{w}^T\mathbf{S}_W\mathbf{w}
    \end{align*}
    where $\mathbf{S}_W$ is the $\emph{within-class}$ covariance matrix and is given by
    \begin{equation}\label{eq:4.28}\tag{4.28}
        \mathbf{S}_W = 
           \sum_{n \in \mathcal{C}_1} 
            (\mathbf{x}_n - \mathbf{m}_1)(\mathbf{x}_n - \mathbf{m}_1)^T
            + \sum_{n \in \mathcal{C}_2}  
            (\mathbf{x}_n - \mathbf{m}_2)(\mathbf{x}_n - \mathbf{m}_2)^T
    \end{equation}
    Finally, by substituting the new expressions into ($\ref{eq:4.25}$), we can rewrite
    the Fischer criterion as
    \begin{equation}\label{eq:4.26}\tag{4.26}
        J(\mathbf{w}) = \frac{\mathbf{w}^T\mathbf{S}_B\mathbf{w}}{\mathbf{w}^T\mathbf{S}_W\mathbf{w}}
    \end{equation}
\end{proof}

\section*{Exercise 4.7 $\star$}
Show that the logistic sigmoid function $(\ref{eq:4.59})$ satisfies the property
$\sigma(-a) = 1 - \sigma(a)$ and that its inverse is given by 
$\sigma^{-1}(y) = \ln\bigg(\displaystyle \frac{y}{1 - y}\bigg)$. 

\vspace{1em}

\begin{proof}
    The sigmoid function is given by
    \begin{equation}\label{eq:4.59}\tag{4.59}
        \sigma(a) = \frac{1}{1 + e^{-a}}
    \end{equation}
    The symmetry property is easily satisfied, as
    \begin{equation}\label{eq:4.60}\tag{4.60}
        \sigma(-a) 
        = \frac{1}{1 + e^a} 
        = \frac{1 + e^a + 1 + e^{-a}}{(1 + e^{-a})(1 + e^a)}
        - \frac{1 + e^a}{(1 + e^{-a})(1 + e^a)}
        = 1 - \frac{1}{1 + e^{-a}} 
        = 1 - \sigma(a)
    \end{equation}\label{eq:4.60}
    The sigmoid function is bijective, so inversable. Therefore, let $\sigma(x) = y$.
    Then,
    \[
        y = \frac{1}{1 + e^{-x}} 
        \iff (1 + e^{-x})y = 1 
        \iff e^{-x} &= \frac{1 - y}{y} 
        \iff x = \ln\bigg(\frac{y}{1-y}\bigg)
    \] 
    so the inverse of the sigmoid function is given by
    \[
        \sigma^{-1}(y) = \ln\bigg(\frac{y}{1 - y}\bigg)
    \] 
\end{proof}

\section*{Exercise 4.8 $\star$}
Using ($\ref{eq:4.57}$) and $(\ref{eq:4.58})$, derive the result $(\ref{eq:4.65})$ 
for the posterior class probability in the two-class generative model with Gaussian
densities, and verify the results $(\ref{eq:4.66})$ and $(\ref{eq:4.67})$ for the
parameters $\mathbf{w}$ and $w_0$.

\vspace{1em}

\begin{proof}
    It is known that the posterior probability for class $\mathcal{C}_1$ can
    be written as
    \begin{equation}\label{eq:4.57}\tag{4.57}
        p(\mathcal{C}_1 | \mathbf{x}) = \sigma(a)
    \end{equation}
    where we have defined 
    \begin{equation}\label{eq:4.58}\tag{4.58}
        a = \ln \frac{p(\mathbf{x} | \mathcal{C}_1)p(\mathcal{C}_1)}
            {p(\mathbf{x} | \mathcal{C}_2)p(\mathcal{C}_2)}
    \end{equation}
    and $\sigma$ is the logistic sigmoid function defined by $(\ref{eq:4.59})$.
    We start by expanding $a$ and rewritting it as
    \[
        a = \ln p(\mathbf{x} | \mathcal{C}_1) - \ln p(\mathbf{x} | \mathcal{C}_2)
            + \ln \frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}
    \] 
    Since the class-conditional densities are Gaussian, i.e. the density
    for a class $\mathcal{C}_k$ is given by
    \begin{equation}\label{eq:4.64}\tag{4.64}
        p(\mathbf{x} | \mathcal{C}_k) = \mathcal{N}(\mathbf{x} | \bm{\mu}_k, \mathbf{\Sigma})
        = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\mathbf{\Sigma}|^{1/2}}
        \exp \bigg\{-\frac{1}{2} (\mathbf{x} - \bm{\mu}_k)^T\mathbf{\Sigma}^{-1}
        (\mathbf{x} - \bm{\mu}_k)\bigg\}
    \end{equation}
    one can easily obtain that
    \begin{align*}
        a 
        &= \ln p(\mathbf{x} | \mathcal{C}_1) - \ln p(\mathbf{x} | \mathcal{C}_2) 
        + \ln \frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\\
        &= \bm{\mu}_1^T\mathbf{\Sigma}^{-1}\mathbf{x}
        - \bm{\mu}_2^T\mathbf{\Sigma}^{-1}\mathbf{x} 
        - \frac{1}{2} \bm{\mu}_1^T\mathbf{\Sigma}^{-1}\bm{\mu}_1
        + \frac{1}{2} \bm{\mu}_2^T\mathbf{\Sigma}^{-1}\bm{\mu}_2
        + \ln \frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\\
        &= (\bm{\mu}_1 - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1} \mathbf{x}
        - \frac{1}{2} \bm{\mu}_1^T\mathbf{\Sigma}^{-1}\bm{\mu}_1
        + \frac{1}{2} \bm{\mu}_2^T\mathbf{\Sigma}^{-1}\bm{\mu}_2
        + \ln \frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\\
        &= \mathbf{w}^T\mathbf{x} + w_0
    \end{align*}
    where we have defined 
    \begin{equation}\label{eq:4.66}\tag{4.66}
        \mathbf{w} = \mathbf{\Sigma}^{-1}(\bm{\mu}_1 - \bm{\mu}_2)
    \end{equation}
    \vspace{-1em}
    \begin{equation}\label{eq:4.67}\tag{4.67}
        w_0 = 
        - \frac{1}{2} \bm{\mu}_1^T\mathbf{\Sigma}^{-1}\bm{\mu}_1
        + \frac{1}{2} \bm{\mu}_2^T\mathbf{\Sigma}^{-1}\bm{\mu}_2
        + \ln \frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}
    \end{equation}
    Therefore, the posterior probability for class $\mathcal{C}_1$ 
    is given by
    \begin{equation}\label{eq:4.65}\tag{4.65}
        p(\mathcal{C}_1 | \mathbf{x}) = \sigma(\mathbf{w}^T\mathbf{x} + w_0)
    \end{equation}
\end{proof}

\section*{Exercise 4.9 $\star$}
Consider a generative classification model for $K$ classes
defined by prior class probabilities $p(\mathcal{C}_k) = \pi_k$
and general class-conditional densities  $p(\bm{\phi} | \mathcal{C}_k)$
where $\mathbf{\phi}$ is the input feature vector. Suppose
we are given a training set $\{\bm{\phi}_n, \mathbf{t}_n\}$ where
$n = 1,\ldots, N$, and $\mathbf{t}_n$ is a binary target vector
of length $K$ that use the 1-of-$K$ coding scheme, so
that it has components $t_{nj} = \mathbf{I}_{jk}$ if pattern
$n$ is from class $\mathcal{C}_k$. Assuming that the data points
are drawn independently from this model, show that the 
maximum-likelihood solution for the prior probabilites is given
by 
\begin{equation}\label{eq:4.159}\tag{4.159}
    \pi_k = \frac{N_k}{N}
\end{equation}
where $N_k$ is the number of data points assigned to class 
$\mathcal{C}_k$.

\vspace{1em}

\begin{proof}
    Let $\mathbf{T}$ be the $N \times K$ matrix with the rows $\mathbf{t}_n^T$ and
    $\mathbf{\Phi}$ the $N \times M$ matrix with the rows $\bm{\phi}_n^T$.
    Also, let's define the column vector $\bm{\pi} = (\pi_1, \pi_2, \ldots, \pi_K)^T$.
    We have that
    \[
        p(\bm{\phi}_n, \mathcal{C}_k)
        = p(\mathcal{C}_k)p(\bm{\phi}_n | \mathcal{C}_k)
        = \pi_k p(\bm{\phi}_n | \mathcal{C}_k)
    \] 
    so the likelihood function is given by
    \[
        p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        = \prod_{n = 1}^N p(\mathbf{t}_n | \mathbf{\Phi}, \bm{\pi})
        = \prod_{n = 1}^N \prod_{j = 1}^K
        \bigg[\pi_j p(\bm{\phi}_n | \mathcal{C}_j)\bigg]^{t_{nj}}
    \] 
    The log likelihood is then easily derived as
    \[
        \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        = \sum_{n=1}^{N} \sum_{j=1}^{K} \bigg(t_{nj} \ln \pi_j
        + t_{nj} \ln p(\bm{\phi}_n | \mathcal{C}_j)\bigg)
    \] 
    We aim to minimize this with respect to $\pi_k$, while
    still maintaining the constraint $\sum_{k=1}^{N} \pi_k = 1$
    Therefore, by only keeping the terms depending on $\pi_k$, we obtain the Lagrangian
    \[
        \mathcal{L}(\bm{\pi}, \lambda)
        = \sum_{n=1}^{N} \sum_{j=1}^{K} t_{nj} \ln \pi_j
        + \lambda \bigg(\sum_{j=1}^{N} \pi_j - 1\bigg)
    \]
    with the gradient
    \[
        \nabla_{\pi_k, \lambda} \mathcal{L}(\bm{\pi}, \lambda)
        = \begin{pmatrix}
            \displaystyle \frac{1}{\pi_k} \sum_{n=1}^{N} t_{nk} + \lambda \\
            \\
            \displaystyle \sum_{k=1}^{N} \pi_k - 1
        \end{pmatrix}
    \] 
    By setting this gradient to $0$, from the first relation we have that
    \[
        \pi_k\lambda = -\sum_{n=1}^{N} t_{nk} = -N_k
    \] 
    Summing this over $k$, one can see that
     \[
         \lambda = -N
    \] 
    After substituting this into the derivative and then setting it to 0, 
    we obtain the maximum-likelihood solution for $\pi_k$, that is
    \[
        \pi_k = \frac{N_k}{N}
    \] 
\end{proof}

\section*{Exercise 4.10 $\star \star$}
Consider the classification model of Exercise 4.9 and now suppose
that the class-conditional densities are given by Gaussian 
distributions with a shared covariance matrix, so that
\begin{equation}\label{eq:4.160}\tag{4.160}
    p(\bm{\phi} | \mathcal{C}_k) = \mathcal{N}(\bm{\phi} | \bm{\mu}_k, \mathbf{\Sigma})
\end{equation}
Show that the maximum likelihood solution for the mean of the
Gaussian distribution for class $\mathcal{C}_k$ is given by
\begin{equation}\label{eq:4.161}\tag{4.161}
    \bm{\mu}_k = \frac{1}{N_k} \sum_{n=1}^{N} t_{nk} \bm{\phi}_n
\end{equation}
which represents the mean of those feature vectors assigned
to class $\mathcal{C}_k$. Similarly, show that the maximum likelihood
solution for the shared covariance matrix is given by
\begin{equation}\label{eq:4.162}\tag{4.162}
    \mathbf{\Sigma} = \sum_{k=1}^{K} \frac{N_k}{N}\mathbf{S}_k
\end{equation}
where 
\begin{equation}\label{eq:4.163}\tag{4.163}
\mathbf{S}_k = \frac{1}{N_k} \sum_{n=1}^{N} 
t_{nk}(\bm{\phi}_n - \bm{\mu_k})(\bm{\phi}_{k} - \bm{\mu}_k)^T
\end{equation}
Thus $\mathbf{\Sigma}$ is given by a weighted average of the covariances
of the data associated with each class, in which the weighting coefficients
are given by the prior probabilities of the classes.

\vspace{1em}

\begin{proof}
    Using the same notations as in the last exercise, we remember that the
    log likelihood is given by
    \[
        \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        = \sum_{n=1}^{N} \sum_{j=1}^{K} \bigg(t_{nj} \ln \pi_j
        + t_{nj} \ln p(\bm{\phi}_n | \mathcal{C}_j)\bigg)
    \] 
    By keeping only the parts depending on $\bm{\mu}_k$,
    \[
        \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        = -\frac{1}{2}\sum_{n=1}^{N} \sum_{j=1}^{K} 
        t_{nj} (\bm{\phi}_n - \bm{\mu}_k)^T\mathbf{\Sigma}^{-1}
            (\bm{\phi}_n - \bm{\mu}_k) + \text{const}
    \] 
    For a symmetric matrix $\mathbf{W}$, one could show that
    \[
        \pdv{s}(\mathbf{x} - \mathbf{s})^T\mathbf{W}(\mathbf{x} - \mathbf{s}) 
        = -2 \mathbf{W}(\mathbf{x} - \mathbf{s})
    \] 
    Therefore, the derivative with respect to $\mathbf{\mu}_k$ of the 
    log-likelihood is given by
    \[
        \pdv{\bm{\mu}_k}
        \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        = \sum_{n=1}^{N} t_{nk} \mathbf{\Sigma}^{-1}(\bm{\phi}_n - \bm{\mu}_k)
    \] 
    Since $\sum_{n=1}^{N} t_{nk} = N_k$, 
    by setting the derivative to 0 and rearranging the terms, we have that
    the solution for maximum likelihood is
    \[
        \bm{\mu}_k = \frac{1}{N_k} \sum_{n=1}^{N} t_{nk} \bm{\phi}_n
        \tag{4.161}
    \] 
    Similarly, we do the same for the shared covariance matrix.
    By keeping only the terms depending on $\mathbf{\Sigma}$, the
    log likelihood is given by
    \begin{align*}
        \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        &= -\frac{N}{2} \ln|\mathbf{\Sigma}| - \frac{1}{2}\sum_{n=1}^{N} \sum_{k=1}^{K} 
            t_{nk} (\bm{\phi}_n - \bm{\mu}_k)^T\mathbf{\Sigma}^{-1}
            (\bm{\phi}_n - \bm{\mu}_k) + \text{const} \\
        &= -\frac{N}{2} \ln|\mathbf{\Sigma}| - \frac{1}{2}\sum_{n=1}^{N} \sum_{k=1}^{K} 
            t_{nk} \big(\bm{\phi}_n^T\mathbf{\Sigma}^{-1}\bm{\phi}_n 
            - 2\bm{\phi}_n^T\mathbf{\Sigma}^{-1}\bm{\mu}_k 
            + \bm{\mu}_k^T\mathbf{\Sigma}^{-1}\bm{\mu}_k\big)
    \end{align*}
    By using (C.28) and 
    \[
    \pdv{\mathbf{X}} \mathbf{a}^T\mathbf{X}^{-1}\mathbf{b}
    = -\mathbf{X}^{-T} \mathbf{ab}^T \mathbf{X}^{-T}
    \] 
    we take the derivative of the log likelihood with respect to $\mathbf{\Sigma}$ 
    and obtain:
    \begin{align*}
        \pdv{\mathbf{\Sigma}} \ln p(\mathbf{T} | \mathbf{\Phi}, \bm{\pi})
        &= -\frac{N}{2} \mathbf{\Sigma}^{-1}
        + \frac{1}{2} \sum_{n=1}^{N} \sum_{j=k}^{K} t_{nk} \mathbf{\Sigma}^{-1}
        \big(\bm{\phi}_n\bm{\phi}_n^T - 2\bm{\phi}_n\bm{\mu}_k^T + \bm{\mu}_k\bm{\mu}_k^T\big) 
        \mathbf{\Sigma}^{-1} \\
        &= -\frac{N}{2} \mathbf{\Sigma}^{-1} 
        + \frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \mathbf{\Sigma}^{-1}
        (\bm{\phi}_n - \bm{\mu}_k)(\bm{\phi}_n - \bm{\mu}_k)^T \mathbf{\Sigma}^{-1} \\
        &= -\frac{N}{2} \mathbf{\Sigma}^{-1} 
        + \frac{1}{2} \mathbf{\Sigma}^{-1} 
        \bigg(\sum_{k=1}^{K} N_k\mathbf{S}_k\bigg) \mathbf{\Sigma}^{-1}
    \end{align*}
    where $\mathbf{S}_k$ is defined by $(\ref{eq:4.163})$. Therefore, by setting
    this derivative to 0 and rearranging the terms,
    we obtain the maximum-likelihood solution for the
    shared covariance matrix
    \[
        \mathbf{\Sigma} = \sum_{k=1}^{K} \frac{N_k}{N} \mathbf{S}_k \tag{4.162}
    \] 
\end{proof}

\section*{Exercise 4.11 $\star \star$}
Consider a classification problem with $K$ classses for which the feature
vector $\mathbf{\phi}$ has $M$ components each of which can take $L$ discrete
states. Let the values of the components be represented by a 1-of-$L$ binary coding scheme.
Further suppose that, conditioned on the class $\mathcal{C}_k$, the $M$ components
of $\bm{\phi}$ are independent, so that the class-conditional density factorizes
with respect to the feature vector components. Show that the quantities given by
$(\ref{eq:4.63})$, which appear in the argument to the softmax function
describing the posterior class probabilties, are linear functions of the components
of $\bm{\phi}$. Note that this represents an example of the naive Bayes model
which is discussed in Section 8.2.2.

\vspace{1em}

\begin{proof}
    We've seen in Section 4.2 that the posterior probabilities
    can be written as $\emph{normalized}$ $\emph{exponentials}$:
    \begin{equation}\label{eq:4.62}\tag{4.62}
        p(\mathcal{C}_k | \bm{\phi}) 
        = \frac{\exp(a_k)}{\sum_{j} \exp(a_j)}
    \end{equation}
    where
    \begin{equation}\label{eq:4.63}\tag{4.63}
        a_k = \ln p(\bm{\phi} | \mathcal{C}_k)p(\mathcal{C}_k)
    \end{equation}
    Considering the setup of our classification problem,
    our class-conditional distribution will be of the form
    \[
        p(\bm{\phi} | \mathcal{C}_k) 
        = \prod_{i = 1}^M \prod_{j = 1}^L
            \bm{\mu}_{kij}^{\bm{\phi}_{ij}}
    \] 
    where $\bm{\mu}_k$ is given by $(\ref{eq:4.161})$. 
    Therefore,
    by replacing into $(\ref{eq:4.63})$, the arguments
    of the softmax function are given by
    \[
        a_k = \ln p(\mathcal{C}_k) 
        + \sum_{i=1}^{M} \sum_{j=1}^{L} \bm{\phi}_{ij} \ln \bm{\mu}_{kij} 
    \] 
    and are obviously linear functions of the components
    of $\bm{\phi}$.
\end{proof}

\section*{Exercise 4.12 $\star$}
Verify the relation ($\ref{eq:4.88}$) for the derivative of the logistic sigmoid
function defined by ($\ref{eq:4.59}$).

\vspace{1em}

\begin{proof}
    By taking the derivative of $(\ref{eq:4.59})$, we have that: 
    \[
        \pdv{a} \sigma(a) 
        = \pdv{a}\bigg(\frac{1}{1 + e^{-a}}\bigg)
        = \frac{e^{-a}}{(1 + e^{-a})^2}
        = \frac{1 + e^{-a}}{(1 + e^{-a})^2} - \frac{1}{(1 + e^{-a})^2} 
        = \frac{1}{1 + e^{-a}} + \bigg(\frac{1}{1 + e^{-a}}\bigg)^2
    \] 
    We recognize the expression of the logistic sigmoid function, so
    \begin{equation}\label{eq:4.88}\tag{4.88}
        \pdv{a} \sigma(a) = \sigma(a) + \sigma(a)^2 = \sigma(a) \big(1 + \sigma(a)\big)
    \end{equation}
\end{proof}

\section*{Exercise 4.13 $\star$}
By making use of the result ($\ref{eq:4.88}$) for the derivative of the logistic
sigmoid, show that the derivative of the error function ($\ref{eq:4.90}$) for 
the logistic regression model is given by (\ref{eq:4.91}).

\vspace{1em}

\begin{proof}
    The error function for the logistic regression is given by
    \begin{equation}\label{eq:4.90}\tag{4.90}
        E(\mathbf{w}) 
        = -\ln p(\mathbf{t} | \mathbf{w})
        = -\sum_{n = 1}^{N} \{t_n \ln y_n + (1 - t_n) \ln(1 - y_n)\} 
    \end{equation}
    where $y_n = \sigma(a_n)$ and $a_n = \mathbf{w}^T\bm{\phi}_n$.
    Taking the derivative of the error function with respect to $\mathbf{w}$
    gives
    \begin{align*}
        \nabla_{\mathbf{w}} E(\mathbf{w})
        &= \nabla_{\mathbf{w}} \bigg(-\sum_{n=1}^{N} \big\{
            t_n \ln \sigma(\mathbf{w}^T\bm{\phi}_n) 
                + (1 - t_n) \ln \sigma(1 - \mathbf{w}^T\bm{\phi}_n)
            \big\}\bigg) \\
        &= -\sum_{n=1}^{N} \bigg[t_n \nabla_{\mathbf{w}} \ln \sigma(\mathbf{w}^T\bm{\phi}_n)
        + (1 - t_n) \nabla_{\mathbf{w}} \ln \{1 - \sigma(\mathbf{w}^T\bm{\phi}_n)\}\bigg]
    \end{align*}
    Using ($\ref{eq:4.88}$), we can compute the first gradient term :
    \begin{align*}
        \nabla_{\mathbf{w}} \ln \sigma(\mathbf{w}^T\bm{\phi}_n)
        = \frac{1}{\sigma(\mathbf{w}^T\bm{\phi}_n)} 
        \bigg(\nabla_{\mathbf{w}} \sigma(\mathbf{w}^T\bm{\phi}_n)\bigg)
        &= \frac{\sigma(\mathbf{w}^T\bm{\phi}_n)\big(1 + \sigma(\mathbf{w}^T\bm{\phi}_n)\big)}
        {\sigma(\mathbf{w}^T\bm{\phi}_n)} \bigg(\nabla_{\mathbf{w}} \mathbf{w}^T\bm{\phi}_n\bigg) \\
        &= \big(1 + \sigma(\mathbf{w}^T\bm{\phi}_n)\big) \bm{\phi}_n
        = (1 + y_n)\bm{\phi}_n
    \end{align*}
    Analogously, but this time using the fact that $1 - \sigma(a) = \sigma(-a)$,
    the second gradient term is given by
    \begin{align*}
        \nabla_{\mathbf{w}} \ln\{1 - \sigma(\mathbf{w}^T\bm{\phi}_n)\}
        = \nabla_{\mathbf{w}} \ln \sigma(-\mathbf{w}^T\bm{\phi}_n)
        = -\big(1 + \sigma(-\mathbf{w}^T\bm{\phi}_n)\big) \bm{\phi}_n
        = -\sigma(\mathbf{w}^T\bm{\phi}_n)\bm{\phi}_n 
        = -y_n \bm{\phi}_n
    \end{align*}
    By replacing back into the gradient of the error function,
    we obtain
    \begin{equation}\label{eq:4.91}\tag{4.91}
        \nabla_{\mathbf{w}} E(\mathbf{w})
        = -\sum_{n=1}^{N} \big\{t_n (1 + y_n) \bm{\phi}_n - (1 - t_n) y_n \bm{\phi}_n\big\}
        &= \sum_{n=1}^{N} (y_n - t_n) \bm{\phi}_n \tag{4.91}
    \end{equation}
\end{proof}

\section*{Exercise 4.14 $\star$}
Show that for a linearly separable data set, the maximum likelihood
solution for the logistic regression model is obtained by finding a vector
$\mathbf{w}$ whose decision boundary $\mathbf{w}^T\bm{\phi}(\mathbf{x}) = 0$ 
separates the classes and then taking the magnitude of $\mathbf{w}$ to infinity.

\vspace{1em}

\begin{proof}
    Suppose there exists $\mathbf{w}$ such that the hyperplane 
    $\mathbf{w}^T\bm{\phi} = 0$ separates the data set.
    Because the data set is linearly separable, 
    $\mathbf{w}^T\bm{\phi}_a < 0$ and $\mathbf{w}^T\bm{\phi}_b > 0$
    for all $\bm{\phi}_a \in \mathcal{C}_1$ and $\bm{\phi}_b \in \mathcal{C}_2$.
    One can observe that the maximum likelihood is obtained
    when  $p(\mathcal{C}_1 | \bm{\phi}_a) = 1$ and $p(\mathcal{C}_2 | \bm{\phi}_b) = 1$ 
    for all $\bm{\phi}_a \in \mathcal{C}_1, \bm{\phi}_b \in \mathcal{C}_2$.
    Since our hyperplane is already chosen, there is a fixed angle $\theta_n$
    between each $\bm{\phi}_n$ and $\mathbf{w}$ such that $\cos \theta_n \neq 0$. Therefore,
    by using the geometric definition of the dot product
    \[
        \mathbf{w}^T\bm{\phi}_n
        = \Vert\mathbf{w}\Vert \Vert\bm{\phi}_n\Vert \cos \theta_n
    \] 
    we see that our maximization is achieved by taking the magnitude
    of $\Vert\mathbf{w}\Vert$ to infinity, as
    \[
        \lim_{\Vert\mathbf{w}\Vert \to \infty} p(\mathcal{C}_1 | \bm{\phi}_a)
        = \lim_{\Vert\mathbf{w}\Vert \to \infty} 
        \sigma(\Vert\mathbf{w}\Vert \Vert{\bm{\phi}_a}\Vert \cos \theta_a)
        = \lim_{\Vert\mathbf{w}\Vert \to \infty}
        \frac{1}{1 + \exp{-\Vert\mathbf{w}\Vert \Vert{\bm{\phi}_a}\Vert \cos \theta_a}} 
        = 1
    \] 
    and 
    \[
        \lim_{\Vert\mathbf{w}\Vert \to \infty} p(\mathcal{C}_2 | \bm{\phi}_b)
        = \lim_{\Vert\mathbf{w}\Vert \to \infty} 
        \sigma(\Vert\mathbf{w}\Vert \Vert{\bm{\phi}_b}\Vert \cos \theta_b)
        = \lim_{\Vert\mathbf{w}\Vert \to \infty}
        \frac{1}{1 + \exp{-\Vert\mathbf{w}\Vert \Vert{\bm{\phi}_b}\Vert \cos \theta_b}} 
        = 0
    \]
    where $\bm{\phi} \in \mathcal{C}_1, \bm{\phi}_b \in \mathcal{C}_2$ and we've used the
    fact that $\mathbf{w}^T\bm{\phi}_a < 0$ and $ \mathbf{w}^T\bm{\phi}_b > 0$.
\end{proof}

\section*{Exercise 4.15 $\star \star$}
Show that the Hessian matrix $\mathbf{H}$ for the logistic regression
model, given by $(\ref{eq:4.97})$, is positive definite. Here $\mathbf{R}$
is a diagonal matrix with elements $y_n(1 - y_n)$, and $y_n$ is
the output of the logistic regression model for input vector $\mathbf{x}_n$.
Hence show that the error function is a convex function of $\mathbf{w}$
and it has an unique minimum.

\vspace{1em}

\begin{proof}
    The Hessian of the error function is given by
    \begin{equation}\label{eq:4.97}\tag{4.97}
        \mathbf{H} 
        = \nabla \nabla E(\mathbf{w})
        = \sum_{n=1}^{N} y_n(1 - y_n)\bm{\phi}_n\bm{\phi}_n^T
        = \mathbf{\Phi}^T\mathbf{R}\mathbf{\Phi}
    \end{equation}
    Let $\mathbf{u}$ be a $M$-dimensional column vector.
    By using the sum formulation for the hessian matrix, we have that
    \[
        \mathbf{u}^T\mathbf{H}\mathbf{u} 
        = \sum_{n=1}^{N} y_n(1 - y_n)\mathbf{u}^T\bm{\phi}_n\bm{\phi}_n^T\mathbf{u}
        = \sum_{n=1}^{N} y_n(1 - y_n) \big(\bm{\phi}_n^T\mathbf{u}\big)^T \bm{\phi}_n^T\mathbf{u}
        = \sum_{n=1}^{N} y_n(1 - y_n) \Vert\bm{\phi}_n^T \mathbf{u}\Vert^2
    \] 
    which is $>0$ since $y_n$ is the output of the logistic sigmoid function,
    so $0 < y_n < 1$. Because $\mathbf{u}$ was chosen arbitrarily, we have that
    $\mathbf{H}$ is positive definite. As a result, the error function is convex and has an unique
    minimum.
\end{proof}

\section*{Exercise 4.16 $\star$}
Consider a binary classification problem in which each observation
$\mathbf{x}_n$ is known to belong to one of two classes, corresponding to $t=0$
and $t=1$, and suppose that the procedure for collecting training data
is imperfect, so that training points are sometimes mislabelled. For
every data point $\mathbf{x}_n$, instead of having a value $t$ for
the class label, we have instead a value $\pi_n$ representing the
probability that $t_n = 1$. Given a probabilistic model $p(t = 1 | \bm{\phi})$,
write down the log likelihood function appropiate for such a data set.

\vspace{1em}

\begin{proof}
    Straight away, we can see that $p(t = 0 | \bm{\phi}) = 1 - p(t = 1 | \bm{\phi})$.
    An fair approach would be to express $p(t_n | \bm{\phi})$ as a weighted
    average of $p(t_n = 0 | \bm{\phi})$ and $p(t_n = 1 | \bm{\phi}$ dictated by $\pi_n$.
    Therefore, the likelihood would be given by 
    \[
        p(\mathbf{t} | \bm{\phi})
        = \prod_{n = 1}^N p(t_n | \bm{\phi})
        = \prod_{n = 1}^N p(t_n = 1 | \bm{\phi})^{\pi_n}
        p(t_n = 0 | \bm{\phi})^{1 - \pi_n}
        = \prod_{n = 1}^N p(t_n = 1 | \bm{\phi})^{\pi_n}
        \big\{1 - p(t_n = 1 | \bm{\phi})\big\}^{1 - \pi_n}
    \] 
    which has the log likelihood
    \[
        \ln p(\mathbf{t} | \bm{\phi}) 
        = \sum_{n=1}^{N} \pi_n p(t_n = 1 | \bm{\phi})
        + (1 - \pi_n)\big\{1 - p(t_n = 1 | \bm{\phi})\big\}
    \] 
\end{proof}

\section*{Exercise 4.17 $\star$}
Show that the derivatives of the softmax activation function
$(\ref{eq:4.104})$ where the $a_k$ are defined by $(\ref{eq:4.105})$, are
given by $(\ref{eq:4.106})$.

\vspace{1em}

\begin{proof}
    The softmax activation function is given by
    \begin{equation}\label{eq:4.104}\tag{4.104}
        y_k = \frac{\exp(a_k)}{\sum_{j} \exp(a_j)}
    \end{equation}
    where
    \begin{equation}\label{eq:4.105}\tag{4.106}
        a_k = \mathbf{w}_k^T\bm{\phi}
    \end{equation}
    Taking the derivative of $(\ref{eq:4.104})$ with respect to
    $a_j$ and applying the quotient rule gives
    \begin{align*}
        \pdv{y_k}{a_j} 
        = \pdv{a_j} \bigg(\frac{\exp(a_k)}{\sum_{i} \exp(a_i)}\bigg)
        &= \frac{I_{kj} \exp(a_k) \sum_{i} \exp(a_i) - \exp(a_k)\exp(a_j)} 
        {\bigg(\sum_{i} \exp(a_i)\bigg)^2} \\
        &= \frac{\exp(a_k)}{\sum_{j} \exp(a_j)}
        \bigg(I_{kj} - \frac{\exp(a_j)}{\sum_{j} \exp(a_j)}\bigg)
    \end{align*}
    which is equivalent to 
    \begin{equation}\label{eq:4.106}\tag{4.106}
        \pdv{a_j} y_k = y_k(I_{kj} - y_j)
    \end{equation}
\end{proof}

\section*{Exercise 4.18 $\star$}
Using the result $(\ref{eq:4.106})$ for
the derivatives of the softmax activation function,
show that the gradients of the cross-entropy error $(\ref{eq:4.108})$
are given by ($\ref{eq:4.109}$).

\vspace{1em}

\begin{proof}
    The cross-entropy error is given by
    \begin{equation}\label{eq:4.108}\tag{4.108}
        E(\mathbf{w}_1, \ldots, \mathbf{w}_K)
        = -\ln p(\mathbf{T} | \mathbf{w}_1, \ldots, \mathbf{w}_K)
        = - \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln y_{nk}
    \end{equation}
    Taking its derivative with respect to $\mathbf{w}_j$ yields 
    \begin{align*}
        \pdv{\mathbf{w}_j} E(\mathbf{w}_1, \ldots, \mathbf{w}_K)
        = \pdv{\mathbf{w}_j} \bigg(-\sum_{n=1}^{K} \sum_{k=1}^{K} t_{nk} \ln y_{nk}\bigg) 
        = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \pdv{\mathbf{w}_j} \ln y_{nk} 
    \end{align*}
    By using $(\ref{eq:4.106})$ and the chain rule, we have that
    \begin{align*}
        \pdv{\mathbf{w}_j} \ln y_{nk}
        = \frac{1}{y_{nk}} \pdv{y_{nk}}{\mathbf{w}_j}
        = \frac{1}{y_{nk}} \pdv{y_{nk}}{a_j} \pdv{a_j}{\mathbf{w}_j}
        = \frac{1}{y_{nk}} y_{nk}(I_{kj} - y_{nj}) \bm{\phi}_n
        = (I_{kj} - y_{nj})\bm{\phi}_n
    \end{align*}
    Replacing back into the gradient,
    \[
        \pdv{\mathbf{w}_j} E(\mathbf{w}_1, \ldots, \mathbf{w}_K)
        = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} (I_{kj} - y_{nj}) \bm{\phi}_n
        = -\sum_{n=1}^{N} t_{nj}\bm{\phi}_n 
        + \sum_{n=1}^{N} \bigg(\sum_{k=1}^{N} t_{nk}\bigg) y_{nj} \bm{\phi}_n
    \] 
    gives the desired result
    \begin{equation}\label{eq:4.109}\tag{4.109}
        \pdv{\mathbf{w}_j}E(\mathbf{w}_1, \ldots, \mathbf{w}_K)
        = \sum_{n=1}^{N} (y_{nj} - t_{nj})\bm{\phi}_n
    \end{equation}
\end{proof}

\section*{Exercise 4.19 $\star$}
Write down expressions for the gradient of the log likelihood,
as well as the corresponding Hessian matrix, for the probit regression
model defined in Section 4.3.5. These are quantities that would be required to train such 
a model using IRLS.

\vspace{1em}

\begin{proof}
    The probit function is given by 
    \begin{equation}\label{eq:4.114}\tag{4.114}
        \Phi(a) = \int_{-\infty}^{a} \mathcal{N}(\theta | 0, 1) \diff \theta
    \end{equation}

    Like we've seen in Section 4.3.2, the log likelihood of 
    the model is given by ($\ref{eq:4.90}$), but this time with
    $y_n = \Phi(a_n)$, so
    \[
        \ln p(\mathbf{t} | \mathbf{w})
        = \sum_{n = 1}^{N} \{t_n \ln y_n + (1 - t_n) \ln(1 - y_n)\} 
    \] 
\end{proof}

